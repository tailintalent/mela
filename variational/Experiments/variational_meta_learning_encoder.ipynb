{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import sys, os\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "    from mela.settings.filepath import variational_model_PATH, dataset_PATH\n",
    "    isplot = True\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\n",
    "    from mela.settings.filepath import variational_model_PATH, dataset_PATH\n",
    "    if dataset_PATH[:2] == \"..\":\n",
    "        dataset_PATH = dataset_PATH[3:]\n",
    "    isplot = False\n",
    "\n",
    "from mela.util import plot_matrices, make_dir, get_struct_str, get_args, Early_Stopping, record_data, manifold_embedding\n",
    "from mela.pytorch.modules import Simple_Layer\n",
    "from mela.pytorch.net import Net, ConvNet\n",
    "from mela.pytorch.util_pytorch import Loss_with_uncertainty, get_criterion, to_np_array\n",
    "from mela.variational.util_variational import get_torch_tasks\n",
    "from mela.variational.variational_meta_learning import Master_Model, Statistics_Net, Generative_Net, load_model_dict, get_regulated_statistics\n",
    "from mela.variational.variational_meta_learning import VAE_Loss, sample_Gaussian, clone_net, get_nets, get_tasks, evaluate, get_reg, load_trained_models\n",
    "from mela.variational.variational_meta_learning import forward, get_forward_pred, get_rollout_pred_loss, get_autoencoder_losses, Loss_with_autoencoder\n",
    "from mela.variational.variational_meta_learning import plot_task_ensembles, plot_individual_tasks, plot_statistics_vs_z, plot_data_record, get_corrcoef\n",
    "from mela.variational.variational_meta_learning import plot_few_shot_loss, plot_individual_tasks_bounce\n",
    "from mela.variational.variational_meta_learning import get_latent_model_data, get_polynomial_class, get_Legendre_class, get_master_function\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(\"is_cuda: {0}\".format(is_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataset(tasks, num = None):\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    num_examples_select = 100\n",
    "    for i, task in enumerate(tasks.values()):\n",
    "        if num is not None and i > num:\n",
    "            break\n",
    "        ((X_train, y_train), (X_test, y_test)), info = task\n",
    "        X_train_list.append(X_train[:num_examples_select])\n",
    "        y_train_list.append(y_train[:num_examples_select])\n",
    "        X_test_list.append(X_test[:num_examples_select])\n",
    "        y_test_list.append(y_test[:num_examples_select])\n",
    "    X_train_all = torch.cat(X_train_list)\n",
    "    y_train_all = torch.cat(y_train_list)\n",
    "    X_test_all = torch.cat(X_test_list)\n",
    "    y_test_all = torch.cat(y_test_list)\n",
    "    return (X_train_all, y_train_all), (X_test_all, y_test_all)\n",
    "\n",
    "def plot_encoding(X, autoencoder, target = \"encoding\"):\n",
    "    axis_list = []\n",
    "    X = X.data\n",
    "    X = X[:200]\n",
    "    for i in range(len(X)):\n",
    "        data = X[i]\n",
    "        row, column = data.squeeze().nonzero().float().mean(0)\n",
    "        axis_list.append([row, column])\n",
    "    axis_list = np.array(axis_list)\n",
    "    encoding = autoencoder.encode(Variable(X)).cpu().data.numpy().astype(float)\n",
    "    if target == \"encoding\":\n",
    "        print(\"row:\")\n",
    "        plt.scatter(encoding[:,0], encoding[:,1], s = 0.5 + 3 * axis_list[:,0])\n",
    "        plt.show()\n",
    "        print(\"column:\")\n",
    "        plt.scatter(encoding[:,0], encoding[:,1], s = 0.5 + 3 * axis_list[:,1])\n",
    "        plt.show()\n",
    "    elif target == \"axis\":\n",
    "        print(\"row:\")\n",
    "        plt.scatter(axis_list[:,0], axis_list[:,1], s = 0.5 + 5 * encoding[:,0])\n",
    "        plt.show()\n",
    "        print(\"column:\")\n",
    "        plt.scatter(axis_list[:,0], axis_list[:,1], s = 0.5 + 5 * encoding[:,1])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_tasks(tasks, master_model = None, model = None, autoencoder = None, forward_steps = None, num_tasks = 3, oracle_size = None):\n",
    "    task_keys = np.random.choice(list(tasks.keys()), num_tasks, replace = False)\n",
    "    for i, task_key in enumerate(task_keys):\n",
    "        task = tasks[task_key]\n",
    "        ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), z_info = task\n",
    "        X_train = forward(autoencoder.encode, X_train_obs)\n",
    "        y_train = forward(autoencoder.encode, y_train_obs[:, forward_steps_idx])\n",
    "        X_test = forward(autoencoder.encode, X_test_obs)\n",
    "        y_test = forward(autoencoder.encode, y_test_obs[:, forward_steps_idx])\n",
    "        if oracle_size is not None:\n",
    "            z_train = Variable(torch.FloatTensor(np.repeat(np.expand_dims(z_info[\"z\"],0), len(X_train), 0)), requires_grad = False)\n",
    "            z_test = Variable(torch.FloatTensor(np.repeat(np.expand_dims(z_info[\"z\"],0), len(X_test), 0)), requires_grad = False)\n",
    "            if X_train.is_cuda:\n",
    "                z_train = z_train.cuda()\n",
    "                z_test = z_test.cuda()\n",
    "            X_train = torch.cat([X_train, z_train], 1)\n",
    "            X_test = torch.cat([X_test, z_test], 1)  \n",
    "        \n",
    "        # Plotting:\n",
    "        print(\"Task {0}:\".format(task_key))\n",
    "        plot_matrices(np.concatenate((to_np_array(X_test_obs[0]), to_np_array(y_test_obs)[:, np.array(forward_steps) - 1][0])))\n",
    "        if master_model is not None:\n",
    "            statistics = master_model.statistics_Net.forward_inputs(X_train, y_train)\n",
    "            latent_pred = get_forward_pred(master_model.generative_Net, X_test, forward_steps, is_time_series = True, latent_param = statistics, jump_step = 2, is_flatten = False)\n",
    "        else:\n",
    "            latent_pred = get_forward_pred(model, X_test, forward_steps, is_time_series = True, jump_step = 2, is_flatten = False)\n",
    "        pred_recons = forward(autoencoder.decode, latent_pred)\n",
    "        if oracle_size is None:\n",
    "            plot_matrices(np.concatenate((to_np_array(forward(autoencoder.decode, X_test.view(X_test.size(0), -1, 2))[0]), to_np_array(pred_recons[0]))))  \n",
    "        else:\n",
    "            plot_matrices(np.concatenate((to_np_array(forward(autoencoder.decode, X_test[:, :-oracle_size].contiguous().view(X_test.size(0), -1, 2))[0]), to_np_array(pred_recons[0]))))  \n",
    "\n",
    "\n",
    "class Conv_Autoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_struct_param,\n",
    "        decoder_struct_param,\n",
    "        enc_flatten_size = 512,\n",
    "        latent_size = (1,2),\n",
    "        settings = {},\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Conv_Autoencoder, self).__init__()\n",
    "        self.enc_flatten_size = enc_flatten_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = ConvNet(input_channels = 1, struct_param = encoder_struct_param, settings = settings, is_cuda = is_cuda)\n",
    "        self.decoder = ConvNet(input_channels = encoder_struct_param[-1][0], struct_param = decoder_struct_param, settings = settings, is_cuda = is_cuda)\n",
    "        self.enc_fully = Simple_Layer(enc_flatten_size, latent_size, settings = {\"activation\": \"linear\"}, is_cuda = is_cuda)\n",
    "        self.dec_fully = Simple_Layer(latent_size, enc_flatten_size, settings = {\"activation\": \"linear\"}, is_cuda = is_cuda)\n",
    "        self.is_cuda = is_cuda\n",
    "    \n",
    "    def encode(self, input):\n",
    "        enc_hidden, _ = self.encoder(input)\n",
    "        self.enc_hidden_size = enc_hidden.size()[1:]\n",
    "        enc_flatten = enc_hidden.view(enc_hidden.size(0), -1)\n",
    "        latent = self.enc_fully(enc_flatten)\n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        dec_hidden = self.dec_fully(latent).view(-1, *self.enc_hidden_size)\n",
    "        output, _ = self.decoder(dec_hidden)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input):\n",
    "        latent = self.encode(input)\n",
    "        return self.decode(latent)\n",
    "    \n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        return self.encoder.get_regularization(source = source, mode = mode) + \\\n",
    "                self.decoder.get_regularization(source = source, mode = mode) + \\\n",
    "                self.enc_fully.get_regularization(source = source, mode = mode) + \\\n",
    "                self.dec_fully.get_regularization(source = source, mode = mode)\n",
    "\n",
    "\n",
    "def train_epoch_pretrain(train_loader, X_test_all, autoencoder, optimizer_pre, isplot = True):\n",
    "    for batch_id, X_batch in enumerate(train_loader):\n",
    "        X_batch = Variable(X_batch)\n",
    "        optimizer_pre.zero_grad()\n",
    "        reconstruct = forward(autoencoder, X_batch)\n",
    "        reg = autoencoder.get_regularization(source = [\"weight\", \"bias\"]) * reg_amp_autoencoder\n",
    "        reg_latent = forward(autoencoder, X_batch).mean() * reg_amp_latent\n",
    "        loss_train = nn.MSELoss()(reconstruct, X_batch) + reg + reg_latent\n",
    "        loss_train.backward()\n",
    "        optimizer_pre.step()\n",
    "\n",
    "    reconstruct_test = forward(autoencoder, X_test_all)\n",
    "    loss_test = nn.MSELoss()(reconstruct_test, X_test_all)\n",
    "    to_stop = early_stopping_pre.monitor(loss_test.data[0])\n",
    "    print(\"epoch {0} \\tloss_train: {1:.6f}\\tloss_test: {2:.6f}\\treg: {3:.6f}\\treg_latent: {4:.6f}\".format(epoch, loss_train.data[0], loss_test.data[0], reg.data[0], reg_latent.data[0]))\n",
    "    if epoch % 10 == 0 and isplot:\n",
    "        plot_matrices(X_batch[0].cpu().data.numpy(), images_per_row = 5)\n",
    "        latent = forward(autoencoder.encode, X_batch)\n",
    "        print(\"latent: {0}\".format(latent.cpu().data.numpy()[0]))\n",
    "        plot_matrices(reconstruct[0].cpu().data.numpy(), images_per_row = 5)\n",
    "        plot_encoding(X_train_all[:,:1], autoencoder, \"axis\")\n",
    "    return to_stop\n",
    "\n",
    "\n",
    "def train_epoch_joint(motion_train_loader, X_motion_test, y_motion_test, conv_encoder, predictor, forward_steps, aux_coeff, isplot = True):\n",
    "    for batch_id, (X_motion_batch, y_motion_batch) in enumerate(motion_train_loader):\n",
    "        X_motion_batch = Variable(X_motion_batch)\n",
    "        y_motion_batch = Variable(y_motion_batch)\n",
    "        optimizer_motion.zero_grad()\n",
    "        loss_auxiliary, loss_pred_recons, pred_recons_batch = get_losses(conv_encoder, predictor, X_motion_batch, y_motion_batch, forward_steps = forward_steps)\n",
    "        reg_conv = conv_encoder.get_regularization(source = [\"weight\", \"bias\"]) * reg_amp_conv * reg_multiplier[epoch]\n",
    "        reg_predictor = predictor.get_regularization(source = [\"weight\", \"bias\"]) * reg_amp_predictor * reg_multiplier[epoch]\n",
    "        loss_train = loss_auxiliary * aux_coeff + loss_pred_recons + reg_conv + reg_predictor\n",
    "        loss_train.backward()\n",
    "        optimizer_motion.step()\n",
    "\n",
    "    loss_auxiliary_test, loss_pred_recons_test, pred_recons_test = get_losses(conv_encoder, predictor, X_motion_test, y_motion_test, forward_steps = forward_steps)\n",
    "    loss_test = loss_auxiliary_test * aux_coeff + loss_pred_recons_test + reg_conv + reg_predictor\n",
    "    to_stop = early_stopping_motion.monitor(loss_test.data[0])\n",
    "    print(\"epoch {0}\\tloss_train: {1:.6f}\\tloss_test: {2:.6f}\\tloss_aux: {3:.6f}\\tloss_pred: {4:.6f}\\treg_conv: {5:.6f}\\treg_predictor: {6:.6f}\".format(\n",
    "        epoch, loss_train.data[0], loss_test.data[0], loss_auxiliary_test.data[0] * aux_coeff, loss_pred_recons_test.data[0], reg_conv.data[0], reg_predictor.data[0]))\n",
    "    if epoch % 10 == 0 and isplot:\n",
    "        print(\"epoch {0}:\".format(epoch))\n",
    "        plot_matrices(np.concatenate((X_motion_batch[0].cpu().data.numpy(), y_motion_batch[:, torch.LongTensor(np.array(forward_steps) - 1).cuda()][0].cpu().data.numpy())))\n",
    "        plot_matrices(np.concatenate((forward(conv_encoder, X_motion_batch)[0].cpu().data.numpy(), pred_recons_batch[0].cpu().data.numpy())))\n",
    "        print(\"encoding:\")\n",
    "        plot_encoding(X_motion_batch[:,:1].contiguous(), conv_encoder, target = \"encoding\")\n",
    "        print(\"axis:\")\n",
    "        plot_encoding(X_motion_batch[:,:1].contiguous(), conv_encoder, target = \"axis\")\n",
    "        print(\"\\n\\n\")\n",
    "    return to_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain conv-autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_struct_param = [\n",
    "#     [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "# ]\n",
    "# settings = settings = {\"activation\": \"relu\"}\n",
    "# decoder_struct_param = [\n",
    "#     [32, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [32, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [1, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "# ]\n",
    "\n",
    "# conv_encoder = Conv_Autoencoder(\n",
    "#     encoder_struct_param,\n",
    "#     decoder_struct_param,\n",
    "#     enc_flatten_size = 512,\n",
    "#     latent_size = 2,\n",
    "#     settings = {\"activation\": \"leakyReluFlat\"},\n",
    "#     is_cuda = True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patience = 30\n",
    "# epochs = 1000\n",
    "# batch_size = 128\n",
    "# lr = 1e-3\n",
    "# reg_amp = 1e-7\n",
    "\n",
    "# (X_train_all, y_train_all), (X_test_all, y_test_all) = combine_dataset(tasks_train)\n",
    "# optimizer = optim.Adam(conv_encoder.parameters(), lr = lr)\n",
    "# train_loader = data_utils.DataLoader(X_train_all.data, batch_size = batch_size, shuffle = True)\n",
    "# early_stopping = Early_Stopping(patience = patience)\n",
    "# to_stop = False\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     to_stop = train_epoch_pretrain(train_loader, X_test_all, conv_encoder)\n",
    "#     if to_stop:\n",
    "#         print(\"Early stopping at iteration {0}\".format(i))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training conv_encoder and prediction at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = tasks_train[list(tasks_train.keys())[0]]\n",
    "# ((X_motion_train, y_motion_train), (X_motion_test, y_motion_test)), _ = task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_struct_param = [\n",
    "#     [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "# ]\n",
    "# settings = settings = {\"activation\": \"relu\"}\n",
    "# decoder_struct_param = [\n",
    "#     [32, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [32, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "#     [1, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "# ]\n",
    "\n",
    "# conv_encoder = Conv_Autoencoder(\n",
    "#     encoder_struct_param,\n",
    "#     decoder_struct_param,\n",
    "#     enc_flatten_size = 512,\n",
    "#     latent_size = 2,\n",
    "#     settings = {\"activation\": \"leakyReluFlat\"},\n",
    "#     is_cuda = True,\n",
    "# )\n",
    "\n",
    "# struct_param_predictor = [[40, \"Simple_Layer\", {}], [40, \"Simple_Layer\", {}], [(1, 2), \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "# predictor = Net(input_size = (3, 2), struct_param = struct_param_predictor, settings = {\"activation\": \"leakyReluFlat\"}, is_cuda = is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_steps = [1,2,4]\n",
    "# aux_coeff = 0.3\n",
    "# # With 10 neurons in the predictor:\n",
    "# # Joint training:\n",
    "# batch_size = 1000\n",
    "# epochs = 1000\n",
    "# patience = 100\n",
    "# lr = 1e-3\n",
    "# reg_amp_conv = 1e-6\n",
    "# reg_amp_predictor = 1e-5\n",
    "# reg_multiplier = np.linspace(0, 1, epochs + 1) ** 2\n",
    "# dataset_motion_train = data_utils.TensorDataset(X_motion_train.data, y_motion_train.data)\n",
    "# motion_train_loader = data_utils.DataLoader(dataset_motion_train, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# optimizer_motion = optim.Adam(params = itertools.chain(conv_encoder.parameters(), predictor.parameters()), lr = lr)\n",
    "# early_stopping_motion = Early_Stopping(patience = patience)\n",
    "# for epoch in range(epochs):\n",
    "#     to_stop = train_epoch_joint(motion_train_loader, X_motion_test, y_motion_test, conv_encoder, predictor, forward_steps)\n",
    "#     if to_stop:\n",
    "#         print(\"Early stopping at epoch {0}\".format(epoch))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_mode: oracle\n",
      "../../data/variational/trained_models/C-May18/ENet_oracle_['bounce-images']_input_14_(100,100)_stat_8_pre_200_pool_max_context_0_hid_(40, 40, 40)_(100, 3)_(60, 3)_VAE_False_0.2_uncer_False_lr_0.0002_reg_1e-07_actgen_leakyRelu_actmodel_leakyRelu_indi_core_mse_pat_500_for_1_C-May18_\n",
      "../../data/variational/trained_models/C-May18/Net_oracle_['bounce-images']_input_14_(100,100)_stat_8_pre_200_pool_max_context_0_hid_(40, 40, 40)_(100, 3)_(60, 3)_VAE_False_0.2_uncer_False_lr_0.0002_reg_1e-07_actgen_leakyRelu_actmodel_leakyRelu_indi_core_mse_pat_500_C-May18_\n"
     ]
    }
   ],
   "source": [
    "task_id_list = [\n",
    "# \"latent-linear\",\n",
    "# \"polynomial-3\",\n",
    "# \"Legendre-3\",\n",
    "# \"M-sawtooth\",\n",
    "# \"M-sin\",\n",
    "# \"M-Gaussian\",\n",
    "# \"M-tanh\",\n",
    "# \"M-softplus\",\n",
    "# \"C-sin\",\n",
    "# \"C-tanh\",\n",
    "# \"bounce-states\",\n",
    "\"bounce-images\",\n",
    "]\n",
    "\n",
    "exp_id = \"C-May18\"\n",
    "exp_mode = \"meta\"\n",
    "# exp_mode = \"finetune\"\n",
    "# exp_mode = \"oracle\"\n",
    "is_VAE = False\n",
    "is_uncertainty_net = False\n",
    "is_regulated_net = False\n",
    "is_load_data = False\n",
    "VAE_beta = 0.2\n",
    "task_id_list = get_args(task_id_list, 3, type = \"tuple\")\n",
    "if task_id_list[0] in [\"C-sin\", \"C-tanh\"]:\n",
    "    statistics_output_neurons = 2 if task_id_list[0] == \"C-sin\" else 4\n",
    "    z_size = 2 if task_id_list[0] == \"C-sin\" else 4\n",
    "    num_shots = 10\n",
    "    input_size = 1\n",
    "    output_size = 1\n",
    "    reg_amp = 1e-6\n",
    "    forward_steps = [1]\n",
    "    is_time_series = False\n",
    "elif task_id_list[0] == \"bounce-states\":\n",
    "    statistics_output_neurons = 8\n",
    "    num_shots = 100\n",
    "    z_size = 8\n",
    "    input_size = 6\n",
    "    output_size = 2\n",
    "    reg_amp = 1e-8\n",
    "    forward_steps = [1]\n",
    "    is_time_series = True\n",
    "elif task_id_list[0] == \"bounce-images\":\n",
    "    statistics_output_neurons = 8\n",
    "    num_shots = 100\n",
    "    z_size = 8\n",
    "    input_size = 6\n",
    "    output_size = 2\n",
    "    reg_amp = 1e-7\n",
    "    forward_steps = [1]\n",
    "    is_time_series = True\n",
    "else:\n",
    "    raise\n",
    "\n",
    "is_autoencoder = True\n",
    "max_forward_steps = 10\n",
    "\n",
    "lr = 2e-4\n",
    "num_train_tasks = 100\n",
    "num_test_tasks = 100\n",
    "batch_size_task = num_train_tasks\n",
    "num_iter = 20000\n",
    "pre_pooling_neurons = 200\n",
    "num_context_neurons = 0\n",
    "statistics_pooling = \"max\"\n",
    "struct_param_pre_neurons = (100,3)\n",
    "struct_param_gen_base_neurons = (60,3)\n",
    "main_hidden_neurons = (40, 40, 40)\n",
    "activation_gen = \"leakyRelu\"\n",
    "activation_model = \"leakyRelu\"\n",
    "optim_mode = \"indi\"\n",
    "loss_core = \"mse\"\n",
    "patience = 500\n",
    "array_id = 0\n",
    "\n",
    "exp_id = get_args(exp_id, 1)\n",
    "exp_mode = get_args(exp_mode, 2)\n",
    "statistics_output_neurons = get_args(statistics_output_neurons, 4, type = \"int\")\n",
    "is_VAE = get_args(is_VAE, 5, type = \"bool\")\n",
    "VAE_beta = get_args(VAE_beta, 6, type = \"float\")\n",
    "lr = get_args(lr, 7, type = \"float\")\n",
    "pre_pooling_neurons = get_args(pre_pooling_neurons, 8, type = \"int\")\n",
    "num_context_neurons = get_args(num_context_neurons, 9, type = \"int\")\n",
    "statistics_pooling = get_args(statistics_pooling, 10)\n",
    "struct_param_pre_neurons = get_args(struct_param_pre_neurons, 11, \"tuple\")\n",
    "struct_param_gen_base_neurons = get_args(struct_param_gen_base_neurons, 12, \"tuple\")\n",
    "main_hidden_neurons = get_args(main_hidden_neurons, 13, \"tuple\")\n",
    "reg_amp = get_args(reg_amp, 14, type = \"float\")\n",
    "activation_gen = get_args(activation_gen, 15)\n",
    "activation_model = get_args(activation_model, 16)\n",
    "optim_mode = get_args(optim_mode, 17)\n",
    "is_uncertainty_net = get_args(is_uncertainty_net, 18, \"bool\")\n",
    "loss_core = get_args(loss_core, 19)\n",
    "patience = get_args(patience, 20, \"int\")\n",
    "forward_steps = get_args(forward_steps, 21, \"tuple\")\n",
    "array_id = get_args(array_id, 22)\n",
    "\n",
    "# Settings:\n",
    "task_settings = {\n",
    "    \"xlim\": (-5, 5),\n",
    "    \"num_examples\": num_shots * 2,\n",
    "    \"test_size\": 0.5,\n",
    "}\n",
    "isParallel = False\n",
    "inspect_interval = 10\n",
    "save_interval = 50\n",
    "num_backwards = 1\n",
    "is_oracle = (exp_mode == \"oracle\")\n",
    "if is_oracle:\n",
    "    input_size += z_size\n",
    "    oracle_size = z_size\n",
    "else:\n",
    "    oracle_size = None\n",
    "print(\"exp_mode: {0}\".format(exp_mode))\n",
    "\n",
    "# Obtain tasks:\n",
    "assert len(task_id_list) == 1\n",
    "dataset_filename = dataset_PATH + task_id_list[0] + \"_{0}-shot.p\".format(num_shots)\n",
    "tasks = pickle.load(open(dataset_filename, \"rb\"))\n",
    "tasks_train = get_torch_tasks(tasks[\"tasks_train\"], task_id_list[0], num_forward_steps = forward_steps[-1], is_oracle = is_oracle, is_cuda = is_cuda)\n",
    "tasks_test = get_torch_tasks(tasks[\"tasks_test\"], task_id_list[0], start_id = num_train_tasks, num_tasks = num_test_tasks, num_forward_steps = forward_steps[-1], is_oracle = is_oracle, is_cuda = is_cuda)\n",
    "\n",
    "# Obtain autoencoder:\n",
    "aux_coeff = 0.3\n",
    "if is_autoencoder:\n",
    "    encoder_struct_param = [\n",
    "        [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "        [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "        [32, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "    ]\n",
    "    settings = settings = {\"activation\": \"relu\"}\n",
    "    decoder_struct_param = [\n",
    "        [32, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "        [32, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "        [1, \"ConvTranspose2d\", {\"kernel_size\": 3, \"stride\": 2}],\n",
    "    ]\n",
    "\n",
    "    autoencoder = Conv_Autoencoder(\n",
    "        encoder_struct_param,\n",
    "        decoder_struct_param,\n",
    "        enc_flatten_size = 512,\n",
    "        latent_size = 2,\n",
    "        settings = {\"activation\": \"leakyReluFlat\"},\n",
    "        is_cuda = is_cuda,\n",
    "    )\n",
    "else:\n",
    "    autoencoder = None\n",
    "        \n",
    "        \n",
    "# Obtain nets:\n",
    "all_keys = list(tasks_train.keys()) + list(tasks_test.keys())\n",
    "data_record = {\"loss\": {key: [] for key in all_keys}, \"loss_sampled\": {key: [] for key in all_keys}, \"mse\": {key: [] for key in all_keys},\n",
    "               \"reg\": {key: [] for key in all_keys}, \"KLD\": {key: [] for key in all_keys}}\n",
    "reg_multiplier = np.linspace(0, 1, num_iter + 1) ** 2\n",
    "if exp_mode in [\"meta\"]:\n",
    "    struct_param_pre = [[struct_param_pre_neurons[0], \"Simple_Layer\", {}] for _ in range(struct_param_pre_neurons[1])]\n",
    "    struct_param_pre.append([pre_pooling_neurons, \"Simple_Layer\", {\"activation\": \"linear\"}])\n",
    "    struct_param_post = None\n",
    "    struct_param_gen_base = [[struct_param_gen_base_neurons[0], \"Simple_Layer\", {}] for _ in range(struct_param_gen_base_neurons[1])]\n",
    "    statistics_Net, generative_Net, generative_Net_logstd = get_nets(input_size = input_size, output_size = output_size, \n",
    "                                                                      target_size = len(forward_steps) * output_size, main_hidden_neurons = main_hidden_neurons,\n",
    "                                                                      pre_pooling_neurons = pre_pooling_neurons, statistics_output_neurons = statistics_output_neurons, num_context_neurons = num_context_neurons,\n",
    "                                                                      struct_param_pre = struct_param_pre,\n",
    "                                                                      struct_param_gen_base = struct_param_gen_base,\n",
    "                                                                      activation_statistics = activation_gen,\n",
    "                                                                      activation_generative = activation_gen,\n",
    "                                                                      activation_model = activation_model,\n",
    "                                                                      statistics_pooling = statistics_pooling,\n",
    "                                                                      isParallel = isParallel,\n",
    "                                                                      is_VAE = is_VAE,\n",
    "                                                                      is_uncertainty_net = is_uncertainty_net,\n",
    "                                                                      is_cuda = is_cuda,\n",
    "                                                                     )\n",
    "    if is_regulated_net:\n",
    "        struct_param_regulated_Net = [[num_neurons, \"Simple_Layer\", {}] for num_neurons in main_hidden_neurons]\n",
    "        struct_param_regulated_Net.append([1, \"Simple_Layer\", {\"activation\": \"linear\"}])\n",
    "        generative_Net = Net(input_size = input_size, struct_param = struct_param_regulated_Net, settings = {\"activation\": activation_model})\n",
    "    master_model = Master_Model(statistics_Net, generative_Net, generative_Net_logstd, is_cuda = is_cuda)\n",
    "    \n",
    "    all_parameter_list = [statistics_Net.parameters(), generative_Net.parameters()]\n",
    "    if is_uncertainty_net:\n",
    "        all_parameter_list.append(generative_Net_logstd.parameters())\n",
    "    if is_autoencoder:\n",
    "        all_parameter_list.append(autoencoder.parameters())\n",
    "    optimizer = optim.Adam(chain.from_iterable(all_parameter_list), lr = lr)\n",
    "    reg_dict = {\"statistics_Net\": {\"weight\": reg_amp, \"bias\": reg_amp},\n",
    "                \"generative_Net\": {\"weight\": reg_amp, \"bias\": reg_amp, \"W_gen\": reg_amp, \"b_gen\": reg_amp},\n",
    "                \"autoencoder\": {\"weight\": 1e-7, \"bias\": 1e-7},\n",
    "               }\n",
    "    record_data(data_record, [struct_param_gen_base, struct_param_pre, struct_param_post], [\"struct_param_gen_base\", \"struct_param_pre\", \"struct_param_post\"])\n",
    "    model = None\n",
    "\n",
    "elif exp_mode in [\"finetune\", \"oracle\"]:\n",
    "    struct_param_net = [[num_neurons, \"Simple_Layer\", {}] for num_neurons in main_hidden_neurons]\n",
    "    struct_param_net.append([output_size, \"Simple_Layer\", {\"activation\": \"linear\"}])\n",
    "    record_data(data_record, [struct_param_net], [\"struct_param_net\"])\n",
    "    model = Net(input_size = input_size,\n",
    "                  struct_param = struct_param_net,\n",
    "                  settings = {\"activation\": activation_model},\n",
    "                  is_cuda = is_cuda,\n",
    "                 )\n",
    "    reg_dict = {\"net\": {\"weight\": reg_amp, \"bias\": reg_amp},\n",
    "                \"autoencoder\": {\"weight\": 1e-7, \"bias\": 1e-7},\n",
    "               }\n",
    "    all_parameter_list = [model.parameters()]\n",
    "    if is_autoencoder:\n",
    "        all_parameter_list.append(autoencoder.parameters())\n",
    "    optimizer = optim.Adam(chain.from_iterable(all_parameter_list), lr = lr)\n",
    "    statistics_Net = None\n",
    "    generative_Net = None\n",
    "    generative_Net_logstd = None\n",
    "    master_model = None\n",
    "\n",
    "# Setting up optimizer and loss functions:\n",
    "forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "if is_cuda:\n",
    "    forward_steps_idx = forward_steps_idx.cuda()\n",
    "if loss_core == \"mse\":\n",
    "    loss_fun_core = nn.MSELoss(size_average = True)\n",
    "elif loss_core == \"huber\":\n",
    "    loss_fun_core = nn.SmoothL1Loss(size_average = True) \n",
    "else:\n",
    "    raise\n",
    "if is_VAE:\n",
    "    criterion = VAE_Loss(criterion = loss_fun_core, prior = \"Gaussian\", beta = VAE_beta)\n",
    "else:\n",
    "    if is_uncertainty_net:\n",
    "        criterion = Loss_with_uncertainty(core = loss_core)\n",
    "    else:\n",
    "        if is_autoencoder:\n",
    "            criterion = Loss_with_autoencoder(core = loss_core, forward_steps = forward_steps, aux_coeff = aux_coeff, is_cuda = is_cuda)\n",
    "        else:\n",
    "            criterion = loss_fun_core\n",
    "early_stopping = Early_Stopping(patience = patience)\n",
    "\n",
    "filename = variational_model_PATH + \"/trained_models/{0}/ENet_{1}_{2}_input_{3}_({4},{5})_stat_{6}_pre_{7}_pool_{8}_context_{9}_hid_{10}_{11}_{12}_VAE_{13}_{14}_uncer_{15}_lr_{16}_reg_{17}_actgen_{18}_actmodel_{19}_{20}_core_{21}_pat_{22}_for_{23}_{24}_\".format(\n",
    "    exp_id, exp_mode, task_id_list, input_size, num_train_tasks, num_test_tasks, statistics_output_neurons, pre_pooling_neurons, statistics_pooling, num_context_neurons, main_hidden_neurons, struct_param_pre_neurons, struct_param_gen_base_neurons, is_VAE, VAE_beta, is_uncertainty_net, lr, reg_amp, activation_gen, activation_model, optim_mode, loss_core, patience, forward_steps[-1], exp_id)\n",
    "make_dir(filename)\n",
    "print(filename)\n",
    "\n",
    "# Setting up recordings:\n",
    "info_dict = {\"array_id\": array_id}\n",
    "info_dict[\"data_record\"] = data_record\n",
    "info_dict[\"model_dict\"] = []\n",
    "record_data(data_record, [exp_id, task_id_list, task_settings, reg_dict, is_uncertainty_net, lr, pre_pooling_neurons, num_backwards, batch_size_task, \n",
    "                          statistics_pooling, activation_gen, activation_model], \n",
    "            [\"exp_id\", \"task_id_list\", \"task_settings\", \"reg_dict\", \"is_uncertainty_net\", \"lr\", \"pre_pooling_neurons\", \"num_backwards\", \"batch_size_task\",\n",
    "             \"statistics_pooling\", \"activation_gen\", \"activation_model\"])\n",
    "\n",
    "filename = variational_model_PATH + \"/trained_models/{0}/Net_{1}_{2}_input_{3}_({4},{5})_stat_{6}_pre_{7}_pool_{8}_context_{9}_hid_{10}_{11}_{12}_VAE_{13}_{14}_uncer_{15}_lr_{16}_reg_{17}_actgen_{18}_actmodel_{19}_{20}_core_{21}_pat_{22}_{23}_\".format(\n",
    "    exp_id, exp_mode, task_id_list, input_size, num_train_tasks, num_test_tasks, statistics_output_neurons, pre_pooling_neurons, statistics_pooling, num_context_neurons, main_hidden_neurons, struct_param_pre_neurons, struct_param_gen_base_neurons, is_VAE, VAE_beta, is_uncertainty_net, lr, reg_amp, activation_gen, activation_model, optim_mode, loss_core, patience, exp_id)\n",
    "make_dir(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the autoencoder for a few epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_pre = 30\n",
    "batch_size = 128\n",
    "lr_pre = 1e-3\n",
    "reg_amp_autoencoder = 1e-7\n",
    "reg_amp_latent = 1e-2\n",
    "\n",
    "(X_train_all, y_train_all), (X_test_all, y_test_all) = combine_dataset(tasks_train, num = 50)\n",
    "optimizer_pre = optim.Adam(autoencoder.parameters(), lr = lr_pre)\n",
    "train_loader = data_utils.DataLoader(X_train_all.data, batch_size = batch_size, shuffle = True)\n",
    "early_stopping_pre = Early_Stopping(patience = patience_pre)\n",
    "to_stop = False\n",
    "\n",
    "print(\"Pre-train autoencoder:\")\n",
    "for epoch in range(11):\n",
    "    to_stop = train_epoch_pretrain(train_loader, X_test_all, autoencoder, optimizer_pre, isplot = isplot)\n",
    "    if to_stop:\n",
    "        print(\"Early stopping at iteration {0}\".format(i))\n",
    "        break\n",
    "print(\"Pretrain completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training:\n",
    "for i in range(num_iter + 1):\n",
    "    \"\"\"Training the meta-autoencoder\"\"\"\n",
    "    chosen_task_keys = np.random.choice(list(tasks_train.keys()), batch_size_task, replace = False).tolist()\n",
    "    if optim_mode == \"indi\":\n",
    "        if is_VAE:\n",
    "            KLD_total = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "            if is_cuda:\n",
    "                KLD_total = KLD_total.cuda()\n",
    "        for task_key, task in tasks_train.items():\n",
    "            if task_key not in chosen_task_keys:\n",
    "                continue\n",
    "            if is_autoencoder:\n",
    "                ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), z_info = task\n",
    "                X_train = forward(autoencoder.encode, X_train_obs)\n",
    "                y_train = forward(autoencoder.encode, y_train_obs[:, forward_steps_idx])\n",
    "                X_test = forward(autoencoder.encode, X_test_obs)\n",
    "                y_test = forward(autoencoder.encode, y_test_obs[:, forward_steps_idx])\n",
    "                if is_oracle:\n",
    "                    z_train = Variable(torch.FloatTensor(np.repeat(np.expand_dims(z_info[\"z\"],0), len(X_train), 0)), requires_grad = False)\n",
    "                    z_test = Variable(torch.FloatTensor(np.repeat(np.expand_dims(z_info[\"z\"],0), len(X_test), 0)), requires_grad = False)\n",
    "                    if is_cuda:\n",
    "                        z_train = z_train.cuda()\n",
    "                        z_test = z_test.cuda()\n",
    "                    X_train = torch.cat([X_train, z_train], 1)\n",
    "                    X_test = torch.cat([X_test, z_test], 1)\n",
    "            else:\n",
    "                ((X_train, y_train), (X_test, y_test)), z_info = task\n",
    "            \n",
    "            for k in range(num_backwards):\n",
    "                optimizer.zero_grad()\n",
    "                if master_model is not None:\n",
    "                    results = master_model.get_predictions(X_test = X_test, X_train = X_train, y_train = y_train, is_time_series = is_time_series, \n",
    "                                                           is_VAE = is_VAE, is_uncertainty_net = is_uncertainty_net, is_regulated_net = is_regulated_net, forward_steps = forward_steps)\n",
    "                else:\n",
    "                    results = {}\n",
    "                    results[\"y_pred\"] = get_forward_pred(model, X_test, forward_steps, is_time_series = is_time_series, jump_step = 2, is_flatten = True, oracle_size = oracle_size)\n",
    "                if is_VAE:\n",
    "                    statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                    statistics = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "                    if is_regulated_net:\n",
    "                        statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "                    y_pred = generative_Net(X_test, statistics)\n",
    "                    loss, KLD = criterion(y_pred, y_test, mu = statistics_mu, logvar = statistics_logvar)\n",
    "                    KLD_total = KLD_total + KLD\n",
    "                else:\n",
    "                    if is_uncertainty_net:\n",
    "                        statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                        y_pred = generative_Net(X_test, statistics_mu)\n",
    "                        y_pred_logstd = generative_Net_logstd(X_test, statistics_logvar)\n",
    "                        loss = criterion(y_pred, y_test, log_std = y_pred_logstd)\n",
    "                    else:\n",
    "                        if is_autoencoder:\n",
    "                            loss = criterion(X_test, results[\"y_pred\"], X_test_obs, y_test_obs, autoencoder, verbose = False, oracle_size = oracle_size)\n",
    "                        else:\n",
    "                            loss = criterion(results[\"y_pred\"], y_test)\n",
    "                reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net, net = model, autoencoder = autoencoder, is_cuda = is_cuda)\n",
    "                loss = loss + reg * reg_multiplier[i]\n",
    "                loss.backward(retain_graph = True)\n",
    "                optimizer.step()\n",
    "        # Perform gradient on the KL-divergence:\n",
    "        if is_VAE:\n",
    "            KLD_total = KLD_total / batch_size_task\n",
    "            optimizer.zero_grad()\n",
    "            KLD_total.backward()\n",
    "            optimizer.step()\n",
    "            record_data(data_record, [KLD_total], [\"KLD_total\"])\n",
    "    elif optim_mode == \"sum\":\n",
    "        optimizer.zero_grad()\n",
    "        loss_total = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            loss_total = loss_total.cuda()\n",
    "        for task_key, task in tasks_train.items():\n",
    "            if task_key not in chosen_task_keys:\n",
    "                continue\n",
    "            if is_autoencoder:\n",
    "                ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), _ = task\n",
    "                X_train = forward(autoencoder.encode, X_train_obs)\n",
    "                y_train = forward(autoencoder.encode, y_train_obs)\n",
    "                X_test = forward(autoencoder.encode, X_test_obs)\n",
    "                y_test = forward(autoencoder.encode, y_test_obs)\n",
    "            else:\n",
    "                ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "            if master_model is not None:\n",
    "                results = master_model.get_predictions(X_test = X_test, X_train = X_train, y_train = y_train, is_time_series = is_time_series, \n",
    "                                                       is_VAE = is_VAE, is_uncertainty_net = is_uncertainty_net, is_regulated_net = is_regulated_net, forward_steps = forward_steps)\n",
    "            else:\n",
    "                results = {}\n",
    "                results[\"y_pred\"] = get_forward_pred(model, X_test, forward_steps, is_time_series = is_time_series, jump_step = 2, is_flatten = True, oracle_size = oracle_size)\n",
    "            if is_VAE:\n",
    "                statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                statistics = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "                y_pred = generative_Net(X_test, statistics)\n",
    "                loss, KLD = criterion(y_pred, y_test, mu = statistics_mu, logvar = statistics_logvar)\n",
    "                loss = loss + KLD\n",
    "            else:\n",
    "                if is_uncertainty_net:\n",
    "                    statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                    y_pred = generative_Net(X_test, statistics_mu)\n",
    "                    y_pred_logstd = generative_Net_logstd(X_test, statistics_logvar)\n",
    "                    loss = criterion(y_pred, y_test, log_std = y_pred_logstd)\n",
    "                else:\n",
    "                    if is_autoencoder:\n",
    "                        loss = criterion(X_test, results[\"y_pred\"], X_test_obs, y_test_obs, autoencoder, verbose = False, oracle_size = oracle_size)\n",
    "                    else:\n",
    "                        loss = criterion(results[\"y_pred\"], y_test)\n",
    "            reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net, autoencoder = autoencoder, net = model, is_cuda = is_cuda)\n",
    "            loss_total = loss_total + loss + reg * reg_multiplier[i]\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        raise Exception(\"optim_mode {0} not recognized!\".format(optim_mode))    \n",
    "\n",
    "    loss_test_record = []\n",
    "    for task_key, task in tasks_test.items():\n",
    "        loss_test, _, _, _ = evaluate(task, master_model = master_model, model = model, criterion = criterion, oracle_size = oracle_size, is_VAE = is_VAE, is_regulated_net = is_regulated_net, autoencoder = autoencoder, forward_steps = forward_steps)\n",
    "        loss_test_record.append(loss_test)\n",
    "    to_stop = early_stopping.monitor(np.mean(loss_test_record))\n",
    "\n",
    "    # Validation and visualization:\n",
    "    if i % inspect_interval == 0 or to_stop:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"training tasks:\")\n",
    "        for task_key, task in tasks_train.items():\n",
    "            loss_test, loss_test_sampled, mse, KLD_test = evaluate(task, master_model = master_model, model = model, criterion = criterion,  oracle_size = oracle_size, is_VAE = is_VAE, is_regulated_net = is_regulated_net, autoencoder = autoencoder, forward_steps = forward_steps)\n",
    "            reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net, autoencoder = autoencoder, net = model, is_cuda = is_cuda).data[0] * reg_multiplier[i]\n",
    "            data_record[\"loss\"][task_key].append(loss_test)\n",
    "            data_record[\"loss_sampled\"][task_key].append(loss_test_sampled)\n",
    "            data_record[\"mse\"][task_key].append(mse)\n",
    "            data_record[\"reg\"][task_key].append(reg)\n",
    "            data_record[\"KLD\"][task_key].append(KLD_test)\n",
    "            print('{0}\\ttrain\\t{1}  \\tloss: {2:.5f}\\tloss_sampled:{3:.5f} \\tmse:{4:.5f}\\tKLD:{5:.6f}\\treg:{6:.6f}'.format(i, task_key, loss_test, loss_test_sampled, mse, KLD_test, reg))\n",
    "        for task_key, task in tasks_test.items():\n",
    "            loss_test, loss_test_sampled, mse, KLD_test = evaluate(task, master_model = master_model, model = model, criterion = criterion,  oracle_size = oracle_size, is_VAE = is_VAE, is_regulated_net = is_regulated_net, autoencoder = autoencoder, forward_steps = forward_steps)\n",
    "            reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net, autoencoder = autoencoder, net = model, is_cuda = is_cuda).data[0] * reg_multiplier[i]\n",
    "            data_record[\"loss\"][task_key].append(loss_test)\n",
    "            data_record[\"loss_sampled\"][task_key].append(loss_test_sampled)\n",
    "            data_record[\"mse\"][task_key].append(mse)\n",
    "            data_record[\"reg\"][task_key].append(reg)\n",
    "            data_record[\"KLD\"][task_key].append(KLD_test)\n",
    "            print('{0}\\ttest\\t{1}  \\tloss: {2:.5f}\\tloss_sampled:{3:.5f} \\tmse:{4:.5f}\\tKLD:{5:.6f}\\treg:{6:.6f}'.format(i, task_key, loss_test, loss_test_sampled, mse, KLD_test, reg))\n",
    "        loss_train_list = [data_record[\"loss\"][task_key][-1] for task_key in tasks_train]\n",
    "        loss_test_list = [data_record[\"loss\"][task_key][-1] for task_key in tasks_test]\n",
    "        loss_train_sampled_list = [data_record[\"loss_sampled\"][task_key][-1] for task_key in tasks_train]\n",
    "        loss_test_sampled_list = [data_record[\"loss_sampled\"][task_key][-1] for task_key in tasks_test]\n",
    "        mse_train_list = [data_record[\"mse\"][task_key][-1] for task_key in tasks_train]\n",
    "        mse_test_list = [data_record[\"mse\"][task_key][-1] for task_key in tasks_test]\n",
    "        reg_train_list = [data_record[\"reg\"][task_key][-1] for task_key in tasks_train]\n",
    "        reg_test_list = [data_record[\"reg\"][task_key][-1] for task_key in tasks_test]\n",
    "        mse_few_shot = plot_few_shot_loss(master_model, tasks_test, isplot = isplot, autoencoder = autoencoder, forward_steps = forward_steps, criterion = criterion)\n",
    "        record_data(data_record, \n",
    "                    [np.mean(loss_train_list), np.median(loss_train_list), np.mean(reg_train_list), i,\n",
    "                     np.mean(loss_test_list), np.median(loss_test_list), np.mean(reg_test_list),\n",
    "                     np.mean(loss_train_sampled_list), np.median(loss_train_sampled_list), \n",
    "                     np.mean(loss_test_sampled_list), np.median(loss_test_sampled_list),\n",
    "                     np.mean(mse_train_list), np.median(mse_train_list), \n",
    "                     np.mean(mse_test_list), np.median(mse_test_list), \n",
    "                     mse_few_shot,\n",
    "                    ], \n",
    "                    [\"loss_mean_train\", \"loss_median_train\", \"reg_mean_train\", \"iter\",\n",
    "                     \"loss_mean_test\", \"loss_median_test\", \"reg_mean_test\",\n",
    "                     \"loss_sampled_mean_train\", \"loss_sampled_median_train\",\n",
    "                     \"loss_sampled_mean_test\", \"loss_sampled_median_test\", \n",
    "                     \"mse_mean_train\", \"mse_median_train\", \"mse_mean_test\", \"mse_median_test\", \n",
    "                     \"mse_few_shot\",\n",
    "                    ])\n",
    "        if isplot:\n",
    "            plot_data_record(data_record, idx = -1, is_VAE = is_VAE, tasks_train_keys = tasks_train.keys(), tasks_test_keys = tasks_test.keys())\n",
    "        print(\"Summary:\")\n",
    "        print('\\n{0}\\ttrain\\tloss_mean: {1:.5f}\\tloss_median: {2:.5f}\\tmse_mean: {3:.6f}\\tmse_median: {4:.6f}\\treg: {5:.6f}'.format(i, data_record[\"loss_mean_train\"][-1], data_record[\"loss_median_train\"][-1], data_record[\"mse_mean_train\"][-1], data_record[\"mse_median_train\"][-1], data_record[\"reg_mean_train\"][-1]))\n",
    "        print('{0}\\ttest\\tloss_mean: {1:.5f}\\tloss_median: {2:.5f}\\tmse_mean: {3:.6f}\\tmse_median: {4:.6f}\\treg: {5:.6f}'.format(i, data_record[\"loss_mean_test\"][-1], data_record[\"loss_median_test\"][-1], data_record[\"mse_mean_test\"][-1], data_record[\"mse_median_test\"][-1], data_record[\"reg_mean_test\"][-1]))\n",
    "        if is_VAE and \"KLD_total\" in locals():\n",
    "            print(\"KLD_total: {0:.5f}\".format(KLD_total.data[0]))\n",
    "        if isplot:\n",
    "            plot_data_record(data_record, is_VAE = is_VAE, tasks_train_keys = tasks_train.keys(), tasks_test_keys = tasks_test.keys())\n",
    "\n",
    "        # Plotting y_pred vs. y_target:\n",
    "        statistics_list_train, z_list_train = plot_task_ensembles(tasks_train, master_model = master_model, model = model, is_VAE = is_VAE, is_oracle = is_oracle, is_regulated_net = is_regulated_net, autoencoder = autoencoder, title = \"y_pred_train vs. y_train\", isplot = isplot, forward_steps = forward_steps, )\n",
    "        statistics_list_test, z_list_test = plot_task_ensembles(tasks_test, master_model = master_model, model = model, is_VAE = is_VAE, is_oracle = is_oracle, is_regulated_net = is_regulated_net, autoencoder = autoencoder, title = \"y_pred_test vs. y_test\", isplot = isplot, forward_steps = forward_steps, )\n",
    "        record_data(data_record, [np.array(z_list_train), np.array(z_list_test), np.array(statistics_list_train), np.array(statistics_list_test)], \n",
    "                    [\"z_list_train_list\", \"z_list_test_list\", \"statistics_list_train_list\", \"statistics_list_test_list\"])\n",
    "        if isplot:\n",
    "            print(\"train statistics vs. z:\")\n",
    "            plot_statistics_vs_z(z_list_train, statistics_list_train)\n",
    "            print(\"test statistics vs. z:\")\n",
    "            plot_statistics_vs_z(z_list_test, statistics_list_test)\n",
    "\n",
    "            # Plotting individual test data:\n",
    "            if \"bounce\" in task_id_list[0]:\n",
    "                if \"bounce-images\" in task_id_list[0]:\n",
    "                    plot_tasks(tasks_test, master_model = master_model, model = model, autoencoder = autoencoder, forward_steps = forward_steps, num_tasks = min(3, num_test_tasks), oracle_size=oracle_size)\n",
    "                plot_individual_tasks_bounce(tasks_test, num_examples_show = 40, num_tasks_show = 6, master_model = master_model, model = model, autoencoder = autoencoder, num_shots = 200, target_forward_steps = len(forward_steps), eval_forward_steps = len(forward_steps))\n",
    "            else:\n",
    "                print(\"train tasks:\")\n",
    "                plot_individual_tasks(tasks_train, master_model = master_model, model = model, is_VAE = is_VAE, is_regulated_net = is_regulated_net, xlim = task_settings[\"xlim\"])\n",
    "                print(\"test tasks:\")\n",
    "                plot_individual_tasks(tasks_test, master_model = master_model, model = model, is_VAE = is_VAE, is_regulated_net = is_regulated_net, xlim = task_settings[\"xlim\"])\n",
    "        print(\"=\" * 50 + \"\\n\\n\")\n",
    "        try:\n",
    "            sys.stdout.flush()\n",
    "        except:\n",
    "            pass\n",
    "    if i % save_interval == 0 or to_stop:\n",
    "        pickle.dump(info_dict, open(filename + \"info.p\", \"wb\"))\n",
    "        make_dir(filename[:-1] + \"/conv-meta_master-model\")\n",
    "        model_save = master_model if master_model is not None else model\n",
    "        torch.save(autoencoder.state_dict(), filename[:-1] + \"/conv-meta_autoencoder_{0}.p\".format(i))\n",
    "        torch.save(model_save.state_dict(), filename[:-1] + \"/conv-meta_model_{0}.p\".format(i))\n",
    "    if to_stop:\n",
    "        print(\"The training loss stops decreasing for {0} steps. Early stopping at {1}.\".format(patience, i))\n",
    "        break\n",
    "\n",
    "\n",
    "# Plotting:\n",
    "if isplot:\n",
    "    for task_key in tasks_train:\n",
    "        plt.semilogy(data_record[\"loss\"][task_key], alpha = 0.6)\n",
    "    plt.show()\n",
    "    for task_key in tasks_test:\n",
    "        plt.semilogy(data_record[\"loss\"][task_key], alpha = 0.6)\n",
    "    plt.show()\n",
    "print(\"completed\")\n",
    "sys.stdout.flush()\n",
    "pickle.dump(info_dict, open(filename + \"info.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
