{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from task import OmniglotTask, MNISTTask\n",
    "from dataset import Omniglot, MNIST\n",
    "from data_loading import get_data_loader\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', \"..\"))\n",
    "from mela.variational.variational_meta_learning import Statistics_Net_Conv, Generative_Net_Conv, Master_Model, load_model_dict\n",
    "from mela.pytorch.util_pytorch import get_num_params, to_Variable, to_np_array\n",
    "from mela.util import plot_matrices, make_dir\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_task(root, n_cl, n_inst, split='train'):\n",
    "    if 'mnist' in root:\n",
    "        return MNISTTask(root, n_cl, n_inst, split)\n",
    "    elif 'omniglot' in root:\n",
    "        return OmniglotTask(root, n_cl, n_inst, split)\n",
    "    else:\n",
    "        print('Unknown dataset')\n",
    "        raise(Exception)\n",
    "\n",
    "def get_metrics(master_model, X_test, y_test, loss_fn):\n",
    "    y_logit = master_model(X_test)\n",
    "    y_pred = y_logit.max(1)[1]\n",
    "    acc = (to_np_array((y_test == y_pred).float().sum()) / len(y_test))[0]\n",
    "    loss = to_np_array(loss_fn(y_logit, y_test))[0]\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MeLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_params: 1819135\n"
     ]
    }
   ],
   "source": [
    "pre_pooling_neurons = 200\n",
    "statistics_output_neurons = 20\n",
    "num_classes = 5\n",
    "input_channels = 3\n",
    "activation_default = \"leakyRelu\"\n",
    "activation_conv = \"leakyReluFlat\"\n",
    "\n",
    "struct_param_pre_conv = [\n",
    "    [8, \"Conv2d\", {\"kernel_size\": 4, \"stride\": 2, \"activation\": activation_conv}],\n",
    "#     [None, \"MaxPool2d\", {\"kernel_size\": 2, \"return_indices\": False}],\n",
    "    [4, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 1, \"activation\": activation_conv}],\n",
    "    [4, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 1, \"activation\": activation_conv}],\n",
    "    [40, \"Simple_Layer\", {\"activation\": \"linear\", \"layer_input_size\": 324}]\n",
    "]\n",
    "struct_param_pre = [[60, \"Simple_Layer\", {\"activation\": activation_default}],\n",
    "                    [pre_pooling_neurons, \"Simple_Layer\", {\"activation\": \"linear\"}],\n",
    "                   ]\n",
    "struct_param_post = [[60, \"Simple_Layer\", {\"activation\": activation_default}],\n",
    "                     [60, \"Simple_Layer\", {\"activation\": activation_default}],\n",
    "                     [statistics_output_neurons, \"Simple_Layer\", {\"activation\": \"linear\"}],\n",
    "                    ]\n",
    "struct_param_gen_base = [[40, \"Simple_Layer\", {\"activation\": activation_default}],\n",
    "                         [10, \"Simple_Layer\", {\"activation\": \"linear\"}],\n",
    "                        ]\n",
    "struct_param_model = [[64, \"Conv2d\", {\"kernel_size\": 3, \"stride\": 1, \"dilation\": 2}], \n",
    "                      [64, \"BatchNorm2d\", {\"activation\": activation_conv}],\n",
    "                     ] * 4 + \\\n",
    "                     [[num_classes, \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "main_weight_neurons = [3*64*3*3, 64, 64*64*3*3, 64, 64*64*3*3, 64, 64*64*3*3, 64, 9216 * 5]\n",
    "main_bias_neurons = [64, 64, 64, 64, 64, 64, 64, 64, 5]\n",
    "\n",
    "W_struct_param_list = []\n",
    "b_struct_param_list = []\n",
    "for i, num_weight_neurons in enumerate(main_weight_neurons):\n",
    "    struct_param_weight = struct_param_gen_base + [[num_weight_neurons, \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "    struct_param_bias = struct_param_gen_base + [[main_bias_neurons[i], \"Simple_Layer\", {\"activation\": \"linear\"}]]\n",
    "    W_struct_param_list.append(struct_param_weight)\n",
    "    b_struct_param_list.append(struct_param_bias)\n",
    "\n",
    "statistics_Net_Conv = Statistics_Net_Conv(input_channels = input_channels,\n",
    "                                          num_classes = num_classes,\n",
    "                                          pre_pooling_neurons = pre_pooling_neurons,\n",
    "                                          struct_param_pre_conv = struct_param_pre_conv, \n",
    "                                          struct_param_pre = struct_param_pre,\n",
    "                                          struct_param_post = struct_param_post,\n",
    "                                          is_cuda = is_cuda,\n",
    "                                         )\n",
    "generative_Net_Conv = Generative_Net_Conv(input_channels = input_channels,\n",
    "                                          latent_size = statistics_output_neurons,\n",
    "                                          W_struct_param_list = W_struct_param_list,\n",
    "                                          b_struct_param_list = b_struct_param_list,\n",
    "                                          struct_param_model = struct_param_model,\n",
    "                                          is_cuda = is_cuda,\n",
    "                                         )\n",
    "master_model = Master_Model(statistics_Net = statistics_Net_Conv, generative_Net = generative_Net_Conv, is_cuda = is_cuda)\n",
    "print(\"Num_params: {0}\".format(get_num_params(master_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_mode = \"indi\"\n",
    "dataset='omniglot'\n",
    "num_inst=6\n",
    "meta_batch_size=20\n",
    "num_updates=15000\n",
    "lr=1e-1\n",
    "meta_lr=1e-3\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "reg_amp = 1e-9\n",
    "exp='maml-omniglot-{0}way-{1}shot-TEST'.format(num_classes, num_inst)\n",
    "make_dir(\"output/{0}/\".format(exp))\n",
    "\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "tr_loss, tr_acc, val_loss, val_acc = [], [], [], []\n",
    "mtr_loss, mtr_acc, mval_loss, mval_acc = [], [], [], []\n",
    "reg_list = []\n",
    "\n",
    "optimizer = torch.optim.Adam(master_model.parameters(), lr = meta_lr)\n",
    "for i in range(num_updates):\n",
    "    # Evaluate on test tasks\n",
    "#     mt_loss, mt_acc, mv_loss, mv_acc = test()\n",
    "#     mtr_loss.append(mt_loss)\n",
    "#     mtr_acc.append(mt_acc)\n",
    "#     mval_loss.append(mv_loss)\n",
    "#     mval_acc.append(mv_acc)\n",
    "    # Collect a meta batch update\n",
    "    grads = []\n",
    "    tloss, tacc, vloss, vacc, reg_batch = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    if optim_mode == \"indi\":\n",
    "        for k in range(meta_batch_size):\n",
    "            # Get data:\n",
    "            task = get_task('../data/{}'.format(dataset), num_classes, num_inst)\n",
    "            train_loader = get_data_loader(task, batch_size = num_inst, split='train')\n",
    "            val_loader = get_data_loader(task, batch_size = num_inst, split='val')\n",
    "            X_train, y_train = train_loader.__iter__().next()\n",
    "            X_test, y_test = val_loader.__iter__().next()\n",
    "            X_train, y_train, X_test, y_test = to_Variable(X_train, y_train, X_test, y_test, is_cuda = is_cuda)\n",
    "            \n",
    "            # Get gradient:\n",
    "            optimizer.zero_grad()\n",
    "            results = master_model.get_predictions(X_test, X_train, y_train, is_time_series = False)\n",
    "            loss = loss_fn(results[\"y_pred\"], y_test)\n",
    "            reg = master_model.get_regularization(source = [\"weight\", \"bias\", \"W_gen\", \"b_gen\"], target = [\"statistics_Net\", \"generative_Net\"]) * reg_amp\n",
    "            loss = loss + reg\n",
    "            loss.backward()\n",
    "            \n",
    "            # Get metrics:\n",
    "            master_model.get_statistics(X_train, y_train)\n",
    "            trl, tra = get_metrics(master_model, X_train, y_train, loss_fn)\n",
    "            vall, vala = get_metrics(master_model, X_test, y_test, loss_fn)\n",
    "            tloss += trl\n",
    "            tacc += tra\n",
    "            vloss += vall\n",
    "            vacc += vala\n",
    "            reg_batch += to_np_array(reg)[0]\n",
    "            \n",
    "            # Gradient descent:\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    elif optim_mode == \"sum\":\n",
    "        optimizer.zero_grad()\n",
    "        loss_total = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            loss_total = loss_total.cuda()\n",
    "        \n",
    "        for i in range(meta_batch_size):\n",
    "            # Get data:\n",
    "            task = get_task('../data/{}'.format(dataset), num_classes, num_inst)\n",
    "            train_loader = get_data_loader(task, batch_size = num_inst, split='train')\n",
    "            val_loader = get_data_loader(task, batch_size = num_inst, split='val')\n",
    "            X_train, y_train = train_loader.__iter__().next()\n",
    "            X_test, y_test = val_loader.__iter__().next()\n",
    "            X_train, y_train, X_test, y_test = to_Variable(X_train, y_train, X_test, y_test, is_cuda = is_cuda)\n",
    "            \n",
    "            # Get single-task loss:\n",
    "            optimizer.zero_grad()\n",
    "            results = master_model.get_predictions(X_test, X_train, y_train, is_time_series = False)\n",
    "            loss = loss_fn(results[\"y_pred\"], y_test)\n",
    "            reg = master_model.get_regularization(source = [\"weight\", \"bias\", \"W_gen\", \"b_gen\"], target = [\"statistics_Net\", \"generative_Net\"]) * reg_amp\n",
    "            loss_total = loss_total + loss + reg\n",
    "\n",
    "            # Get metrics:\n",
    "            master_model.generative_Net.set_latent_param(results[\"statistics\"])\n",
    "            trl, tra = get_metrics(master_model, X_train, y_train, loss_fn)\n",
    "            vall, vala = get_metrics(master_model, X_test, y_test, loss_fn)\n",
    "            tloss += trl\n",
    "            tacc += tra\n",
    "            vloss += vall\n",
    "            vacc += vala\n",
    "            reg_batch += to_np_array(reg)[0]\n",
    "        \n",
    "        # Gradient descient on the sum of loss:\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        raise Exception(\"optim_mode {0} not recognized!\".format(optim_mode))\n",
    "    \n",
    "    # Save stuff\n",
    "    tr_loss.append(tloss / meta_batch_size)\n",
    "    tr_acc.append(tacc / meta_batch_size)\n",
    "    val_loss.append(vloss / meta_batch_size)\n",
    "    val_acc.append(vacc / meta_batch_size)\n",
    "    reg_list.append(reg_batch / meta_batch_size)\n",
    "    \n",
    "    print(\"iter {0}\\ttrain_loss: {1:.4f}\\ttest_loss: {2:.4f}\\ttrain_acc: {3:.4f}\\ttest_acc: {4:.4f}\\treg: {5:.6f}\".format(i, tr_loss[-1], val_loss[-1], tr_acc[-1], val_acc[-1], reg_list[-1]))\n",
    "\n",
    "    np.save('output/{}/tr_loss.npy'.format(exp), np.array(tr_loss))\n",
    "    np.save('output/{}/tr_acc.npy'.format(exp), np.array(tr_acc))\n",
    "    np.save('output/{}/val_loss.npy'.format(exp), np.array(val_loss))\n",
    "    np.save('output/{}/val_acc.npy'.format(exp), np.array(val_acc))\n",
    "\n",
    "#     np.save('output/{}/meta_tr_loss.npy'.format(exp), np.array(mtr_loss))\n",
    "#     np.save('output/{}/meta_tr_acc.npy'.format(exp), np.array(mtr_acc))\n",
    "#     np.save('output/{}/meta_val_loss.npy'.format(exp), np.array(mval_loss))\n",
    "#     np.save('output/{}/meta_val_acc.npy'.format(exp), np.array(mval_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
