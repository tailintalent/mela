{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from AI_scientist.util import plot_matrices, make_dir, get_struct_str, get_args, Early_Stopping, record_data, manifold_embedding\n",
    "from AI_scientist.settings.filepath import dataset_PATH_short\n",
    "from AI_scientist.pytorch.net import Net\n",
    "from AI_scientist.pytorch.util_pytorch import Loss_with_uncertainty\n",
    "from AI_scientist.variational.util_variational import get_numpy_tasks, get_torch_tasks\n",
    "from AI_scientist.variational.variational_meta_learning import Master_Model, Statistics_Net, Generative_Net, load_model_dict, get_regulated_statistics\n",
    "from AI_scientist.variational.variational_meta_learning import VAE_Loss, sample_Gaussian, clone_net, get_nets, get_tasks, evaluate, get_reg, load_trained_models\n",
    "from AI_scientist.variational.variational_meta_learning import plot_task_ensembles, plot_individual_tasks, plot_statistics_vs_z, plot_data_record, get_corrcoef\n",
    "from AI_scientist.variational.variational_meta_learning import plot_few_shot_loss, plot_individual_tasks_bounce\n",
    "from AI_scientist.variational.variational_meta_learning import get_latent_model_data, get_polynomial_class, get_Legendre_class, get_master_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_id = \"C-tanh\"\n",
    "# task_id = \"C-sin\"\n",
    "task_id = \"bounce-states\"\n",
    "# task_id = \"bounce-images\"\n",
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if task_id in [\"C-sin\", \"C-tanh\"]:\n",
    "    num_shots = 10\n",
    "    num_train_tasks = 100\n",
    "    num_test_tasks = 20000\n",
    "elif task_id == \"bounce-states\":\n",
    "    num_shots = 100\n",
    "    num_train_tasks = 100\n",
    "    num_test_tasks = 1000\n",
    "elif task_id == \"bounce-images\":\n",
    "    num_shots = 100\n",
    "    num_train_tasks = 100\n",
    "    num_test_tasks = 100\n",
    "else:\n",
    "    raise\n",
    "task_settings = {\"test_size\": 0.5, \"num_examples\": num_shots * 2}\n",
    "tasks_train, tasks_test = get_tasks([task_id], num_train_tasks, num_test_tasks, task_settings = task_settings, forward_steps = list(range(1,11)))\n",
    "filename = dataset_PATH_short + task_id + \"_{0}-shot.p\".format(num_shots)\n",
    "\n",
    "tasks = {\"tasks_train\": get_numpy_tasks(tasks_train),\n",
    "         \"tasks_test\": get_numpy_tasks(tasks_test),\n",
    "         }\n",
    "pickle.dump(tasks, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load_tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_id = \"C-tanh\"\n",
    "# task_id = \"C-sin\"\n",
    "# task_id = \"bounce-states\"\n",
    "task_id = \"bounce-images\"\n",
    "if task_id in [\"C-sin\", \"C-tanh\"]:\n",
    "    num_shots = 10\n",
    "elif task_id in [\"bounce-states\", \"bounce-images\"]:\n",
    "    num_shots = 100\n",
    "\n",
    "filename = dataset_PATH_short + task_id + \"_{0}-shot.p\".format(num_shots)\n",
    "tasks = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "tasks_train = tasks[\"tasks_train\"]\n",
    "tasks_test = tasks[\"tasks_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with training tasks:\n",
    "for task in tasks_train:\n",
    "    ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "\n",
    "# Validate with testing tasks:\n",
    "# This is only for tanh/sin tasks, where we partition the 20000 testing tasks into 200 x evaluations, and each evaluation has 100 testing tasks:\n",
    "loss_list = []\n",
    "for i in range(100):\n",
    "    tasks_test_iter = tasks_test[i * 100 : (i + 1) * 100]\n",
    "    # Perform evaluation, accumulate the loss:\n",
    "    loss = evaluate(tasks_test_iter)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "# Then obtain mean and std:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
