{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from AI_scientist.util import plot_matrices, make_dir, get_struct_str, get_args, Early_Stopping, record_data, manifold_embedding\n",
    "from AI_scientist.settings.filepath import dataset_PATH\n",
    "from AI_scientist.pytorch.net import Net\n",
    "from AI_scientist.pytorch.util_pytorch import Loss_with_uncertainty\n",
    "from AI_scientist.variational.util_variational import get_numpy_tasks\n",
    "from AI_scientist.variational.variational_meta_learning import Master_Model, Statistics_Net, Generative_Net, load_model_dict, get_regulated_statistics\n",
    "from AI_scientist.variational.variational_meta_learning import VAE_Loss, sample_Gaussian, clone_net, get_nets, get_tasks, evaluate, get_reg, load_trained_models\n",
    "from AI_scientist.variational.variational_meta_learning import plot_task_ensembles, plot_individual_tasks, plot_statistics_vs_z, plot_data_record, get_corrcoef\n",
    "from AI_scientist.variational.variational_meta_learning import plot_few_shot_loss, plot_individual_tasks_bounce\n",
    "from AI_scientist.variational.variational_meta_learning import get_latent_model_data, get_polynomial_class, get_Legendre_class, get_master_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_id = \"C-tanh\"\n",
    "# task_id = \"C-sin\"\n",
    "\n",
    "# seed = 1\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# if task_id == \"C-sin\":\n",
    "#     task_id_list = [\"C-sin\"]\n",
    "#     task_settings = {\"test_size\": 0.5, \"num_examples\": 20}\n",
    "#     num_train_tasks = 50\n",
    "#     num_test_tasks = 5000\n",
    "# elif task_id == \"C-tanh\":\n",
    "#     task_id_list = [\"C-tanh\"]\n",
    "#     task_settings = {\"test_size\": 0.5, \"num_examples\": 20}\n",
    "#     num_train_tasks = 50\n",
    "#     num_test_tasks = 5000\n",
    "# else:\n",
    "#     raise\n",
    "# tasks_train, tasks_test = get_tasks(task_id_list, num_train_tasks, num_test_tasks, task_settings = task_settings)\n",
    "# filename = dataset_PATH + task_id + \".p\"\n",
    "\n",
    "# tasks = {\"tasks_train\": get_numpy_tasks(tasks_train),\n",
    "#          \"tasks_test\": get_numpy_tasks(tasks_test),\n",
    "#          }\n",
    "# pickle.dump(tasks, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load_tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = \"C-tanh\"\n",
    "# task_id = \"C-sin\"\n",
    "\n",
    "filename = dataset_PATH + task_id + \".p\"\n",
    "tasks = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "tasks_train = tasks[\"tasks_train\"]\n",
    "tasks_test = tasks[\"tasks_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with training tasks:\n",
    "for task in tasks_train:\n",
    "    ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "   \n",
    "\n",
    "# Validate with testing tasks:\n",
    "# This is only for tanh/sin tasks, where we partition the 5000 testing tasks into 100 x evaluations, and each evaluation has 50 testing tasks:\n",
    "loss_list = []\n",
    "for i in range(100):\n",
    "    tasks_test_iter = tasks_test[i * 50 : (i + 1) * 50]\n",
    "    # Perform evaluation, accumulate the loss:\n",
    "    loss = evaluate(tasks_test_iter)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "# Then obtain mean and std:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
