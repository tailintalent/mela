{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from mela.prepare_dataset import Dataset_Gen\n",
    "from mela.util import plot_matrices\n",
    "from mela.settings.a2c_env_settings import ENV_SETTINGS_CHOICE\n",
    "from mela.settings.global_param import COLOR_LIST\n",
    "from mela.pytorch.net import Net, ConvNet\n",
    "from mela.pytorch.util_pytorch import get_activation, get_optimizer, get_criterion, Loss_Fun, to_Variable, to_np_array, to_one_hot, flatten\n",
    "from mela.variational.util_variational import sort_datapoints, predict_forward, reshape_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions:\n",
    "class Master_Model(nn.Module):\n",
    "    def __init__(self, statistics_Net = None, generative_Net = None, generative_Net_logstd = None, is_cuda = False):\n",
    "        super(Master_Model, self).__init__()\n",
    "        self.statistics_Net = statistics_Net\n",
    "        self.generative_Net = generative_Net\n",
    "        self.generative_Net_logstd = generative_Net_logstd\n",
    "        self.use_net = \"generative\"\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Master_Model\"}\n",
    "        model_dict[\"statistics_Net\"] = self.statistics_Net.model_dict\n",
    "        model_dict[\"generative_Net\"] = self.generative_Net.model_dict\n",
    "        if self.generative_Net_logstd is not None:\n",
    "            model_dict[\"generative_Net_logstd\"] = self.generative_Net_logstd.model_dict   \n",
    "        return model_dict\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def get_statistics(self, X, y):\n",
    "        statistics = self.statistics_Net(torch.cat([X, y], 1))\n",
    "        if isinstance(statistics, tuple):\n",
    "            statistics = statistics[0]\n",
    "        else:\n",
    "            statistics = statistics\n",
    "        self.generative_Net.set_latent_param(statistics)\n",
    "\n",
    "    def use_clone_net(self, clone_parameters = True):\n",
    "        self.cloned_net = clone_net(self.generative_Net, clone_parameters = clone_parameters)\n",
    "        self.use_net = \"cloned\"\n",
    "    \n",
    "    def get_clone_net(self, X = None, y = None, clone_parameters = True):\n",
    "        if X is not None or y is not None:\n",
    "            self.get_statistics(X, y)\n",
    "        return clone_net(self.generative_Net, clone_parameters = clone_parameters)\n",
    "\n",
    "    def use_generative_net(self):\n",
    "        self.use_net = \"generative\"\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.use_net == \"generative\":\n",
    "            return self.generative_Net(X)\n",
    "        elif self.use_net == \"cloned\":\n",
    "            return self.cloned_net(X)\n",
    "        else:\n",
    "            raise Exception(\"use_net {0} not recognized!\".format(self.use_net))\n",
    "\n",
    "    \n",
    "    def get_predictions(\n",
    "        self,\n",
    "        X_test,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        is_time_series = True,\n",
    "        is_VAE = False,\n",
    "        is_uncertainty_net = False,\n",
    "        is_regulated_net = False,\n",
    "        forward_steps = [1],\n",
    "        ):\n",
    "        results = {}\n",
    "        if is_VAE:\n",
    "            statistics_mu, statistics_logvar = self.statistics_Net.forward_inputs(X_train, y_train)\n",
    "            statistics = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "            results[\"statistics_mu\"] = statistics_mu\n",
    "            results[\"statistics_logvar\"] = statistics_logvar\n",
    "            results[\"statistics\"] = statistics\n",
    "            if is_regulated_net:\n",
    "                statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "                results[\"statistics_feed\"] = statistics\n",
    "            y_pred = self.generative_Net(X_test, statistics)\n",
    "            results[\"y_pred\"] = y_pred\n",
    "        else:\n",
    "            if is_uncertainty_net:\n",
    "                statistics_mu, statistics_logvar = self.statistics_Net.forward_inputs(X_train, y_train)\n",
    "                results[\"statistics_mu\"] = statistics_mu\n",
    "                results[\"statistics_logvar\"] = statistics_logvar\n",
    "                results[\"statistics\"] = statistics\n",
    "                if is_regulated_net:\n",
    "                    statistics_mu = get_regulated_statistics(self.generative_Net, statistics_mu)\n",
    "                    statistics_logvar = get_regulated_statistics(self.generative_Net_logstd, statistics_logvar)\n",
    "                    results[\"statistics_mu_feed\"] = statistics_mu\n",
    "                    results[\"statistics_logvar_feed\"] = statistics_logvar    \n",
    "                y_pred = self.generative_Net(X_test, statistics_mu)\n",
    "                y_pred_logstd = self.generative_Net_logstd(X_test, statistics_logvar)\n",
    "                \n",
    "                results[\"y_pred\"] = y_pred\n",
    "                results[\"y_pred_logstd\"] = y_pred_logstd\n",
    "            else:\n",
    "                statistics = self.statistics_Net.forward_inputs(X_train, y_train)\n",
    "                results[\"statistics\"] = statistics\n",
    "                if is_regulated_net:\n",
    "                    statistics = get_regulated_statistics(self.generative_Net, statistics)\n",
    "                    results[\"statistics_feed\"] = statistics\n",
    "                y_pred = get_forward_pred(self.generative_Net, X_test, forward_steps, is_time_series = is_time_series, latent_param = statistics, jump_step = 2, is_flatten = True)\n",
    "                results[\"y_pred\"] = y_pred\n",
    "        return results\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], target = [\"statistics_Net\", \"generative_Net\"], mode = \"L1\"):\n",
    "        if target == \"all\":\n",
    "            if self.use_net == \"generative\":\n",
    "                target = [\"statistics_Net\", \"generative_Net\"]\n",
    "            elif self.use_net == \"cloned\":\n",
    "                target = [\"cloned_Net\"]\n",
    "            else:\n",
    "                raise\n",
    "        if not isinstance(target, list):\n",
    "            target = [target]\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for target_ele in target:\n",
    "            if target_ele == \"statistics_Net\":\n",
    "                assert self.use_net == \"generative\"\n",
    "                reg = reg + self.statistics_Net.get_regularization(source = source, mode = mode)\n",
    "            elif target_ele == \"generative_Net\":\n",
    "                assert self.use_net == \"generative\"\n",
    "                reg = reg + self.generative_Net.get_regularization(source = source, mode = mode)\n",
    "            elif target_ele == \"cloned_Net\":\n",
    "                assert self.use_net == \"cloned\"\n",
    "                reg = reg + self.cloned_net.get_regularization(source = source, mode = mode)\n",
    "            else:\n",
    "                raise Exception(\"target element {0} not recognized!\".format(target_ele))\n",
    "        return reg\n",
    "\n",
    "    def latent_param_quick_learn(self, X, y, validation_data, loss_core = \"huber\", epochs = 10, batch_size = 128, lr = 1e-2, optim_type = \"LBFGS\", reset_latent_param = False):\n",
    "        if reset_latent_param:\n",
    "            self.get_statistics(X, y)\n",
    "        return self.generative_Net.latent_param_quick_learn(X = X, y = y, validation_data = validation_data, loss_core = loss_core,\n",
    "                                                            epochs = epochs, batch_size = batch_size, lr = lr, optim_type = optim_type)\n",
    "\n",
    "    def clone_net_quick_learn(self, X, y, validation_data, loss_core = \"huber\", epochs = 40, batch_size = 128, lr = 1e-3, optim_type = \"adam\"):\n",
    "        mse_list, self.cloned_net = quick_learn(self.cloned_net, X, y, validation_data, loss_core = loss_core, batch_size = batch_size, epochs = epochs, lr = lr, optim_type = optim_type)\n",
    "        return mse_list\n",
    "\n",
    "\n",
    "def quick_learn(model, X, y, validation_data, forward_steps = [1], is_time_series = True, loss_core = \"huber\", batch_size = 128, epochs = 40, lr = 1e-3, optim_type = \"adam\"):\n",
    "    model_train = deepcopy(model)\n",
    "    net_optimizer = get_optimizer(optim_type = optim_type, lr = lr, parameters = model_train.parameters())\n",
    "    criterion = get_criterion(loss_core)\n",
    "    mse_list = []\n",
    "    X_test, y_test = validation_data\n",
    "    batch_size = min(batch_size, len(X))\n",
    "    if isinstance(X, Variable):\n",
    "        X = X.data\n",
    "    if isinstance(y, Variable):\n",
    "        y = y.data\n",
    "\n",
    "    dataset_train = data_utils.TensorDataset(X, y)\n",
    "    train_loader = data_utils.DataLoader(dataset_train, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    y_pred_test = get_forward_pred(model_train, X_test, forward_steps = forward_steps, is_time_series = is_time_series, jump_step = 2, is_flatten = True)\n",
    "    mse_test = get_criterion(\"mse\")(y_pred_test, y_test)\n",
    "    mse_list.append(mse_test.data[0])\n",
    "    for i in range(epochs):\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch = Variable(X_batch)\n",
    "            y_batch = Variable(y_batch)\n",
    "            if optim_type == \"LBFGS\":\n",
    "                def closure():\n",
    "                    net_optimizer.zero_grad()\n",
    "                    y_pred = get_forward_pred(model_train, X_batch, forward_steps = forward_steps, is_time_series = is_time_series, jump_step = 2, is_flatten = True)\n",
    "                    loss = criterion(y_pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                net_optimizer.step(closure)\n",
    "            else:\n",
    "                net_optimizer.zero_grad()\n",
    "                y_pred = get_forward_pred(model_train, X_batch, forward_steps = forward_steps, is_time_series = is_time_series, jump_step = 2, is_flatten = True)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                net_optimizer.step()\n",
    "        y_pred_test = get_forward_pred(model_train, X_test, forward_steps = forward_steps, is_time_series = is_time_series, jump_step = 2, is_flatten = True)\n",
    "        mse_test = get_criterion(\"mse\")(y_pred_test, y_test)\n",
    "        mse_list.append(mse_test.data[0])\n",
    "    mse_list = np.array(mse_list)\n",
    "    return mse_list, model_train\n",
    "\n",
    "    \n",
    "def load_model_dict(model_dict, is_cuda = False):\n",
    "    if model_dict[\"type\"] == \"Statistics_Net\":\n",
    "        net = Statistics_Net(input_size = model_dict[\"input_size\"],\n",
    "                             pre_pooling_neurons = model_dict[\"pre_pooling_neurons\"],\n",
    "                             struct_param_pre = model_dict[\"struct_param_pre\"],\n",
    "                             struct_param_post = model_dict[\"struct_param_post\"],\n",
    "                             struct_param_post_logvar = model_dict[\"struct_param_post_logvar\"],\n",
    "                             pooling = model_dict[\"pooling\"],\n",
    "                             settings = model_dict[\"settings\"],\n",
    "                             layer_type = model_dict[\"layer_type\"],\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "        net.encoding_statistics_Net.load_model_dict(model_dict[\"encoding_statistics_Net\"])\n",
    "        net.post_pooling_Net.load_model_dict(model_dict[\"post_pooling_Net\"])\n",
    "        if model_dict[\"struct_param_post_logvar\"] is not None:\n",
    "            net.post_pooling_logvar_Net.load_model_dict(model_dict[\"post_pooling_logvar_Net\"])\n",
    "    elif model_dict[\"type\"] == \"Statistics_Net_Conv\":\n",
    "        net = Statistics_Net_Conv(input_channels = model_dict[\"input_channels\"],\n",
    "                                  num_cls = model_dict[\"num_cls\"],\n",
    "                                  pre_pooling_neurons = model_dict[\"pre_pooling_neurons\"],\n",
    "                                  struct_param_pre_conv = model_dict[\"struct_param_pre_conv\"],\n",
    "                                  struct_param_pre = model_dict[\"struct_param_pre\"],\n",
    "                                  struct_param_post = model_dict[\"struct_param_post\"],\n",
    "                                  struct_param_post_logvar = model_dict[\"struct_param_post_logvar\"],\n",
    "                                  pooling = model_dict[\"pooling\"],\n",
    "                                  settings = model_dict[\"settings\"],\n",
    "                                  layer_type = model_dict[\"layer_type\"],\n",
    "                                  is_cuda = is_cuda,\n",
    "                                 )\n",
    "        net.encoding_statistics_ConvNet.load_model_dict(model_dict[\"encoding_statistics_ConvNet\"])\n",
    "        net.encoding_statistics_Net.load_model_dict(model_dict[\"encoding_statistics_Net\"])\n",
    "        net.post_pooling_Net.load_model_dict(model_dict[\"post_pooling_Net\"])\n",
    "        if model_dict[\"struct_param_post_logvar\"] is not None:\n",
    "            net.post_pooling_logvar_Net.load_model_dict(model_dict[\"post_pooling_logvar_Net\"])\n",
    "    elif model_dict[\"type\"] == \"Generative_Net\":\n",
    "        learnable_latent_param = model_dict[\"learnable_latent_param\"] if \"learnable_latent_param\" in model_dict else False\n",
    "        net = Generative_Net(input_size = model_dict[\"input_size\"],\n",
    "                             W_struct_param_list = model_dict[\"W_struct_param_list\"],\n",
    "                             b_struct_param_list = model_dict[\"b_struct_param_list\"],\n",
    "                             num_context_neurons = model_dict[\"num_context_neurons\"],\n",
    "                             settings_generative = model_dict[\"settings_generative\"],\n",
    "                             settings_model = model_dict[\"settings_model\"],\n",
    "                             learnable_latent_param = True,\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "        for i, W_struct_param in enumerate(model_dict[\"W_struct_param_list\"]):\n",
    "            getattr(net, \"W_gen_{0}\".format(i)).load_model_dict(model_dict[\"W_gen_{0}\".format(i)])\n",
    "            getattr(net, \"b_gen_{0}\".format(i)).load_model_dict(model_dict[\"b_gen_{0}\".format(i)])\n",
    "        if \"latent_param\" in model_dict and model_dict[\"latent_param\"] is not None:\n",
    "            if net.latent_param is not None:\n",
    "                net.latent_param.data.copy_(torch.FloatTensor(model_dict[\"latent_param\"]))\n",
    "            else:\n",
    "                net.latent_param = Variable(torch.FloatTensor(model_dict[\"latent_param\"]), requires_grad = False)\n",
    "                if is_cuda:\n",
    "                    net.latent_param = net.latent_param.cuda()\n",
    "        if \"context\" in model_dict:\n",
    "            net.context.data.copy_(torch.FloatTensor(model_dict[\"context\"]))\n",
    "    elif model_dict[\"type\"] in [\"Master_Model\", \"Full_Net\"]: # The \"Full_Net\" name is for legacy\n",
    "        statistics_Net = load_model_dict(model_dict[\"statistics_Net\"], is_cuda = is_cuda)\n",
    "        generative_Net = load_model_dict(model_dict[\"generative_Net\"], is_cuda = is_cuda)\n",
    "        if \"generative_Net_logstd\" in model_dict:\n",
    "            generative_Net_logstd = load_model_dict(model_dict[\"generative_Net_logstd\"], is_cuda = is_cuda)\n",
    "        else:\n",
    "            generative_Net_logstd = None\n",
    "        net = Master_Model(statistics_Net = statistics_Net, generative_Net = generative_Net, generative_Net_logstd = generative_Net_logstd)\n",
    "    else:\n",
    "        raise Exception(\"type {0} not recognized!\".format(model_dict[\"type\"]))\n",
    "    return net\n",
    "\n",
    "\n",
    "class Statistics_Net(nn.Module):\n",
    "    def __init__(self, input_size, pre_pooling_neurons, struct_param_pre, struct_param_post, struct_param_post_logvar = None, pooling = \"max\", settings = {\"activation\": \"leakyRelu\"}, layer_type = \"Simple_layer\", is_cuda = False):\n",
    "        super(Statistics_Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.pre_pooling_neurons = pre_pooling_neurons\n",
    "        self.struct_param_pre = struct_param_pre\n",
    "        self.struct_param_post = struct_param_post\n",
    "        self.struct_param_post_logvar = struct_param_post_logvar\n",
    "        self.pooling = pooling\n",
    "        self.settings = settings\n",
    "        self.layer_type = layer_type\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "        self.encoding_statistics_Net = Net(input_size = self.input_size, struct_param = self.struct_param_pre, settings = self.settings, is_cuda = is_cuda)\n",
    "        self.post_pooling_Net = Net(input_size = self.pre_pooling_neurons, struct_param = self.struct_param_post, settings = self.settings, is_cuda = is_cuda)\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            self.post_pooling_logvar_Net = Net(input_size = self.pre_pooling_neurons, struct_param = self.struct_param_post_logvar, settings = self.settings, is_cuda = is_cuda)\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Statistics_Net\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"pre_pooling_neurons\"] = self.pre_pooling_neurons\n",
    "        model_dict[\"struct_param_pre\"] = self.struct_param_pre\n",
    "        model_dict[\"struct_param_post\"] = self.struct_param_post\n",
    "        model_dict[\"struct_param_post_logvar\"] = self.struct_param_post_logvar\n",
    "        model_dict[\"pooling\"] = self.pooling\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"layer_type\"] = self.layer_type\n",
    "        model_dict[\"encoding_statistics_Net\"] = self.encoding_statistics_Net.model_dict\n",
    "        model_dict[\"post_pooling_Net\"] = self.post_pooling_Net.model_dict\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            model_dict[\"post_pooling_logvar_Net\"] = self.post_pooling_logvar_Net.model_dict\n",
    "        return model_dict\n",
    "    \n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def forward(self, input):\n",
    "        encoding = self.encoding_statistics_Net(input)\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = encoding.mean(0)\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled = encoding.max(0)[0]\n",
    "        else:\n",
    "            raise Exception(\"pooling {0} not recognized!\".format(self.pooling))\n",
    "        output = self.post_pooling_Net(pooled.unsqueeze(0))\n",
    "        if self.struct_param_post_logvar is None:\n",
    "            return output\n",
    "        else:\n",
    "            logvar = self.post_pooling_logvar_Net(pooled.unsqueeze(0))\n",
    "            return output, logvar\n",
    "    \n",
    "    def forward_inputs(self, X, y):\n",
    "        return self(torch.cat([X, y], 1))\n",
    "    \n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = self.encoding_statistics_Net.get_regularization(source = source, mode = mode) + \\\n",
    "              self.post_pooling_Net.get_regularization(source = source, mode = mode)\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            reg = reg + self.post_pooling_logvar_Net.get_regularization(source = source, mode = mode)\n",
    "        return reg\n",
    "\n",
    "    \n",
    "class Statistics_Net_Conv(nn.Module):\n",
    "    def __init__(self, input_channels, num_cls, pre_pooling_neurons, struct_param_pre_conv, struct_param_pre, struct_param_post, struct_param_post_logvar = None, pooling = \"max\", settings = {\"activation\": \"leakyRelu\"}, layer_type = \"Simple_layer\", is_cuda = False):\n",
    "        super(Statistics_Net_Conv, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.num_cls = num_cls\n",
    "        self.pre_pooling_neurons = pre_pooling_neurons\n",
    "        self.struct_param_pre_conv = struct_param_pre_conv\n",
    "        self.struct_param_pre = struct_param_pre\n",
    "        self.struct_param_post = struct_param_post\n",
    "        self.struct_param_post_logvar = struct_param_post_logvar\n",
    "        self.pooling = pooling\n",
    "        self.settings = settings\n",
    "        self.layer_type = layer_type\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "        self.encoding_statistics_ConvNet = ConvNet(input_channels = self.input_channels, struct_param = self.struct_param_pre_conv, settings = self.settings, is_cuda = is_cuda)\n",
    "        X = Variable(torch.zeros(10, 3, 28, 28))\n",
    "        if is_cuda:\n",
    "            X = X.cuda()\n",
    "        dim_enc_conv = flatten(self.encoding_statistics_ConvNet(X)[0]).size(1)\n",
    "        self.encoding_statistics_Net = Net(input_size = dim_enc_conv + num_cls, struct_param = self.struct_param_pre, settings = self.settings, is_cuda = is_cuda)\n",
    "        self.post_pooling_Net = Net(input_size = self.pre_pooling_neurons, struct_param = self.struct_param_post, settings = self.settings, is_cuda = is_cuda)\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            self.post_pooling_logvar_Net = Net(input_size = self.pre_pooling_neurons, struct_param = self.struct_param_post_logvar, settings = self.settings, is_cuda = is_cuda)\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Statistics_Net_Conv\"}\n",
    "        model_dict[\"input_channels\"] = self.input_channels\n",
    "        model_dict[\"num_cls\"] = self.num_cls\n",
    "        model_dict[\"pre_pooling_neurons\"] = self.pre_pooling_neurons\n",
    "        model_dict[\"struct_param_pre_conv\"] = self.struct_param_pre_conv\n",
    "        model_dict[\"struct_param_pre\"] = self.struct_param_pre\n",
    "        model_dict[\"struct_param_post\"] = self.struct_param_post\n",
    "        model_dict[\"struct_param_post_logvar\"] = self.struct_param_post_logvar\n",
    "        model_dict[\"pooling\"] = self.pooling\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"layer_type\"] = self.layer_type\n",
    "        model_dict[\"encoding_statistics_Net\"] = self.encoding_statistics_Net.model_dict\n",
    "        model_dict[\"encoding_statistics_ConvNet\"] = self.encoding_statistics_ConvNet.model_dict\n",
    "        model_dict[\"post_pooling_Net\"] = self.post_pooling_Net.model_dict\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            model_dict[\"post_pooling_logvar_Net\"] = self.post_pooling_logvar_Net.model_dict\n",
    "        return model_dict\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        encoding_X, _ = self.encoding_statistics_ConvNet(X)\n",
    "        encoding_X = flatten(encoding_X)\n",
    "        encoding = torch.cat([encoding_X, to_one_hot(y, self.num_cls)], 1)\n",
    "        encoding = self.encoding_statistics_Net(encoding)\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = encoding.mean(0)\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled = encoding.max(0)[0]\n",
    "        else:\n",
    "            raise Exception(\"pooling {0} not recognized!\".format(self.pooling))\n",
    "        output = self.post_pooling_Net(pooled.unsqueeze(0))\n",
    "        if self.struct_param_post_logvar is None:\n",
    "            return output\n",
    "        else:\n",
    "            logvar = self.post_pooling_logvar_Net(pooled.unsqueeze(0))\n",
    "            return output, logvar\n",
    "    \n",
    "    def forward_inputs(self, X, y):\n",
    "        return self(X, y)\n",
    "    \n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = self.encoding_statistics_Net.get_regularization(source = source, mode = mode) + \\\n",
    "              self.post_pooling_Net.get_regularization(source = source, mode = mode)\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            reg = reg + self.post_pooling_logvar_Net.get_regularization(source = source, mode = mode)\n",
    "        return reg\n",
    "\n",
    "\n",
    "class Generative_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size,\n",
    "        W_struct_param_list,\n",
    "        b_struct_param_list, \n",
    "        num_context_neurons = 0, \n",
    "        settings_generative = {\"activation\": \"leakyRelu\"}, \n",
    "        settings_model = {\"activation\": \"leakyRelu\"}, \n",
    "        learnable_latent_param = False,\n",
    "        last_layer_linear = True,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Generative_Net, self).__init__()\n",
    "        assert(len(W_struct_param_list) == len(b_struct_param_list))\n",
    "        self.input_size = input_size\n",
    "        self.W_struct_param_list = W_struct_param_list\n",
    "        self.b_struct_param_list = b_struct_param_list\n",
    "        self.num_context_neurons = num_context_neurons\n",
    "        self.settings_generative = settings_generative\n",
    "        self.settings_model = settings_model\n",
    "        self.learnable_latent_param = learnable_latent_param\n",
    "        self.last_layer_linear = last_layer_linear\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "        for i, W_struct_param in enumerate(self.W_struct_param_list):\n",
    "            setattr(self, \"W_gen_{0}\".format(i), Net(input_size = self.input_size + num_context_neurons, struct_param = W_struct_param, settings = self.settings_generative, is_cuda = is_cuda))\n",
    "            setattr(self, \"b_gen_{0}\".format(i), Net(input_size = self.input_size + num_context_neurons, struct_param = self.b_struct_param_list[i], settings = self.settings_generative, is_cuda = is_cuda))\n",
    "        # Setting up latent param and context param:\n",
    "        self.latent_param = nn.Parameter(torch.randn(1, self.input_size)) if learnable_latent_param else None\n",
    "        if self.num_context_neurons > 0:\n",
    "            self.context = nn.Parameter(torch.randn(1, self.num_context_neurons))\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Generative_Net\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"W_struct_param_list\"] = self.W_struct_param_list\n",
    "        model_dict[\"b_struct_param_list\"] = self.b_struct_param_list\n",
    "        model_dict[\"num_context_neurons\"] = self.num_context_neurons\n",
    "        model_dict[\"settings_generative\"] = self.settings_generative\n",
    "        model_dict[\"settings_model\"] = self.settings_model\n",
    "        model_dict[\"learnable_latent_param\"] = self.learnable_latent_param\n",
    "        model_dict[\"last_layer_linear\"] = self.last_layer_linear\n",
    "        for i, W_struct_param in enumerate(self.W_struct_param_list):\n",
    "            model_dict[\"W_gen_{0}\".format(i)] = getattr(self, \"W_gen_{0}\".format(i)).model_dict\n",
    "            model_dict[\"b_gen_{0}\".format(i)] = getattr(self, \"b_gen_{0}\".format(i)).model_dict\n",
    "        if self.latent_param is None:\n",
    "            model_dict[\"latent_param\"] = None\n",
    "        else:\n",
    "            model_dict[\"latent_param\"] = self.latent_param.cpu().data.numpy() if self.is_cuda else self.latent_param.data.numpy()\n",
    "        if hasattr(self, \"context\"):\n",
    "            model_dict[\"context\"] = self.context.data.numpy() if not self.is_cuda else self.context.cpu().data.numpy()\n",
    "        return model_dict\n",
    "    \n",
    "    def set_latent_param_learnable(self, mode):\n",
    "        if mode == \"on\":\n",
    "            if not self.learnable_latent_param:\n",
    "                self.learnable_latent_param = True\n",
    "                if self.latent_param is None:\n",
    "                    self.latent_param = nn.Parameter(torch.randn(1, self.input_size))\n",
    "                else:\n",
    "                    self.latent_param = nn.Parameter(self.latent_param.data)\n",
    "            else:\n",
    "                assert isinstance(self.latent_param, nn.Parameter)\n",
    "        elif mode == \"off\":\n",
    "            if self.learnable_latent_param:\n",
    "                assert isinstance(self.latent_param, nn.Parameter)\n",
    "                self.learnable_latent_param = False\n",
    "                self.latent_param = Variable(self.latent_param.data, requires_grad = False)\n",
    "            else:\n",
    "                assert isinstance(self.latent_param, Variable) or self.latent_param is None\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def init_weights_bias(self, latent_param):\n",
    "        if self.num_context_neurons > 0:\n",
    "            latent_param = torch.cat([latent_param, self.context], 1)\n",
    "        for i in range(len(self.W_struct_param_list)):\n",
    "            setattr(self, \"W_{0}\".format(i), (getattr(self, \"W_gen_{0}\".format(i))(latent_param)).squeeze(0))\n",
    "            setattr(self, \"b_{0}\".format(i), getattr(self, \"b_gen_{0}\".format(i))(latent_param))       \n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, isplot = False, latent_param = None):\n",
    "        if latent_param is not None:\n",
    "            self.init_weights_bias(latent_param)\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        if W_source is not None:\n",
    "            for k in range(len(self.W_struct_param_list)):\n",
    "                if W_source == \"core\":\n",
    "                    W = getattr(self, \"W_{0}\".format(k)).data.numpy()\n",
    "                else:\n",
    "                    raise Exception(\"W_source '{0}' not recognized!\".format(W_source))\n",
    "                W_list.append(W)\n",
    "        if b_source is not None:\n",
    "            for k in range(len(self.b_struct_param_list)):\n",
    "                if b_source == \"core\":\n",
    "                    b = getattr(self, \"b_{0}\".format(k)).data.numpy()\n",
    "                else:\n",
    "                    raise Exception(\"b_source '{0}' not recognized!\".format(b_source))\n",
    "                b_list.append(b)\n",
    "        if isplot:\n",
    "            if W_source is not None:\n",
    "                print(\"weight {0}:\".format(W_source))\n",
    "                plot_matrices(W_list)\n",
    "            if b_source is not None:\n",
    "                print(\"bias {0}:\".format(b_source))\n",
    "                plot_matrices(b_list)\n",
    "        return W_list, b_list\n",
    "\n",
    "    \n",
    "    def set_latent_param(self, latent_param):\n",
    "        assert isinstance(latent_param, Variable), \"The latent_param must be a Variable!\"\n",
    "        if self.learnable_latent_param:\n",
    "            self.latent_param.data.copy_(latent_param.data)\n",
    "        else:\n",
    "            self.latent_param = latent_param\n",
    "    \n",
    "    \n",
    "    def latent_param_quick_learn(self, X, y, validation_data, loss_core = \"huber\", epochs = 10, batch_size = 128, lr = 1e-2, optim_type = \"LBFGS\"):\n",
    "        assert self.learnable_latent_param is True, \"To quick-learn latent_param, you must set learnable_latent_param as True!\"\n",
    "        self.latent_param_optimizer = get_optimizer(optim_type = optim_type, lr = lr, parameters = [self.latent_param])\n",
    "        self.criterion = get_criterion(loss_core)\n",
    "        loss_list = []\n",
    "        X_test, y_test = validation_data\n",
    "        batch_size = min(batch_size, len(X))\n",
    "        if isinstance(X, Variable):\n",
    "            X = X.data\n",
    "        if isinstance(y, Variable):\n",
    "            y = y.data\n",
    "\n",
    "        dataset_train = data_utils.TensorDataset(X, y)\n",
    "        train_loader = data_utils.DataLoader(dataset_train, batch_size = batch_size, shuffle = True)\n",
    "        \n",
    "        y_pred_test = self(X_test)\n",
    "        loss = get_criterion(\"mse\")(y_pred_test, y_test)\n",
    "        loss_list.append(loss.data[0])\n",
    "        for i in range(epochs):\n",
    "            for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                X_batch = Variable(X_batch)\n",
    "                y_batch = Variable(y_batch)\n",
    "                if optim_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.latent_param_optimizer.zero_grad()\n",
    "                        y_pred = self(X_batch)\n",
    "                        loss = self.criterion(y_pred, y_batch)\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                    self.latent_param_optimizer.step(closure)\n",
    "                else:\n",
    "                    self.latent_param_optimizer.zero_grad()\n",
    "                    y_pred = self(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    self.latent_param_optimizer.step()\n",
    "            y_pred_test = self(X_test)\n",
    "            loss = get_criterion(\"mse\")(y_pred_test, y_test)\n",
    "            loss_list.append(loss.data[0])\n",
    "        loss_list = np.array(loss_list)\n",
    "        return loss_list\n",
    "\n",
    "\n",
    "    def forward(self, input, latent_param = None):\n",
    "        if latent_param is None:\n",
    "            latent_param = self.latent_param\n",
    "        self.init_weights_bias(latent_param)\n",
    "        output = input\n",
    "        for i in range(len(self.W_struct_param_list)):\n",
    "            output = torch.matmul(output, getattr(self, \"W_{0}\".format(i))) + getattr(self, \"b_{0}\".format(i))\n",
    "            if i == len(self.W_struct_param_list) - 1 and hasattr(self, \"last_layer_linear\") and self.last_layer_linear:\n",
    "                activation = \"linear\"\n",
    "            else:\n",
    "                activation = self.settings_model[\"activation\"] if \"activation\" in self.settings_model else \"leakyRelu\"\n",
    "            output = get_activation(activation)(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for reg_type in source:\n",
    "            if reg_type == \"weight\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    if mode == \"L1\":\n",
    "                        reg = reg + getattr(self, \"W_{0}\".format(i)).abs().sum()\n",
    "                    else:\n",
    "                        raise\n",
    "            elif reg_type == \"bias\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    if mode == \"L1\":\n",
    "                        reg = reg + getattr(self, \"b_{0}\".format(i)).abs().sum()\n",
    "                    else:\n",
    "                        raise\n",
    "            elif reg_type == \"W_gen\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    reg = reg + getattr(self, \"W_gen_{0}\".format(i)).get_regularization(source = source, mode = mode)\n",
    "            elif reg_type == \"b_gen\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    reg = reg + getattr(self, \"b_gen_{0}\".format(i)).get_regularization(source = source, mode = mode)\n",
    "            else:\n",
    "                raise Exception(\"source {0} not recognized!\".format(reg_type))\n",
    "        return reg\n",
    "\n",
    "\n",
    "class VAE_Loss(nn.Module):\n",
    "    def __init__(self, criterion, prior = \"Gaussian\", beta = 1):\n",
    "        super(VAE_Loss, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.prior = \"Gaussian\"\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, input, target, mu, logvar):\n",
    "        reconstuction_loss = self.criterion(input, target)\n",
    "        if self.prior == \"Gaussian\":\n",
    "            KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        else:\n",
    "            raise Exception(\"prior {0} not recognized!\".format(self.prior))\n",
    "        return reconstuction_loss, KLD * self.beta\n",
    "  \n",
    "\n",
    "\n",
    "def forward(model, X):\n",
    "    \"\"\"General function for applying the same model at multiple time steps\"\"\"\n",
    "    output_list = []\n",
    "    for i in range(X.size(1)):\n",
    "        output = model(X[:,i:i+1,...])\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        output_list.append(output)\n",
    "    output_seq = torch.cat(output_list, 1)\n",
    "    return output_seq\n",
    "\n",
    "\n",
    "def get_forward_pred(predictor, latent, forward_steps, latent_param = None, is_time_series = True, jump_step = 2, is_flatten = False, oracle_size = None):\n",
    "    \"\"\"Applying the same model to roll out several time steps\"\"\"\n",
    "    if not is_time_series:\n",
    "        if latent_param is None:\n",
    "            pred_list = predictor(latent)\n",
    "        else:\n",
    "            pred_list = predictor(latent, latent_param)      \n",
    "    else:\n",
    "        max_forward_steps = max(forward_steps)\n",
    "        current_latent = latent\n",
    "        pred_list = []\n",
    "        for i in range(1, max_forward_steps + 1):\n",
    "            if latent_param is None:\n",
    "                current_pred = predictor(current_latent)\n",
    "            else:\n",
    "                current_pred = predictor(current_latent, latent_param)\n",
    "            pred_list.append(current_pred)\n",
    "            if oracle_size is None:\n",
    "                current_latent = torch.cat([current_latent[:,jump_step:], current_pred], 1)\n",
    "            else:\n",
    "                current_latent = torch.cat([current_latent[:,jump_step:-oracle_size], current_pred, current_latent[:,-oracle_size:]], 1)\n",
    "        pred_list = torch.cat(pred_list, 1)\n",
    "        pred_list = pred_list.view(pred_list.size(0), -1, 2)\n",
    "        forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "        if predictor.is_cuda:\n",
    "            forward_steps_idx = forward_steps_idx.cuda()\n",
    "        pred_list = pred_list[:, forward_steps_idx]\n",
    "        if is_flatten:\n",
    "            pred_list = pred_list.view(pred_list.size(0), -1)\n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def get_autoencoder_losses(conv_encoder, predictor, X_motion, y_motion, forward_steps):\n",
    "    \"\"\"Getting autoencoder loss\"\"\"\n",
    "    latent = forward(conv_encoder.encode, X_motion).view(X_motion.size(0), -1, 2)\n",
    "    latent_pred = get_forward_pred(predictor, latent, forward_steps = forward_steps, is_time_series = True)\n",
    "    \n",
    "    pred_recons = forward(conv_encoder.decode, latent_pred)\n",
    "    recons = forward(conv_encoder.decode, latent)\n",
    "    loss_auxiliary = nn.MSELoss()(recons, X_motion)\n",
    "    forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "    if predictor.is_cuda:\n",
    "        forward_steps_idx = forward_steps_idx.cuda()\n",
    "    y_motion = y_motion[:, forward_steps_idx]\n",
    "    loss_pred_recons = nn.MSELoss()(pred_recons, y_motion)\n",
    "    return loss_auxiliary, loss_pred_recons, pred_recons\n",
    "\n",
    "\n",
    "def get_rollout_pred_loss(conv_encoder, predictor, X_motion, y_motion, max_step, isplot = True):\n",
    "    \"\"\"Getting the loss for multiple forward steps\"\"\"\n",
    "    step_list = []\n",
    "    loss_step_list = []\n",
    "    for i in range(1, max_step + 1):\n",
    "        _, loss_pred_recons, _ = get_losses(conv_encoder, predictor, X_motion, y_motion, forward_steps = [i])\n",
    "        step_list.append(i)\n",
    "        loss_step_list.append(loss_pred_recons.data[0])\n",
    "    if isplot:\n",
    "        plt.plot(step_list, loss_step_list)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    return step_list, loss_step_list\n",
    "\n",
    "\n",
    "class Loss_with_autoencoder(nn.Module):\n",
    "    def __init__(self, core, forward_steps, aux_coeff = 0.5, is_cuda = False):\n",
    "        super(Loss_with_autoencoder, self).__init__()\n",
    "        self.core = core\n",
    "        self.aux_coeff = aux_coeff\n",
    "        self.loss_fun = get_criterion(self.core)\n",
    "        self.is_cuda = is_cuda\n",
    "        self.forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "        if self.is_cuda:\n",
    "            self.forward_steps_idx = self.forward_steps_idx.cuda()\n",
    "    \n",
    "    def forward(self, X_latent, y_latent_pred, X_train_obs, y_train_obs, autoencoder, loss_fun = None, verbose = False, oracle_size = None):\n",
    "        if oracle_size is not None:\n",
    "            X_latent = X_latent[:, : -oracle_size].contiguous()\n",
    "        X_latent = X_latent.view(X_latent.size(0), -1, 2)\n",
    "        recons = forward(autoencoder.decode, X_latent)\n",
    "        pred_recons = forward(autoencoder.decode, y_latent_pred.view(y_latent_pred.size(0), -1, 2))\n",
    "        if loss_fun is None:\n",
    "            loss_fun = self.loss_fun\n",
    "        loss_auxilliary = loss_fun(recons, X_train_obs)\n",
    "        loss_pred = loss_fun(pred_recons, y_train_obs[:, self.forward_steps_idx])\n",
    "        if verbose:\n",
    "            print(\"loss_aux: {0:.6f}\\t loss_pred: {1:.6f}\".format(loss_auxilliary.data[0], loss_pred.data[0]))\n",
    "        return loss_pred + loss_auxilliary * self.aux_coeff\n",
    "\n",
    "\n",
    "def get_relevance(X, y, statistics_Net):\n",
    "    concat = torch.cat([X, y], 1)\n",
    "    max_datapoint = statistics_Net.encoding_statistics_Net(concat).max(0)[1].data.numpy()\n",
    "    unique, counts = np.unique(max_datapoint, return_counts = True)\n",
    "    relevance = np.zeros(len(X))\n",
    "    relevance[unique] = counts\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def sample_Gaussian(mu, logvar):\n",
    "    std = logvar.mul(0.5).exp_()\n",
    "    eps = Variable(torch.randn(std.size()), requires_grad = False)\n",
    "    if mu.is_cuda:\n",
    "        eps = eps.cuda()\n",
    "    return mu + std * eps\n",
    "\n",
    "\n",
    "def clone_net(generative_Net, layer_type = \"Simple_Layer\", clone_parameters = True):\n",
    "    W_init_list = []\n",
    "    b_init_list = []\n",
    "    input_size = generative_Net.W_struct_param_list[0][-1][0][0]\n",
    "    struct_param = []\n",
    "    statistics = generative_Net.latent_param\n",
    "    if clone_parameters and generative_Net.num_context_neurons > 0:\n",
    "        statistics = torch.cat([statistics, generative_Net.context], 1)\n",
    "    for i in range(len(generative_Net.W_struct_param_list)):\n",
    "        num_neurons = generative_Net.b_struct_param_list[i][-1][0]\n",
    "        layer_struct_param = [num_neurons, layer_type, {}]\n",
    "        struct_param.append(layer_struct_param)\n",
    "        if clone_parameters:\n",
    "            W_init = (getattr(generative_Net, \"W_gen_{0}\".format(i))(statistics)).squeeze(0)\n",
    "            b_init = getattr(generative_Net, \"b_gen_{0}\".format(i))(statistics)\n",
    "            if generative_Net.is_cuda:\n",
    "                W_init = W_init.cpu()\n",
    "                b_init = b_init.cpu()\n",
    "            W_init_list.append(W_init.data.numpy())\n",
    "            b_init_list.append(b_init.data.numpy()[0])\n",
    "        else:\n",
    "            W_init_list.append(None)\n",
    "            b_init_list.append(None)\n",
    "    if generative_Net.last_layer_linear is True:\n",
    "        struct_param[-1][2][\"activation\"] = \"linear\"\n",
    "    return Net(input_size = input_size, struct_param = struct_param, W_init_list = W_init_list, b_init_list = b_init_list, settings = generative_Net.settings_model, is_cuda = generative_Net.is_cuda)\n",
    "\n",
    "\n",
    "def get_nets(\n",
    "    input_size,\n",
    "    output_size,\n",
    "    target_size = None,\n",
    "    main_hidden_neurons = [20, 20],\n",
    "    pre_pooling_neurons = 60,\n",
    "    statistics_output_neurons = 10,\n",
    "    num_context_neurons = 0,\n",
    "    struct_param_gen_base = None,\n",
    "    struct_param_pre = None,\n",
    "    struct_param_post = None,\n",
    "    struct_param_post_logvar = None,\n",
    "    statistics_pooling = \"mean\",\n",
    "    activation_statistics = \"leakyRelu\",\n",
    "    activation_generative = \"leakyRelu\",\n",
    "    activation_model = \"leakyRelu\",\n",
    "    learnable_latent_param = False,\n",
    "    isParallel = False,\n",
    "    is_VAE = False,\n",
    "    is_uncertainty_net = False,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    layer_type = \"Simple_Layer\"\n",
    "    struct_param_pre = [\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [pre_pooling_neurons, layer_type, {\"activation\": \"linear\"}],\n",
    "    ] if struct_param_pre is None else struct_param_pre\n",
    "    struct_param_post = [\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [statistics_output_neurons, layer_type, {\"activation\": \"linear\"}],\n",
    "    ] if struct_param_post is None else struct_param_post\n",
    "    if is_VAE or is_uncertainty_net:\n",
    "        if struct_param_post_logvar is None:\n",
    "            struct_param_post_logvar = struct_param_post\n",
    "    if target_size is None:\n",
    "        target_size = output_size\n",
    "    statistics_Net = Statistics_Net(input_size = input_size + target_size,\n",
    "                                    pre_pooling_neurons = pre_pooling_neurons,\n",
    "                                    struct_param_pre = struct_param_pre,\n",
    "                                    struct_param_post = struct_param_post,\n",
    "                                    struct_param_post_logvar = struct_param_post_logvar,\n",
    "                                    pooling = statistics_pooling,\n",
    "                                    settings = {\"activation\": activation_statistics},\n",
    "                                    is_cuda = is_cuda,\n",
    "                                   )\n",
    "\n",
    "    # For Generative_Net:\n",
    "    struct_param_gen_base = [\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "    ] if struct_param_gen_base is None else struct_param_gen_base\n",
    "    \n",
    "    W_struct_param_list = []\n",
    "    b_struct_param_list = []\n",
    "    all_neurons = list(main_hidden_neurons) + [output_size]\n",
    "    for i, num_neurons in enumerate(all_neurons):\n",
    "        num_neurons_prev = all_neurons[i - 1] if i > 0 else input_size\n",
    "        struct_param_weight = struct_param_gen_base + [[(num_neurons_prev, num_neurons), layer_type, {\"activation\": \"linear\"}]]\n",
    "        struct_param_bias = struct_param_gen_base + [[num_neurons, layer_type, {\"activation\": \"linear\"}]]\n",
    "        W_struct_param_list.append(struct_param_weight)\n",
    "        b_struct_param_list.append(struct_param_bias)\n",
    "    generative_Net = Generative_Net(input_size = statistics_output_neurons,\n",
    "                                    num_context_neurons = num_context_neurons,\n",
    "                                    W_struct_param_list = W_struct_param_list,\n",
    "                                    b_struct_param_list = b_struct_param_list,\n",
    "                                    settings_generative = {\"activation\": activation_generative},\n",
    "                                    settings_model = {\"activation\": activation_model},\n",
    "                                    learnable_latent_param = learnable_latent_param,\n",
    "                                    last_layer_linear = True,\n",
    "                                    is_cuda = is_cuda,\n",
    "                                   )\n",
    "    if is_uncertainty_net:\n",
    "        generative_Net_logstd = Generative_Net(input_size = statistics_output_neurons,\n",
    "                                                num_context_neurons = num_context_neurons,\n",
    "                                                W_struct_param_list = W_struct_param_list,\n",
    "                                                b_struct_param_list = b_struct_param_list,\n",
    "                                                settings_generative = {\"activation\": activation_generative},\n",
    "                                                settings_model = {\"activation\": activation_model},\n",
    "                                                learnable_latent_param = learnable_latent_param,\n",
    "                                                last_layer_linear = True,\n",
    "                                                is_cuda = is_cuda,\n",
    "                                               )\n",
    "    else:\n",
    "        generative_Net_logstd = None\n",
    "    if isParallel:\n",
    "        print(\"Using Parallel training.\")\n",
    "        statistics_Net = nn.DataParallel(statistics_Net)\n",
    "        generative_Net = nn.DataParallel(generative_Net)\n",
    "        if is_uncertainty_net:\n",
    "            generative_Net_logstd = nn.DataParallel(generative_Net_logstd)\n",
    "    return statistics_Net, generative_Net, generative_Net_logstd\n",
    "\n",
    "\n",
    "def get_tasks(task_id_list, num_train, num_test, task_settings = {}, is_cuda = False, verbose = False, **kwargs):\n",
    "    num_tasks = num_train + num_test\n",
    "    tasks = {}\n",
    "    for j in range(num_tasks):\n",
    "        if verbose:\n",
    "            print(j)\n",
    "        task_id = np.random.choice(task_id_list)\n",
    "        num_examples = task_settings[\"num_examples\"] if \"num_examples\" in task_settings else 2000\n",
    "        if task_id[:12] == \"latent-linear\":\n",
    "            task = get_latent_model_data(task_settings[\"z_settings\"], settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:10] == \"polynomial\":\n",
    "            order = int(task_id.split(\"-\")[1])\n",
    "            task = get_polynomial_class(task_settings[\"z_settings\"], order = order, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:8] == \"Legendre\":\n",
    "            order = int(task_id.split(\"-\")[1])\n",
    "            task = get_Legendre_class(task_settings[\"z_settings\"], order = order, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:2] == \"M-\":\n",
    "            task_mode = task_id.split(\"-\")[1]\n",
    "            task = get_master_function(task_settings[\"z_settings\"], mode = task_mode, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:2] == \"C-\":\n",
    "            task_mode = task_id.split(\"-\")[1]\n",
    "            task = get_master_function_comparison(mode = task_mode, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id == \"bounce-states\":\n",
    "            task = get_bouncing_states(data_format = \"states\", settings = task_settings, num_examples = num_examples, is_cuda = is_cuda, **kwargs)\n",
    "        elif task_id == \"bounce-images\":\n",
    "            task = get_bouncing_states(data_format = \"images\", settings = task_settings, num_examples = num_examples, is_cuda = is_cuda, **kwargs)\n",
    "        else:\n",
    "            task = Dataset_Gen(task_id, settings = {\"domain\": (-3,3),\n",
    "                                                    \"num_train\": 200,\n",
    "                                                    \"num_test\": 200,\n",
    "                                                    \"isTorch\": True,\n",
    "                                                   })\n",
    "        for k in range(num_tasks):\n",
    "            if \"{0}_{1}\".format(task_id, k) in tasks:\n",
    "                continue\n",
    "            else:\n",
    "                task_key = \"{0}_{1}\".format(task_id, k)\n",
    "        tasks[task_key] = task\n",
    "    task_id_train = np.random.choice(list(tasks.keys()), num_train, replace = False).tolist()\n",
    "    tasks_train = {key: value for key, value in tasks.items() if key in task_id_train}\n",
    "    tasks_test = {key: value for key, value in tasks.items() if key not in task_id_train}\n",
    "    tasks_train = OrderedDict(sorted(tasks_train.items(), key=lambda t: t[0]))\n",
    "    tasks_test = OrderedDict(sorted(tasks_test.items(), key=lambda t: t[0]))\n",
    "    return tasks_train, tasks_test\n",
    "\n",
    "\n",
    "def evaluate(task, master_model = None, model = None, criterion = None, is_time_series = True, oracle_size = None, is_VAE = False, is_regulated_net = False, autoencoder = None, forward_steps = [1], **kwargs):\n",
    "    if autoencoder is not None:\n",
    "        forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)    \n",
    "        ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), z_info = task\n",
    "        if X_train_obs.is_cuda:\n",
    "            forward_steps_idx = forward_steps_idx.cuda()   \n",
    "        X_train = forward(autoencoder.encode, X_train_obs)\n",
    "        y_train = forward(autoencoder.encode, y_train_obs[:, forward_steps_idx])\n",
    "        X_test = forward(autoencoder.encode, X_test_obs)\n",
    "        y_test = forward(autoencoder.encode, y_test_obs[:, forward_steps_idx])\n",
    "        if oracle_size is not None:\n",
    "            z_train = Variable(torch.FloatTensor(np.repeat(np.expand_dims(z_info[\"z\"],0), len(X_train), 0)), requires_grad = False)\n",
    "            z_test = Variable(torch.FloatTensor(np.repeat(np.expand_dims(z_info[\"z\"],0), len(X_test), 0)), requires_grad = False)\n",
    "            if X_train.is_cuda:\n",
    "                z_train = z_train.cuda()\n",
    "                z_test = z_test.cuda()\n",
    "            X_train = torch.cat([X_train, z_train], 1)\n",
    "            X_test = torch.cat([X_test, z_test], 1)\n",
    "    else:\n",
    "        ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "\n",
    "    loss_fun = nn.MSELoss()\n",
    "    if master_model is not None:\n",
    "        assert model is None\n",
    "        if is_VAE:\n",
    "            statistics_mu, statistics_logvar = master_model.statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "            statistics_sampled = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "            y_pred_sampled = master_model.generative_Net(X_test, statistics_sampled)\n",
    "            loss_sampled, KLD = criterion(y_pred_sampled, y_test, statistics_mu, statistics_logvar)\n",
    "\n",
    "            y_pred = master_model.generative_Net(X_test, statistics_mu)\n",
    "            loss = criterion.criterion(y_pred, y_test)\n",
    "            mse = loss_fun(y_pred, y_test)\n",
    "            return loss.data[0], loss_sampled.data[0], mse.data[0], KLD.data[0]\n",
    "        else:\n",
    "            if master_model.generative_Net_logstd is None:\n",
    "                statistics = master_model.statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                if is_regulated_net:\n",
    "                    statistics = get_regulated_statistics(master_model.generative_Net, statistics)\n",
    "                if autoencoder is not None:\n",
    "                    master_model.generative_Net.set_latent_param(statistics)\n",
    "                    y_pred = get_forward_pred(master_model.generative_Net, X_test, forward_steps, is_time_series = is_time_series)\n",
    "                    loss = criterion(X_test, y_pred, X_test_obs, y_test_obs, autoencoder)\n",
    "                    mse = criterion(X_test, y_pred, X_test_obs, y_test_obs, autoencoder, loss_fun = loss_fun, verbose = False)\n",
    "                else:\n",
    "                    y_pred = get_forward_pred(master_model.generative_Net, X_test, forward_steps, is_time_series = is_time_series, latent_param = statistics, jump_step = 2, is_flatten = True)\n",
    "                    loss = criterion(y_pred, y_test)\n",
    "                    mse = loss_fun(y_pred, y_test)  \n",
    "            else:\n",
    "                statistics_mu, statistics_logvar = master_model.statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                if is_regulated_net:\n",
    "                    statistics_mu = get_regulated_statistics(master_model.generative_Net, statistics_mu)\n",
    "                    statistics_logvar = get_regulated_statistics(master_model.generative_Net_logstd, statistics_logvar)\n",
    "                y_pred = master_model.generative_Net(X_test, statistics_mu)\n",
    "                y_pred_logstd = master_model.generative_Net_logstd(X_test, statistics_logvar)\n",
    "                loss = criterion(y_pred, y_test, log_std = y_pred_logstd)\n",
    "                mse = loss_fun(y_pred, y_test)\n",
    "            return loss.data[0], loss.data[0], mse.data[0], 0\n",
    "    else:\n",
    "        if autoencoder is not None:\n",
    "            y_pred = get_forward_pred(model, X_test, forward_steps, is_time_series = is_time_series)\n",
    "            loss = loss_sampled = loss_test_sampled = criterion(X_test, y_pred, X_test_obs, y_test_obs, autoencoder, oracle_size = oracle_size)\n",
    "            mse = criterion(X_test, y_pred, X_test_obs, y_test_obs, autoencoder, loss_fun = loss_fun, verbose = True, oracle_size = oracle_size)\n",
    "        else:\n",
    "            y_pred = get_forward_pred(model, X_test, forward_steps, is_time_series = is_time_series)\n",
    "            loss = loss_sampled = criterion(y_pred, y_test)\n",
    "            mse = loss_fun(y_pred, y_test)\n",
    "        return loss.data[0], loss_sampled.data[0], mse.data[0], 0     \n",
    "\n",
    "\n",
    "def get_reg(reg_dict, statistics_Net = None, generative_Net = None, autoencoder = None, net = None, is_cuda = False):\n",
    "    reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "    if is_cuda:\n",
    "        reg = reg.cuda()\n",
    "    for net_name, reg_info in reg_dict.items():\n",
    "        if net_name == \"statistics_Net\":\n",
    "            reg_net = statistics_Net\n",
    "        elif net_name == \"generative_Net\":\n",
    "            reg_net = generative_Net\n",
    "        elif net_name == \"autoencoder\":\n",
    "            reg_net = autoencoder\n",
    "        elif net_name == \"net\":\n",
    "            reg_net = net\n",
    "        if isinstance(reg_net, nn.DataParallel):\n",
    "            reg_net = reg_net.module\n",
    "        if reg_net is not None:\n",
    "            for reg_type, reg_amp in reg_info.items():\n",
    "                reg = reg + reg_net.get_regularization(source = [reg_type]) * reg_amp\n",
    "    return reg\n",
    "\n",
    "\n",
    "def get_regulated_statistics(generative_Net, statistics):\n",
    "    assert len(statistics.view(-1)) == len(generative_Net.struct_param) * 2 or len(statistics.view(-1)) == len(generative_Net.struct_param)\n",
    "    if len(statistics.view(-1)) == len(generative_Net.struct_param) * 2:\n",
    "        statistics = {i: statistics.view(-1)[2*i: 2*i+2] for i in range(len(generative_Net.struct_param))}\n",
    "    else:\n",
    "        statistics = {i: statistics.view(-1)[i: i+1] for i in range(len(generative_Net.struct_param))}\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def load_trained_models(filename):\n",
    "    statistics_Net = torch.load(filename + \"statistics_Net.pt\")\n",
    "    generative_Net = torch.load(filename + \"generative_Net.pt\")\n",
    "    data_record = pickle.load(open(filename + \"data.p\", \"rb\"))\n",
    "    return statistics_Net, generative_Net, data_record\n",
    "\n",
    "\n",
    "def plot_task_ensembles(tasks, master_model = None, model = None, is_time_series = True, is_oracle = False, is_VAE = False, is_uncertainty_net = False, is_regulated_net = False, autoencoder = None, title = None, isplot = True, **kwargs):\n",
    "    import matplotlib.pyplot as plt\n",
    "    statistics_list = []\n",
    "    z_list = []\n",
    "    for task_key, task in tasks.items():\n",
    "        if autoencoder is not None:\n",
    "            forward_steps = kwargs[\"forward_steps\"]\n",
    "            forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "            ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), info = task\n",
    "            if X_test_obs.is_cuda:\n",
    "                forward_steps_idx = forward_steps_idx.cuda()\n",
    "            X_train = forward(autoencoder.encode, X_train_obs)\n",
    "            y_train = forward(autoencoder.encode, y_train_obs[:, forward_steps_idx])\n",
    "            X_test = forward(autoencoder.encode, X_test_obs)\n",
    "            y_test = forward(autoencoder.encode, y_test_obs[:, forward_steps_idx])\n",
    "            if is_oracle:\n",
    "                z_train = Variable(torch.FloatTensor(np.repeat(np.expand_dims(info[\"z\"],0), len(X_train), 0)), requires_grad = False)\n",
    "                z_test = Variable(torch.FloatTensor(np.repeat(np.expand_dims(info[\"z\"],0), len(X_test), 0)), requires_grad = False)\n",
    "                if X_train.is_cuda:\n",
    "                    z_train = z_train.cuda()\n",
    "                    z_test = z_test.cuda()\n",
    "                X_train = torch.cat([X_train, z_train], 1)\n",
    "                X_test = torch.cat([X_test, z_test], 1)\n",
    "        else:\n",
    "            ((X_train, y_train), (X_test, y_test)), info = task\n",
    "            \n",
    "        if master_model is not None:\n",
    "            results = master_model.get_predictions(X_test = X_test, X_train = X_train, y_train = y_train, is_time_series = is_time_series,\n",
    "                                                  is_VAE = is_VAE, is_uncertainty_net = is_uncertainty_net, is_regulated_net = is_regulated_net)\n",
    "            statistics_list.append(to_np_array(results[\"statistics\"])[0])\n",
    "        else:\n",
    "            results = {}\n",
    "            results[\"y_pred\"] = model(X_test)\n",
    "            statistics_list.append([0, 0])\n",
    "        z_list.append(info[\"z\"])\n",
    "        if isplot:\n",
    "            plt.plot(to_np_array(y_test)[:,0], to_np_array(results[\"y_pred\"])[:,0], \".\", markersize = 1, alpha = 0.5)\n",
    "    if isplot:\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    return np.array(statistics_list), np.array(z_list)\n",
    "\n",
    "\n",
    "def plot_individual_tasks(tasks, master_model = None, model = None, max_plots = 24, is_time_series = True,\n",
    "                          is_VAE = False, is_uncertainty_net = False, is_regulated_net = False, xlim = (-4, 4), sample_times = None, is_oracle = False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    num_columns = 8\n",
    "    max_plots = max(num_columns * 3, max_plots)\n",
    "    num_rows = int(np.ceil(max_plots / num_columns))\n",
    "    fig = plt.figure(figsize = (25, num_rows * 3.3))\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    statistics_list = []\n",
    "    if len(tasks) > max_plots:\n",
    "        chosen_id = np.random.choice(list(tasks.keys()), max_plots, replace = False).tolist()\n",
    "        chosen_id = sorted(chosen_id)\n",
    "    else:\n",
    "        chosen_id = sorted(list(tasks.keys()))\n",
    "    i = 0\n",
    "    is_cuda = tasks[list(tasks.keys())[0]][0][0][0].is_cuda\n",
    "    if xlim is not None:\n",
    "        X_linspace = Variable(torch.linspace(xlim[0], xlim[1], 200).unsqueeze(1))\n",
    "        if is_cuda:\n",
    "            X_linspace = X_linspace.cuda()\n",
    "    for task_id, task in tasks.items():\n",
    "        ((X_train, y_train), (X_test, y_test)), info = task\n",
    "        \n",
    "        if is_oracle:\n",
    "            input_size = X_test.size(1) - len(info[\"z\"].squeeze())\n",
    "        else:\n",
    "            input_size = X_test.size(1)\n",
    "        chosen_dim = np.random.choice(range(input_size))\n",
    "        \n",
    "        if master_model is not None:\n",
    "            results = master_model.get_predictions(X_test = X_linspace, X_train = X_train, y_train = y_train, is_time_series = is_time_series, \n",
    "                                                  is_VAE = is_VAE, is_uncertainty_net = is_uncertainty_net, is_regulated_net = is_regulated_net)\n",
    "            statistics_list.append(to_np_array(results[\"statistics\"]))\n",
    "        else:\n",
    "            results = {}\n",
    "            if is_oracle:\n",
    "                z = to_Variable(np.repeat(np.expand_dims(info[\"z\"], 0), len(X_linspace), 0), is_cuda = is_cuda)\n",
    "                X_linspace_feed = torch.cat([X_linspace, z], 1)\n",
    "            else:\n",
    "                X_linspace_feed = X_linspace\n",
    "            results[\"y_pred\"] = model(X_linspace_feed)\n",
    "            statistics_list.append([0,0])\n",
    "        \n",
    "        if task_id not in chosen_id:\n",
    "            continue\n",
    "        \n",
    "        ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "        if sample_times is None:\n",
    "            if input_size == 1:\n",
    "                X_linspace_numpy, y_pred_numpy = to_np_array(X_linspace, results[\"y_pred\"])\n",
    "                ax.plot(X_linspace_numpy[:, chosen_dim], y_pred_numpy.squeeze(), \"-r\", markersize = 3, label = \"pred\")\n",
    "                if master_model is not None and master_model.generative_Net_logstd is not None:\n",
    "                    y_pred_std = torch.exp(results[\"y_pred_logstd\"])\n",
    "                    y_pred_std_numpy = to_np_array(y_pred_std)\n",
    "                    ax.fill_between(X_linspace_numpy[:, chosen_dim], (y_pred_numpy - y_pred_std_numpy).squeeze(), (y_pred_numpy + y_pred_std_numpy).squeeze(), color = \"r\", alpha = 0.3)\n",
    "        else:\n",
    "            y_pred_list = []\n",
    "            for j in range(sample_times):\n",
    "                statistics_sampled = sample_Gaussian(results[\"statistics\"], results[\"statistics_logvar\"])\n",
    "                y_pred = master_model.generative_Net(X_linspace, results[\"statistics\"])\n",
    "                y_pred_list.append(to_np_array(y_pred))\n",
    "            y_pred_list = np.concatenate(y_pred_list, 1)\n",
    "            y_pred_mean = np.mean(y_pred_list, 1)\n",
    "            y_pred_std = np.std(y_pred_list, 1)\n",
    "            ax.errorbar(to_np_array(X_linspace)[:, chosen_dim], to_np_array(y_pred_mean), yerr = to_np_array(y_pred_std), fmt=\"-r\", markersize = 3, label = \"pred\")\n",
    "        ax.plot(to_np_array(X_test)[:, chosen_dim], to_np_array(y_test).squeeze(), \".\", markersize = 3, label = \"target\")\n",
    "        \n",
    "        ax.set_xlabel(\"x_{0}\".format(chosen_dim))\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_title(task_id)\n",
    "        i += 1\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return [statistics_list]\n",
    "\n",
    "\n",
    "def plot_individual_tasks_bounce(\n",
    "    tasks,\n",
    "    num_examples_show = 30,\n",
    "    num_tasks_show = 6,\n",
    "    master_model = None,\n",
    "    model = None,\n",
    "    autoencoder = None,\n",
    "    num_shots = None,\n",
    "    highlight_top = None,\n",
    "    valid_input_dims = None,\n",
    "    target_forward_steps = 1,\n",
    "    eval_forward_steps = 1,\n",
    "    **kwargs\n",
    "    ):\n",
    "    import matplotlib.pylab as plt\n",
    "    fig = plt.figure(figsize = (25, num_tasks_show / 3 * 8))\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    tasks_key_show = np.random.choice(list(tasks.keys()), min(num_tasks_show, len(tasks)), replace = False)\n",
    "    for k, task_key in enumerate(tasks_key_show):\n",
    "        if autoencoder is not None:\n",
    "            forward_steps = list(range(1, eval_forward_steps + 1))\n",
    "            forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "            if autoencoder.is_cuda:\n",
    "                forward_steps_idx = forward_steps_idx.cuda()\n",
    "            ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), _ = tasks[task_key]\n",
    "            X_train = forward(autoencoder.encode, X_train_obs)\n",
    "            y_train = forward(autoencoder.encode, y_train_obs[:, forward_steps_idx])\n",
    "            X_test = forward(autoencoder.encode, X_test_obs)\n",
    "            y_test = forward(autoencoder.encode, y_test_obs[:, forward_steps_idx])\n",
    "        else:\n",
    "            ((X_train, y_train), (X_test, y_test)), _ = tasks[task_key]\n",
    "        num_steps = int(X_test.size(1) / 2)\n",
    "        is_cuda = X_train.is_cuda\n",
    "        X_test_numpy, y_test_numpy = to_np_array(X_test, y_test)\n",
    "        if len(X_test_numpy.shape) == 2:\n",
    "            X_test_numpy = X_test_numpy.reshape(-1, num_steps, 2)\n",
    "            y_test_numpy = y_test_numpy.reshape(-1, int(y_test_numpy.shape[1] / 2), 2)\n",
    "\n",
    "        # Get highlighted examples:\n",
    "        if highlight_top is not None:\n",
    "            relevance_train = get_relevance(X_train, y_train, master_model.statistics_Net)\n",
    "            X_sorted, y_sorted, relevance_sorted = sort_datapoints(X_train, y_train, relevance_train, top = highlight_top)\n",
    "            if len(X_sorted.shape) == 2:\n",
    "                X_sorted = X_sorted.view(-1, num_steps, 2)\n",
    "                y_sorted = y_sorted.view(-1, int(y_sorted.shape[1] / 2), 2)\n",
    "            X_sorted, y_sorted = to_np_array(X_sorted, y_sorted)\n",
    "\n",
    "        # Get model prediction:\n",
    "        if master_model is not None:\n",
    "            if num_shots is None:\n",
    "                statistics = master_model.statistics_Net.forward_inputs(X_train, y_train[:, :target_forward_steps * 2])\n",
    "            else:\n",
    "                idx = torch.LongTensor(np.random.choice(range(len(X_train)), min(len(X_train), num_shots), replace = False))\n",
    "                if is_cuda:\n",
    "                    idx = idx.cuda()\n",
    "                statistics = master_model.statistics_Net.forward_inputs(X_train[idx], y_train[idx, :target_forward_steps * 2])\n",
    "            if isinstance(statistics, tuple):\n",
    "                statistics = statistics[0]\n",
    "\n",
    "            master_model.generative_Net.set_latent_param(statistics)\n",
    "            model_core = master_model.generative_Net\n",
    "\n",
    "            # Prediction for highlighted examples:\n",
    "            if highlight_top is not None:\n",
    "                y_sorted_pred = model_core(to_Variable(X_sorted.reshape(X_sorted.shape[0], -1), is_cuda = is_cuda))\n",
    "                y_sorted_pred = to_np_array(y_sorted_pred)\n",
    "                if len(y_sorted_pred.shape) == 2:\n",
    "                    y_sorted_pred = y_sorted_pred.reshape(-1, int(y_sorted_pred.shape[1] / 2), 2)\n",
    "        else:\n",
    "            assert model is not None\n",
    "            model_core = model\n",
    "\n",
    "        preds = predict_forward(model_core, X_test, num_forward_steps = eval_forward_steps)\n",
    "        y_pred_numpy = to_np_array(reshape_time_series(preds))\n",
    "\n",
    "        # Plotting:\n",
    "        ax = fig.add_subplot(int(np.ceil(num_tasks_show / float(3))), 3, k + 1)\n",
    "        for i in range(len(X_test_numpy)):\n",
    "            if i >= num_examples_show:\n",
    "                break\n",
    "            x_ele = X_test_numpy[i]\n",
    "            if valid_input_dims is not None:\n",
    "                x_ele = x_ele[:int(valid_input_dims / 2), :]\n",
    "            y_ele = y_test_numpy[i]\n",
    "            ax.plot(np.concatenate((x_ele[:,0], y_ele[:,0])), np.concatenate((x_ele[:,1], y_ele[:,1])), \".-\", color = COLOR_LIST[i % len(COLOR_LIST)], zorder = -1)\n",
    "            ax.scatter(y_ele[:,0], y_ele[:,1], s = np.linspace(10, 20, len(y_ele[:,0])), marker = \"o\", color = \"r\", zorder = 2)\n",
    "            ax.set_title(task_key)\n",
    "            if master_model is not None or model is not None:\n",
    "                y_pred_ele = y_pred_numpy[i]\n",
    "                ax.plot(np.concatenate((x_ele[:,0], y_pred_ele[:,0])), np.concatenate((x_ele[:,1], y_pred_ele[:,1])), \".--\", color = COLOR_LIST[i % len(COLOR_LIST)], zorder = -1)\n",
    "                ax.scatter(y_pred_ele[:,0], y_pred_ele[:,1], s = np.linspace(10, 20, len(y_ele[:,0])), marker = \"o\", color = \"b\", zorder = 2)\n",
    "\n",
    "        # Plotting highlighted examples:\n",
    "        if highlight_top is not None:\n",
    "            for i in range(highlight_top):\n",
    "                x_ele = X_sorted[i]\n",
    "                y_ele = y_sorted[i]\n",
    "                ax.plot(np.concatenate((x_ele[:,0], y_ele[:,0])), np.concatenate((x_ele[:,1], y_ele[:,1])), \".-\", color = \"k\", zorder = -1)\n",
    "                ax.scatter(y_ele[:,0], y_ele[:,1], s = np.linspace(10, 20, len(y_ele[:,0])), marker = \"o\", color = \"r\", zorder = 2)\n",
    "                ax.set_title(task_key)\n",
    "                if master_model is not None or model is not None:\n",
    "                    y_pred_ele = y_sorted_pred[i]\n",
    "                    ax.plot(np.concatenate((x_ele[:,0], y_pred_ele[:,0])), np.concatenate((x_ele[:,1], y_pred_ele[:,1])), \".--\", color = \"k\", zorder = -1)\n",
    "                    ax.scatter(y_pred_ele[:,0], y_pred_ele[:,1], s = np.linspace(10, 20, len(y_ele[:,0])), marker = \"o\", color = \"k\", zorder = 2)\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_few_shot_loss(master_model, tasks, isplot = True, is_time_series = True, autoencoder = None, min_shots = None, forward_steps = [1], **kwargs):\n",
    "    if master_model is None:\n",
    "        return []\n",
    "    num_shots_list = [10, 20, 30, 40, 50, 70, 100, 200, 300, 500, 1000]\n",
    "    mse_list_whole = []\n",
    "    for task_key, task in tasks.items():\n",
    "        mse_list = []\n",
    "        if autoencoder is not None:\n",
    "            forward_steps_idx = torch.LongTensor(np.array(forward_steps) - 1)\n",
    "            if autoencoder.is_cuda:\n",
    "                forward_steps_idx = forward_steps_idx.cuda()\n",
    "            ((X_train_obs, y_train_obs), (X_test_obs, y_test_obs)), _ = task\n",
    "            X_train = forward(autoencoder.encode, X_train_obs)\n",
    "            y_train = forward(autoencoder.encode, y_train_obs[:, forward_steps_idx])\n",
    "            X_test = forward(autoencoder.encode, X_test_obs)\n",
    "            y_test = forward(autoencoder.encode, y_test_obs[:, forward_steps_idx])\n",
    "        else:\n",
    "            ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "        is_cuda = X_train.is_cuda\n",
    "        for num_shots in num_shots_list:\n",
    "            if num_shots > len(X_train):\n",
    "                continue\n",
    "            if min_shots is not None:\n",
    "                if num_shots < min_shots:\n",
    "                    continue\n",
    "            idx = torch.LongTensor(np.random.choice(range(len(X_train)), num_shots, replace = False))\n",
    "            if is_cuda:\n",
    "                idx = idx.cuda()\n",
    "            X_few_shot = X_train[idx]\n",
    "            y_few_shot = y_train[idx]\n",
    "            statistics = master_model.statistics_Net.forward_inputs(X_few_shot, y_few_shot)\n",
    "            if isinstance(statistics, tuple):\n",
    "                statistics = statistics[0]\n",
    "            if autoencoder is not None:\n",
    "                master_model.generative_Net.set_latent_param(statistics)\n",
    "                y_pred = get_forward_pred(master_model.generative_Net, X_test, forward_steps, is_time_series = is_time_series)\n",
    "                mse = kwargs[\"criterion\"](X_test, y_pred, X_test_obs, y_test_obs, autoencoder, loss_fun = nn.MSELoss()).data[0]\n",
    "            else:\n",
    "                y_test_pred = get_forward_pred(master_model.generative_Net, X_test, forward_steps, is_time_series = is_time_series, latent_param = statistics, jump_step = 2, is_flatten = True)\n",
    "                mse = nn.MSELoss()(y_test_pred, y_test).data[0]\n",
    "            mse_list.append(mse)\n",
    "        mse_list_whole.append(mse_list)\n",
    "    mse_list_whole = np.array(mse_list_whole)\n",
    "    mse_mean = mse_list_whole.mean(0)\n",
    "    mse_std = mse_list_whole.std(0)\n",
    "    if isplot:\n",
    "        import matplotlib.pylab as plt\n",
    "        plt.figure(figsize = (8,6))\n",
    "        plt.errorbar(num_shots_list[:len(mse_mean)], mse_mean, mse_std, fmt = \"o\")\n",
    "        ax = plt.gca()\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"number of shots\")\n",
    "        ax.set_ylabel(\"mse\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize = (8,6))\n",
    "        plt.errorbar(num_shots_list[:len(mse_mean)], mse_mean, mse_std, fmt = \"o\")\n",
    "        ax = plt.gca()\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"number of shots\")\n",
    "        ax.set_ylabel(\"mse\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    return mse_list_whole\n",
    "\n",
    "\n",
    "def plot_quick_learn_performance(models, tasks, learning_type = \"clone_net\", is_time_series = True, forward_steps = [1], loss_core = \"huber\", epochs = 50, lr = 1e-3, batch_size = 128, optim_type = \"adam\", isplot = True, scale = \"normal\"):\n",
    "    if not isinstance(models, dict):\n",
    "        models = {\"model_0\": models}\n",
    "    mse_dict_whole = {model_key: [] for model_key in models.keys()}\n",
    "    for model_key, model in models.items():\n",
    "        for task_key, task in tasks.items():\n",
    "            ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "            if learning_type == \"clone_net\":\n",
    "                if model.__class__.__name__ == \"Master_Model\":\n",
    "                    model_core = model.get_clone_net(X_train, y_train)\n",
    "                else:\n",
    "                    model_core = model\n",
    "                mse_list = quick_learn(model_core, X_train, y_train, validation_data = (X_test, y_test), forward_steps = forward_steps, is_time_series = is_time_series, loss_core = loss_core, \n",
    "                                       batch_size = batch_size, epochs = epochs, lr = lr, optim_type = optim_type)[0]\n",
    "            elif learning_type == \"latent_param\":\n",
    "                mse_list = model.latent_param_quick_learn(X_train, y_train, validation_data = (X_test, y_test), is_time_series = is_time_series, loss_core = loss_core, \n",
    "                                                          epochs = epochs, batch_size = batch_size, lr = lr, optim_type = optim_type, reset_latent_param = True)\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "            mse_dict_whole[model_key].append(mse_list)\n",
    "        mse_dict_whole[model_key] = np.array(mse_dict_whole[model_key])\n",
    "    epoch_list = list(range(epochs + 1))\n",
    "    if isplot:\n",
    "        import matplotlib.pylab as plt\n",
    "        plt.figure(figsize = (8,6))\n",
    "        for model_key, model in models.items():\n",
    "            plt.errorbar(epoch_list, mse_dict_whole[model_key].mean(0), mse_dict_whole[model_key].std(0), fmt = \"o\", label = model_key)\n",
    "        ax = plt.gca()\n",
    "        ax.legend()\n",
    "        if scale == \"log\":\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"number of epochs\")\n",
    "        ax.set_ylabel(\"mse\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    return mse_dict_whole\n",
    "\n",
    "\n",
    "def get_corrcoef(x, y):\n",
    "    import scipy\n",
    "    corrcoef = np.zeros((y.shape[1], x.shape[1]))\n",
    "    for i in range(corrcoef.shape[0]):\n",
    "        for j in range(corrcoef.shape[1]):\n",
    "            corrcoef[i, j] = scipy.stats.pearsonr(y[:,i], x[:, j])[0]\n",
    "    return corrcoef\n",
    "\n",
    "\n",
    "def plot_statistics_vs_z(z_list, statistics_list, mode = \"corrcoef\", title = None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    num_columns = 5\n",
    "    if isinstance(z_list, list):\n",
    "        z_list = np.stack(z_list, 0)\n",
    "    if isinstance(statistics_list, list):\n",
    "        statistics_list = np.stack(statistics_list, 0)\n",
    "    if len(z_list.shape) == 1:\n",
    "        z_list = np.expand_dims(z_list, 1)\n",
    "    z_size = z_list.shape[1]\n",
    "    num_rows = int(np.ceil(z_size / num_columns))\n",
    "    fig = plt.figure(figsize = (25, num_rows * 3.2))\n",
    "\n",
    "    for i in range(z_size):\n",
    "        ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "        for j in range(statistics_list.shape[1]):\n",
    "            ax.plot(z_list[:,i], statistics_list[:,j], color = COLOR_LIST[j], marker = \".\", linestyle = 'None', alpha = 0.6, markersize = 2)\n",
    "            ax.set_title(\"statistics vs. z_{0}\".format(i))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot coefficient for linear regression:\n",
    "    info = {}\n",
    "    if mode == \"corrcoef\":\n",
    "        print(\"statistics (row) vs. z (column) pearsonr correlation coefficient (abs value):\")\n",
    "        cross_corrcoef = get_corrcoef(z_list, statistics_list)\n",
    "        plot_matrices([np.abs(cross_corrcoef)], title = title)\n",
    "        print(\"statistics correlation matrix:\")\n",
    "        self_corrcoef = np.corrcoef(statistics_list, rowvar = False)\n",
    "        plot_matrices([np.abs(self_corrcoef)])\n",
    "        print(\"pca explained variance ratio:\")\n",
    "        pca = PCA()\n",
    "        pca.fit(statistics_list)\n",
    "        print(pca.explained_variance_ratio_)\n",
    "        \n",
    "        info[\"cross_corrcoef\"] = cross_corrcoef\n",
    "        info[\"self_corrcoef\"] = self_corrcoef\n",
    "        info[\"explained_variance_ratio\"] = pca.explained_variance_ratio_     \n",
    "    else:\n",
    "        print(\"statistics (row) vs. z (column) linear regression abs(coeff):\")\n",
    "        from sklearn import linear_model\n",
    "        reg = linear_model.LinearRegression()\n",
    "        coeff_list = []\n",
    "        for i in range(statistics_list.shape[1]):\n",
    "            reg.fit(z_list, statistics_list[:,i])\n",
    "            coeff_list.append(reg.coef_)\n",
    "        coeff_list = np.array(coeff_list)\n",
    "        plot_matrices([np.abs(coeff_list)])\n",
    "        info[\"coeff_list\"] = coeff_list\n",
    "    return info\n",
    "\n",
    "\n",
    "def plot_data_record(data_record, idx = None, is_VAE = False, tasks_train_keys = None, tasks_test_keys = None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    source = [\"loss\", \"loss_sampled\", \"mse\"] if is_VAE else [\"loss\", \"mse\"]\n",
    "    fig = plt.figure(figsize = (len(source) * 8, 6))\n",
    "    for i, key in enumerate(source):\n",
    "        if \"{0}_mean_train\".format(key) in data_record:\n",
    "            ax = fig.add_subplot(1, len(source), i + 1)\n",
    "            if idx is None:\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_mean_train\".format(key)], label = '{0}_mean_train'.format(key), c = \"b\")\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_mean_test\".format(key)], label = '{0}_mean_test'.format(key), c = \"r\")\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_median_train\".format(key)], label = '{0}_median_train'.format(key), c = \"b\", linestyle = \"--\")\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_median_test\".format(key)], label = '{0}_median_test'.format(key), c = \"r\", linestyle = \"--\")\n",
    "\n",
    "                ax.legend()\n",
    "                ax.set_xlabel(\"training step\")\n",
    "                ax.set_ylabel(key)\n",
    "                ax.set_title(\"{0} vs. training step\".format(key))\n",
    "            else:\n",
    "                if \"tasks_train\" in data_record:\n",
    "                    loss_train_list = [data_record[key][task_key][idx] for task_key in data_record[\"tasks_train\"][0].keys()]\n",
    "                    loss_test_list = [data_record[key][task_key][-1] for task_key in data_record[\"tasks_test\"][0].keys()]\n",
    "                else:\n",
    "                    loss_train_list = [data_record[key][task_key][idx] for task_key in tasks_train_keys]\n",
    "                    loss_test_list = [data_record[key][task_key][-1] for task_key in tasks_test_keys]\n",
    "                ax.hist(loss_train_list, bins = 20, density = True, alpha = 0.3, color=\"b\")\n",
    "                ax.hist(loss_test_list, bins = 20, density = True, alpha = 0.3, color=\"r\")\n",
    "                ax.axvline(x= np.mean(loss_train_list), c = \"b\", alpha = 0.6, label = \"train_mean\")\n",
    "                ax.axvline(x= np.median(loss_train_list), c = \"b\", linestyle = \"--\", alpha = 0.6, label = \"train_median\")\n",
    "                ax.axvline(x= np.mean(loss_test_list), c = \"r\", alpha = 0.6, label = \"test_mean\")\n",
    "                ax.axvline(x= np.median(loss_test_list), c = \"r\", linestyle = \"--\", alpha = 0.6, label = \"test_median\")\n",
    "                ax.legend()\n",
    "                ax.set_title(\"Histogram for {0}:\".format(key))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, z, zdim = 1, num_layers = 1, activation = \"tanh\"):\n",
    "    \"\"\"Generating latent-model data:\"\"\"\n",
    "    A0 = lambda z: np.tanh(z)\n",
    "    A1 = lambda z: z ** 2 / (1 + z ** 2)\n",
    "    A2 = lambda z: np.sin(z)\n",
    "    A3 = lambda z: z\n",
    "    A4 = lambda z: z ** 2 - z\n",
    "    input_size = x.shape[1]\n",
    "    if zdim == 1:\n",
    "        output = x[:,0:1] * A0(z) + x[:,1:2] * A1(z) + x[:,2:3] * A2(z) + x[:,3:4] * A3(z) + A4(z)\n",
    "        output = get_activation(activation)(output)\n",
    "        if num_layers >= 2:\n",
    "            pass\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_latent_model_data(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn() * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand() * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    num_layers = settings[\"num_layers\"] if \"num_layers\" in settings else 1\n",
    "    activation = settings[\"activation\"] if \"activation\" in settings else \"tanh\"\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-5,5)\n",
    "    input_size = settings[\"input_size\"] if \"input_size\" in settings else 5\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    \n",
    "    X = Variable(torch.rand(num_examples, input_size) * (xlim[1] - xlim[0]) + xlim[0], requires_grad = False)\n",
    "    y = f(X, z, zdim = settings[\"zdim\"], num_layers = num_layers, activation = activation)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.data.numpy(), y.data.numpy(), test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "\n",
    "def get_polynomial_class(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    order = 3,\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn(order + 1) * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand(order + 1) * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-3,3)\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    X = np.random.rand(num_examples, 1) * (xlim[1] - xlim[0]) + xlim[0]\n",
    "    y = z[0]\n",
    "    for i in range(1, order + 1):\n",
    "        y = y + X ** i * z[i]\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "def get_Legendre_class(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    order = 3,\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn(order + 1) * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand(order + 1) * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-1,1)\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    \n",
    "    X = np.random.rand(num_examples, 1) * (xlim[1] - xlim[0]) + xlim[0]\n",
    "    y = np.polynomial.legendre.legval(X, z)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "def get_master_function_comparison(\n",
    "    z_settings = {},\n",
    "    mode = \"sin\",\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.5\n",
    "    z_info = {}\n",
    "    if mode == \"sin\":\n",
    "        amp_range = [0.1, 5.0]\n",
    "        phase_range = [0, np.pi]\n",
    "        xlim = (-5,5)\n",
    "\n",
    "        X = np.random.uniform(xlim[0], xlim[1], [num_examples, 1])\n",
    "        amp = np.random.uniform(amp_range[0], amp_range[1])\n",
    "        phase = np.random.uniform(phase_range[0], phase_range[1])\n",
    "        y = amp * np.sin(X - phase)\n",
    "        z_info[\"z\"] = np.array([amp, phase])\n",
    "    elif mode == \"tanh\":\n",
    "        freq_range = [0.5, 1.5]\n",
    "        x0_range = [-1, 1]\n",
    "        amp_range = [1, 2]\n",
    "        const_range = [-1, 1]\n",
    "        xlim = (-5,5)\n",
    "        \n",
    "        X = np.random.uniform(xlim[0], xlim[1], [num_examples, 1])\n",
    "        freq = np.random.uniform(freq_range[0], freq_range[1])\n",
    "        x0 = np.random.uniform(x0_range[0], x0_range[1])\n",
    "        amp = np.random.uniform(amp_range[0], amp_range[1])\n",
    "        const = np.random.uniform(const_range[0], const_range[1])\n",
    "        y = np.tanh((X - x0) * freq) * amp + const\n",
    "        z_info[\"z\"] = np.array([const, amp, freq, x0])\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), z_info\n",
    "\n",
    "\n",
    "def get_master_function(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    mode = \"sawtooth\",\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    def trianglewave(\n",
    "        x,\n",
    "        frequency = 0.25,\n",
    "        height = 1,\n",
    "        ):\n",
    "        remainder = x % (1 / float(frequency))\n",
    "        slope = height * frequency * 2\n",
    "        return 2 * np.minimum(slope * remainder, 2 * height - slope * remainder) - 1\n",
    "    def S(x):\n",
    "        return np.sin(x * np.pi / 2)\n",
    "    def Gaussian(x):\n",
    "        return 1 / np.sqrt(2 * np.pi) * np.exp(- x ** 2 / 2)\n",
    "    def Softplus(x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn(4) * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand(4) * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-3,3)\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    z[0] = np.abs(z[0]) + 0.5\n",
    "    z[2] = np.abs(z[2] + 1)\n",
    "    frequency = z[0]\n",
    "    x0 = z[1]\n",
    "    amp = z[2]\n",
    "    const = z[3]\n",
    "    \n",
    "    X = np.random.rand(num_examples, 1) * (xlim[1] - xlim[0]) + xlim[0]\n",
    "    if mode == \"sawtooth\":\n",
    "        f = trianglewave\n",
    "    elif mode == \"sin\":\n",
    "        f = S\n",
    "    elif mode == \"tanh\":\n",
    "        f = np.tanh\n",
    "    elif mode == \"Gaussian\":\n",
    "        f = Gaussian\n",
    "    elif mode == \"softplus\":\n",
    "        f = Softplus\n",
    "    else:\n",
    "        raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "    \n",
    "    y = f((X - x0) * frequency) * amp + const\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "def get_bouncing_states(settings, num_examples, data_format = \"states\", is_cuda = False, **kwargs):\n",
    "    from mela.variational.util_variational import get_env_data\n",
    "    from mela.settings.a2c_env_settings import ENV_SETTINGS_CHOICE\n",
    "    render = kwargs[\"render\"] if \"render\" in kwargs else False\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    env_name = \"envBounceStates\"\n",
    "    screen_size = ENV_SETTINGS_CHOICE[env_name][\"screen_height\"]\n",
    "    ball_radius = ENV_SETTINGS_CHOICE[env_name][\"ball_radius\"]\n",
    "    \n",
    "    vertex_bottom_left = tuple(np.random.rand(2) * screen_size / 3 + ball_radius)\n",
    "    vertex_bottom_right = (screen_size - np.random.rand() * screen_size / 3 - ball_radius, np.random.rand() * screen_size / 3 + ball_radius)\n",
    "    vertex_top_right = tuple(screen_size - np.random.rand(2) * screen_size / 3 - ball_radius)\n",
    "    vertex_top_left = (np.random.rand() * screen_size / 3 + ball_radius, screen_size - np.random.rand() * screen_size / 3 - ball_radius)\n",
    "    boundaries = [vertex_bottom_left, vertex_bottom_right, vertex_top_right, vertex_top_left]\n",
    "    \n",
    "    ((X_train, y_train), (X_test, y_test), (reflected_train, reflected_test)), info = \\\n",
    "        get_env_data(\n",
    "            env_name,\n",
    "            data_format = data_format,\n",
    "            num_examples = num_examples,\n",
    "            test_size = test_size,\n",
    "            isplot = False,\n",
    "            is_cuda = False,\n",
    "            output_dims = (0,1),\n",
    "            episode_length = 200,\n",
    "            boundaries = boundaries,\n",
    "            verbose = True,\n",
    "            **kwargs\n",
    "        )\n",
    "    if is_cuda:\n",
    "        X_train = X_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        X_test = X_test.cuda()\n",
    "        y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": np.array(boundaries).reshape(-1)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
