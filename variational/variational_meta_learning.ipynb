{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from AI_scientist.prepare_dataset import Dataset_Gen\n",
    "from AI_scientist.util import plot_matrices\n",
    "from AI_scientist.settings.a2c_env_settings import ENV_SETTINGS_CHOICE\n",
    "from AI_scientist.settings.global_param import COLOR_LIST\n",
    "from AI_scientist.pytorch.net import Net\n",
    "from AI_scientist.pytorch.util_pytorch import get_activation, get_optimizer, get_criterion, Loss_Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions:\n",
    "class Master_Model(nn.Module):\n",
    "    def __init__(self, statistics_Net = None, generative_Net = None, generative_Net_logstd = None, is_cuda = False):\n",
    "        super(Master_Model, self).__init__()\n",
    "        self.statistics_Net = statistics_Net\n",
    "        self.generative_Net = generative_Net\n",
    "        self.generative_Net_logstd = generative_Net_logstd\n",
    "        self.use_net = \"generative\"\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Master_Model\"}\n",
    "        model_dict[\"statistics_Net\"] = self.statistics_Net.model_dict\n",
    "        model_dict[\"generative_Net\"] = self.generative_Net.model_dict\n",
    "        if self.generative_Net_logstd is not None:\n",
    "            model_dict[\"generative_Net_logstd\"] = self.generative_Net_logstd.model_dict   \n",
    "        return model_dict\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def get_statistics(self, X, y):\n",
    "        statistics = self.statistics_Net(torch.cat([X, y], 1))\n",
    "        if isinstance(statistics, tuple):\n",
    "            statistics = statistics[0]\n",
    "        else:\n",
    "            statistics = statistics\n",
    "        self.generative_Net.set_latent_param(statistics)\n",
    "\n",
    "    def use_clone_net(self, clone_parameters = True):\n",
    "        self.cloned_net = clone_net(self.generative_Net, clone_parameters = clone_parameters)\n",
    "        self.use_net = \"cloned\"\n",
    "\n",
    "    def use_generative_net(self):\n",
    "        self.use_net = \"generative\"\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.use_net == \"generative\":\n",
    "            return self.generative_Net(X)\n",
    "        elif self.use_net == \"cloned\":\n",
    "            return self.cloned_net(X)\n",
    "        else:\n",
    "            raise Exception(\"use_net {0} not recognized!\".format(self.use_net))\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], target = [\"statistics_Net\", \"generative_Net\"], mode = \"L1\"):\n",
    "        if target == \"all\":\n",
    "            if self.use_net == \"generative\":\n",
    "                target = [\"statistics_Net\", \"generative_Net\"]\n",
    "            elif self.use_net == \"cloned\":\n",
    "                target = [\"cloned_Net\"]\n",
    "            else:\n",
    "                raise\n",
    "        if not isinstance(target, list):\n",
    "            target = [target]\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for target_ele in target:\n",
    "            if target_ele == \"statistics_Net\":\n",
    "                assert self.use_net == \"generative\"\n",
    "                reg = reg + self.statistics_Net.get_regularization(source = source, mode = mode)\n",
    "            elif target_ele == \"generative_Net\":\n",
    "                assert self.use_net == \"generative\"\n",
    "                reg = reg + self.generative_Net.get_regularization(source = source, mode = mode)\n",
    "            elif target_ele == \"cloned_Net\":\n",
    "                assert self.use_net == \"cloned\"\n",
    "                reg = reg + self.cloned_net.get_regularization(source = source, mode = mode)\n",
    "            else:\n",
    "                raise Exception(\"target element {0} not recognized!\".format(target_ele))\n",
    "        return reg\n",
    "\n",
    "    def latent_param_quick_learn(self, X, y, validation_data, epochs = 10, batch_size = 128, lr = 1e-2, optim_type = \"LBFGS\"):\n",
    "        return self.generative_Net.latent_param_quick_learn(X = X, y = y, validation_data = validation_data, \n",
    "                                                            epochs = epochs, batch_size = batch_size, lr = lr, optim_type = optim_type)\n",
    "\n",
    "    def clone_net_quick_learn(self, X, y, validation_data, batch_size = 128, epochs = 20, lr = 1e-3, optim_type = \"adam\"):\n",
    "        self.clone_net_optimizer = get_optimizer(optim_type = optim_type, lr = lr, parameters = self.cloned_net.parameters())\n",
    "        self.criterion = get_criterion(\"huber\")\n",
    "        loss_list = []\n",
    "        X_test, y_test = validation_data\n",
    "        batch_size = min(batch_size, len(X))\n",
    "        if isinstance(X, Variable):\n",
    "            X = X.data\n",
    "        if isinstance(y, Variable):\n",
    "            y = y.data\n",
    "\n",
    "        dataset_train = data_utils.TensorDataset(X, y)\n",
    "        train_loader = data_utils.DataLoader(dataset_train, batch_size = batch_size, shuffle = True)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                X_batch = Variable(X_batch)\n",
    "                y_batch = Variable(y_batch)\n",
    "                if optim_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.clone_net_optimizer.zero_grad()\n",
    "                        y_pred = self(X_batch)\n",
    "                        loss = self.criterion(y_pred, y_batch)\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                    self.clone_net_optimizer.step(closure)\n",
    "                else:\n",
    "                    self.clone_net_optimizer.zero_grad()\n",
    "                    y_pred = self(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    self.clone_net_optimizer.step()\n",
    "            y_pred_test = self(X_test)\n",
    "            loss = get_criterion(\"mse\")(y_pred_test, y_test)\n",
    "            loss_list.append(loss.data[0])\n",
    "        loss_list = np.array(loss_list)\n",
    "        return loss_list\n",
    "\n",
    "    \n",
    "def load_model_dict(model_dict, is_cuda = False):\n",
    "    if model_dict[\"type\"] == \"Statistics_Net\":\n",
    "        net = Statistics_Net(input_size = model_dict[\"input_size\"],\n",
    "                             pre_pooling_neurons = model_dict[\"pre_pooling_neurons\"],\n",
    "                             struct_param_pre = model_dict[\"struct_param_pre\"],\n",
    "                             struct_param_post = model_dict[\"struct_param_post\"],\n",
    "                             struct_param_post_logvar = model_dict[\"struct_param_post_logvar\"],\n",
    "                             pooling = model_dict[\"pooling\"],\n",
    "                             settings = model_dict[\"settings\"],\n",
    "                             layer_type = model_dict[\"layer_type\"],\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "        net.encoding_statistics_Net.load_model_dict(model_dict[\"encoding_statistics_Net\"])\n",
    "        net.post_pooling_Net.load_model_dict(model_dict[\"post_pooling_Net\"])\n",
    "        if model_dict[\"struct_param_post_logvar\"] is not None:\n",
    "            net.post_pooling_logvar_Net.load_model_dict(model_dict[\"post_pooling_logvar_Net\"])\n",
    "    elif model_dict[\"type\"] == \"Generative_Net\":\n",
    "        learnable_latent_param = model_dict[\"learnable_latent_param\"] if \"learnable_latent_param\" in model_dict else False\n",
    "        net = Generative_Net(input_size = model_dict[\"input_size\"],\n",
    "                             W_struct_param_list = model_dict[\"W_struct_param_list\"],\n",
    "                             b_struct_param_list = model_dict[\"b_struct_param_list\"],\n",
    "                             num_context_neurons = model_dict[\"num_context_neurons\"],\n",
    "                             settings_generative = model_dict[\"settings_generative\"],\n",
    "                             settings_model = model_dict[\"settings_model\"],\n",
    "                             learnable_latent_param = True,\n",
    "                             is_cuda = is_cuda,\n",
    "                            )\n",
    "        for i, W_struct_param in enumerate(model_dict[\"W_struct_param_list\"]):\n",
    "            getattr(net, \"W_gen_{0}\".format(i)).load_model_dict(model_dict[\"W_gen_{0}\".format(i)])\n",
    "            getattr(net, \"b_gen_{0}\".format(i)).load_model_dict(model_dict[\"b_gen_{0}\".format(i)])\n",
    "        if \"latent_param\" in model_dict and model_dict[\"latent_param\"] is not None:\n",
    "            if net.latent_param is not None:\n",
    "                net.latent_param.data.copy_(torch.FloatTensor(model_dict[\"latent_param\"]))\n",
    "            else:\n",
    "                net.latent_param = Variable(torch.FloatTensor(model_dict[\"latent_param\"]), requires_grad = False)\n",
    "                if is_cuda:\n",
    "                    net.latent_param = net.latent_param.cuda()\n",
    "        if \"context\" in model_dict:\n",
    "            net.context.data.copy_(torch.FloatTensor(model_dict[\"context\"]))\n",
    "    elif model_dict[\"type\"] in [\"Master_Model\", \"Full_Net\"]: # The \"Full_Net\" name is for legacy\n",
    "        statistics_Net = load_model_dict(model_dict[\"statistics_Net\"], is_cuda = is_cuda)\n",
    "        generative_Net = load_model_dict(model_dict[\"generative_Net\"], is_cuda = is_cuda)\n",
    "        if \"generative_Net_logstd\" in model_dict:\n",
    "            generative_Net_logstd = load_model_dict(model_dict[\"generative_Net_logstd\"], is_cuda = is_cuda)\n",
    "        else:\n",
    "            generative_Net_logstd = None\n",
    "        net = Master_Model(statistics_Net = statistics_Net, generative_Net = generative_Net, generative_Net_logstd = generative_Net_logstd)\n",
    "    else:\n",
    "        raise Exception(\"type {0} not recognized!\".format(model_dict[\"type\"]))\n",
    "    return net\n",
    "\n",
    "\n",
    "class Statistics_Net(nn.Module):\n",
    "    def __init__(self, input_size, pre_pooling_neurons, struct_param_pre, struct_param_post, struct_param_post_logvar = None, pooling = \"max\", settings = {\"activation\": \"leakyRelu\"}, layer_type = \"Simple_layer\", is_cuda = False):\n",
    "        super(Statistics_Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.pre_pooling_neurons = pre_pooling_neurons\n",
    "        self.struct_param_pre = struct_param_pre\n",
    "        self.struct_param_post = struct_param_post\n",
    "        self.struct_param_post_logvar = struct_param_post_logvar\n",
    "        self.pooling = pooling\n",
    "        self.settings = settings\n",
    "        self.layer_type = layer_type\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "        self.encoding_statistics_Net = Net(input_size = self.input_size, struct_param = self.struct_param_pre, settings = self.settings, is_cuda = is_cuda)\n",
    "        self.post_pooling_Net = Net(input_size = self.pre_pooling_neurons, struct_param = self.struct_param_post, settings = self.settings, is_cuda = is_cuda)\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            self.post_pooling_logvar_Net = Net(input_size = self.pre_pooling_neurons, struct_param = self.struct_param_post_logvar, settings = self.settings, is_cuda = is_cuda)\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Statistics_Net\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"pre_pooling_neurons\"] = self.pre_pooling_neurons\n",
    "        model_dict[\"struct_param_pre\"] = self.struct_param_pre\n",
    "        model_dict[\"struct_param_post\"] = self.struct_param_post\n",
    "        model_dict[\"struct_param_post_logvar\"] = self.struct_param_post_logvar\n",
    "        model_dict[\"pooling\"] = self.pooling\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"layer_type\"] = self.layer_type\n",
    "        model_dict[\"encoding_statistics_Net\"] = self.encoding_statistics_Net.model_dict\n",
    "        model_dict[\"post_pooling_Net\"] = self.post_pooling_Net.model_dict\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            model_dict[\"post_pooling_logvar_Net\"] = self.post_pooling_logvar_Net.model_dict\n",
    "        return model_dict\n",
    "    \n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def forward(self, input):\n",
    "        encoding = self.encoding_statistics_Net(input)\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = encoding.mean(0)\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled = encoding.max(0)[0]\n",
    "        else:\n",
    "            raise Exception(\"pooling {0} not recognized!\".format(self.pooling))\n",
    "        output = self.post_pooling_Net(pooled.unsqueeze(0))\n",
    "        if self.struct_param_post_logvar is None:\n",
    "            return output\n",
    "        else:\n",
    "            logvar = self.post_pooling_logvar_Net(pooled.unsqueeze(0))\n",
    "            return output, logvar\n",
    "    \n",
    "    def forward_inputs(self, X, y):\n",
    "        return self(torch.cat([X, y], 1))\n",
    "    \n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = self.encoding_statistics_Net.get_regularization(source = source, mode = mode) + \\\n",
    "              self.post_pooling_Net.get_regularization(source = source, mode = mode)\n",
    "        if self.struct_param_post_logvar is not None:\n",
    "            reg = reg + self.post_pooling_logvar_Net.get_regularization(source = source, mode = mode)\n",
    "        return reg\n",
    "\n",
    "\n",
    "class Generative_Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size,\n",
    "        W_struct_param_list,\n",
    "        b_struct_param_list, \n",
    "        num_context_neurons = 0, \n",
    "        settings_generative = {\"activation\": \"leakyRelu\"}, \n",
    "        settings_model = {\"activation\": \"leakyRelu\"}, \n",
    "        learnable_latent_param = False,\n",
    "        last_layer_linear = False,\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Generative_Net, self).__init__()\n",
    "        assert(len(W_struct_param_list) == len(b_struct_param_list))\n",
    "        self.input_size = input_size\n",
    "        self.W_struct_param_list = W_struct_param_list\n",
    "        self.b_struct_param_list = b_struct_param_list\n",
    "        self.num_context_neurons = num_context_neurons\n",
    "        self.settings_generative = settings_generative\n",
    "        self.settings_model = settings_model\n",
    "        self.learnable_latent_param = learnable_latent_param\n",
    "        self.last_layer_linear = last_layer_linear\n",
    "        self.is_cuda = is_cuda\n",
    "\n",
    "        for i, W_struct_param in enumerate(self.W_struct_param_list):\n",
    "            setattr(self, \"W_gen_{0}\".format(i), Net(input_size = self.input_size + num_context_neurons, struct_param = W_struct_param, settings = self.settings_generative, is_cuda = is_cuda))\n",
    "            setattr(self, \"b_gen_{0}\".format(i), Net(input_size = self.input_size + num_context_neurons, struct_param = self.b_struct_param_list[i], settings = self.settings_generative, is_cuda = is_cuda))\n",
    "        # Setting up latent param and context param:\n",
    "        self.latent_param = nn.Parameter(torch.randn(1, self.input_size)) if learnable_latent_param else None\n",
    "        if self.num_context_neurons > 0:\n",
    "            self.context = nn.Parameter(torch.randn(1, self.num_context_neurons))\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Generative_Net\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"W_struct_param_list\"] = self.W_struct_param_list\n",
    "        model_dict[\"b_struct_param_list\"] = self.b_struct_param_list\n",
    "        model_dict[\"num_context_neurons\"] = self.num_context_neurons\n",
    "        model_dict[\"settings_generative\"] = self.settings_generative\n",
    "        model_dict[\"settings_model\"] = self.settings_model\n",
    "        model_dict[\"learnable_latent_param\"] = self.learnable_latent_param\n",
    "        model_dict[\"last_layer_linear\"] = self.last_layer_linear\n",
    "        for i, W_struct_param in enumerate(self.W_struct_param_list):\n",
    "            model_dict[\"W_gen_{0}\".format(i)] = getattr(self, \"W_gen_{0}\".format(i)).model_dict\n",
    "            model_dict[\"b_gen_{0}\".format(i)] = getattr(self, \"b_gen_{0}\".format(i)).model_dict\n",
    "        if self.latent_param is None:\n",
    "            model_dict[\"latent_param\"] = None\n",
    "        else:\n",
    "            model_dict[\"latent_param\"] = self.latent_param.cpu().data.numpy() if self.is_cuda else self.latent_param.data.numpy()\n",
    "        if hasattr(self, \"context\"):\n",
    "            model_dict[\"context\"] = self.context.data.numpy() if not self.is_cuda else self.context.cpu().data.numpy()\n",
    "        return model_dict\n",
    "    \n",
    "    def set_latent_param_learnable(self, mode):\n",
    "        if mode == \"on\":\n",
    "            if not self.learnable_latent_param:\n",
    "                self.learnable_latent_param = True\n",
    "                if self.latent_param is None:\n",
    "                    self.latent_param = nn.Parameter(torch.randn(1, self.input_size))\n",
    "                else:\n",
    "                    self.latent_param = nn.Parameter(self.latent_param.data)\n",
    "            else:\n",
    "                assert isinstance(self.latent_param, nn.Parameter)\n",
    "        elif mode == \"off\":\n",
    "            if self.learnable_latent_param:\n",
    "                assert isinstance(self.latent_param, nn.Parameter)\n",
    "                self.learnable_latent_param = False\n",
    "                self.latent_param = Variable(self.latent_param.data, requires_grad = False)\n",
    "            else:\n",
    "                assert isinstance(self.latent_param, Variable) or self.latent_param is None\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)\n",
    "\n",
    "    def init_weights_bias(self, latent_param):\n",
    "        if self.num_context_neurons > 0:\n",
    "            latent_param = torch.cat([latent_param, self.context], 1)\n",
    "        for i in range(len(self.W_struct_param_list)):\n",
    "            setattr(self, \"W_{0}\".format(i), (getattr(self, \"W_gen_{0}\".format(i))(latent_param)).squeeze(0))\n",
    "            setattr(self, \"b_{0}\".format(i), getattr(self, \"b_gen_{0}\".format(i))(latent_param))       \n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, isplot = False, latent_param = None):\n",
    "        if latent_param is not None:\n",
    "            self.init_weights_bias(latent_param)\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        if W_source is not None:\n",
    "            for k in range(len(self.W_struct_param_list)):\n",
    "                if W_source == \"core\":\n",
    "                    W = getattr(self, \"W_{0}\".format(k)).data.numpy()\n",
    "                else:\n",
    "                    raise Exception(\"W_source '{0}' not recognized!\".format(W_source))\n",
    "                W_list.append(W)\n",
    "        if b_source is not None:\n",
    "            for k in range(len(self.b_struct_param_list)):\n",
    "                if b_source == \"core\":\n",
    "                    b = getattr(self, \"b_{0}\".format(k)).data.numpy()\n",
    "                else:\n",
    "                    raise Exception(\"b_source '{0}' not recognized!\".format(b_source))\n",
    "                b_list.append(b)\n",
    "        if isplot:\n",
    "            if W_source is not None:\n",
    "                print(\"weight {0}:\".format(W_source))\n",
    "                plot_matrices(W_list)\n",
    "            if b_source is not None:\n",
    "                print(\"bias {0}:\".format(b_source))\n",
    "                plot_matrices(b_list)\n",
    "        return W_list, b_list\n",
    "\n",
    "    \n",
    "    def set_latent_param(self, latent_param):\n",
    "        assert isinstance(latent_param, Variable), \"The latent_param must be a Variable!\"\n",
    "        if self.learnable_latent_param:\n",
    "            self.latent_param.data.copy_(latent_param.data)\n",
    "        else:\n",
    "            self.latent_param = latent_param\n",
    "    \n",
    "    \n",
    "    def latent_param_quick_learn(self, X, y, validation_data, batch_size = 128, epochs = 10, lr = 1e-2, optim_type = \"LBFGS\"):\n",
    "        assert self.learnable_latent_param is True, \"To quick-learn latent_param, you must set learnable_latent_param as True!\"\n",
    "        self.latent_param_optimizer = get_optimizer(optim_type = optim_type, lr = lr, parameters = [self.latent_param])\n",
    "        self.criterion = get_criterion(\"huber\")\n",
    "        loss_list = []\n",
    "        X_test, y_test = validation_data\n",
    "        batch_size = min(batch_size, len(X))\n",
    "        if isinstance(X, Variable):\n",
    "            X = X.data\n",
    "        if isinstance(y, Variable):\n",
    "            y = y.data\n",
    "\n",
    "        dataset_train = data_utils.TensorDataset(X, y)\n",
    "        train_loader = data_utils.DataLoader(dataset_train, batch_size = batch_size, shuffle = True)\n",
    "        for i in range(epochs):\n",
    "            for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                X_batch = Variable(X_batch)\n",
    "                y_batch = Variable(y_batch)\n",
    "                if optim_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.latent_param_optimizer.zero_grad()\n",
    "                        y_pred = self(X_batch)\n",
    "                        loss = self.criterion(y_pred, y_batch)\n",
    "                        loss.backward()\n",
    "                        return loss\n",
    "                    self.latent_param_optimizer.step(closure)\n",
    "                else:\n",
    "                    self.latent_param_optimizer.zero_grad()\n",
    "                    y_pred = self(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    self.latent_param_optimizer.step()\n",
    "            y_pred_test = self(X_test)\n",
    "            loss = get_criterion(\"mse\")(y_pred_test, y_test)\n",
    "            loss_list.append(loss.data[0])\n",
    "        loss_list = np.array(loss_list)\n",
    "        return loss_list\n",
    "\n",
    "\n",
    "    def forward(self, input, latent_param = None):\n",
    "        if latent_param is None:\n",
    "            latent_param = self.latent_param\n",
    "        self.init_weights_bias(latent_param)\n",
    "        output = input\n",
    "        for i in range(len(self.W_struct_param_list)):\n",
    "            output = torch.matmul(output, getattr(self, \"W_{0}\".format(i))) + getattr(self, \"b_{0}\".format(i))\n",
    "            if i == len(self.W_struct_param_list) - 1 and hasattr(self, \"last_layer_linear\") and self.last_layer_linear:\n",
    "                activation = \"linear\"\n",
    "            else:\n",
    "                activation = self.settings_model[\"activation\"] if \"activation\" in self.settings_model else \"leakyRelu\"\n",
    "            output = get_activation(activation)(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = Variable(torch.FloatTensor(np.array([0])), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for reg_type in source:\n",
    "            if reg_type == \"weight\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    if mode == \"L1\":\n",
    "                        reg = reg + getattr(self, \"W_{0}\".format(i)).abs().sum()\n",
    "                    else:\n",
    "                        raise\n",
    "            elif reg_type == \"bias\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    if mode == \"L1\":\n",
    "                        reg = reg + getattr(self, \"b_{0}\".format(i)).abs().sum()\n",
    "                    else:\n",
    "                        raise\n",
    "            elif reg_type == \"W_gen\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    reg = reg + getattr(self, \"W_gen_{0}\".format(i)).get_regularization(source = source, mode = mode)\n",
    "            elif reg_type == \"b_gen\":\n",
    "                for i in range(len(self.W_struct_param_list)):\n",
    "                    reg = reg + getattr(self, \"b_gen_{0}\".format(i)).get_regularization(source = source, mode = mode)\n",
    "            else:\n",
    "                raise Exception(\"source {0} not recognized!\".format(reg_type))\n",
    "        return reg\n",
    "\n",
    "\n",
    "class VAE_Loss(nn.Module):\n",
    "    def __init__(self, criterion, prior = \"Gaussian\", beta = 1):\n",
    "        super(VAE_Loss, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.prior = \"Gaussian\"\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, input, target, mu, logvar):\n",
    "        reconstuction_loss = self.criterion(input, target)\n",
    "        if self.prior == \"Gaussian\":\n",
    "            KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        else:\n",
    "            raise Exception(\"prior {0} not recognized!\".format(self.prior))\n",
    "        return reconstuction_loss, KLD * self.beta\n",
    "\n",
    "\n",
    "def get_relevance(X, y, statistics_Net):\n",
    "    concat = torch.cat([X, y], 1)\n",
    "    max_datapoint = statistics_Net.encoding_statistics_Net(concat).max(0)[1].data.numpy()\n",
    "    unique, counts = np.unique(max_datapoint, return_counts = True)\n",
    "    relevance = np.zeros(len(X))\n",
    "    relevance[unique] = counts\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def sample_Gaussian(mu, logvar):\n",
    "    std = logvar.mul(0.5).exp_()\n",
    "    eps = Variable(torch.randn(std.size()), requires_grad = False)\n",
    "    if mu.is_cuda:\n",
    "        eps = eps.cuda()\n",
    "    return mu + std * eps\n",
    "\n",
    "\n",
    "def clone_net(generative_Net, layer_type = \"Simple_Layer\", clone_parameters = True):\n",
    "    W_init_list = []\n",
    "    b_init_list = []\n",
    "    input_size = generative_Net.W_struct_param_list[0][-1][0][0]\n",
    "    struct_param = []\n",
    "    statistics = generative_Net.latent_param\n",
    "    if clone_parameters and generative_Net.num_context_neurons > 0:\n",
    "        statistics = torch.cat([statistics, generative_Net.context], 1)\n",
    "    for i in range(len(generative_Net.W_struct_param_list)):\n",
    "        num_neurons = generative_Net.b_struct_param_list[i][-1][0]\n",
    "        layer_struct_param = [num_neurons, layer_type, {}]\n",
    "        struct_param.append(layer_struct_param)\n",
    "        if clone_parameters:\n",
    "            W_init = (getattr(generative_Net, \"W_gen_{0}\".format(i))(statistics)).squeeze(0)\n",
    "            b_init = getattr(generative_Net, \"b_gen_{0}\".format(i))(statistics)\n",
    "            if generative_Net.is_cuda:\n",
    "                W_init = W_init.cpu()\n",
    "                b_init = b_init.cpu()\n",
    "            W_init_list.append(W_init.data.numpy())\n",
    "            b_init_list.append(b_init.data.numpy()[0])\n",
    "        else:\n",
    "            W_init_list.append(None)\n",
    "            b_init_list.append(None)\n",
    "    return Net(input_size = input_size, struct_param = struct_param, W_init_list = W_init_list, b_init_list = b_init_list, settings = generative_Net.settings_model)\n",
    "\n",
    "\n",
    "def get_nets(\n",
    "    input_size,\n",
    "    output_size,\n",
    "    main_hidden_neurons = [20, 20],\n",
    "    pre_pooling_neurons = 60,\n",
    "    statistics_output_neurons = 10,\n",
    "    num_context_neurons = 0,\n",
    "    struct_param_gen_base = None,\n",
    "    struct_param_pre = None,\n",
    "    struct_param_post = None,\n",
    "    struct_param_post_logvar = None,\n",
    "    statistics_pooling = \"mean\",\n",
    "    activation_statistics = \"leakyRelu\",\n",
    "    activation_generative = \"leakyRelu\",\n",
    "    activation_model = \"leakyRelu\",\n",
    "    learnable_latent_param = False,\n",
    "    isParallel = False,\n",
    "    is_VAE = False,\n",
    "    is_uncertainty_net = False,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    layer_type = \"Simple_Layer\"\n",
    "    struct_param_pre = [\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [pre_pooling_neurons, layer_type, {\"activation\": \"linear\"}],\n",
    "    ] if struct_param_pre is None else struct_param_pre\n",
    "    struct_param_post = [\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [statistics_output_neurons, layer_type, {\"activation\": \"linear\"}],\n",
    "    ] if struct_param_post is None else struct_param_post\n",
    "    if is_VAE or is_uncertainty_net:\n",
    "        if struct_param_post_logvar is None:\n",
    "            struct_param_post_logvar = struct_param_post\n",
    "    statistics_Net = Statistics_Net(input_size = input_size + output_size,\n",
    "                                    pre_pooling_neurons = pre_pooling_neurons,\n",
    "                                    struct_param_pre = struct_param_pre,\n",
    "                                    struct_param_post = struct_param_post,\n",
    "                                    struct_param_post_logvar = struct_param_post_logvar,\n",
    "                                    pooling = statistics_pooling,\n",
    "                                    settings = {\"activation\": activation_statistics},\n",
    "                                    is_cuda = is_cuda,\n",
    "                                   )\n",
    "\n",
    "    # For Generative_Net:\n",
    "    struct_param_gen_base = [\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "        [60, layer_type, {}],\n",
    "    ] if struct_param_gen_base is None else struct_param_gen_base\n",
    "    \n",
    "    W_struct_param_list = []\n",
    "    b_struct_param_list = []\n",
    "    all_neurons = list(main_hidden_neurons) + [output_size]\n",
    "    for i, num_neurons in enumerate(all_neurons):\n",
    "        num_neurons_prev = all_neurons[i - 1] if i > 0 else input_size\n",
    "        struct_param_weight = struct_param_gen_base + [[(num_neurons_prev, num_neurons), layer_type, {\"activation\": \"linear\"}]]\n",
    "        struct_param_bias = struct_param_gen_base + [[num_neurons, layer_type, {\"activation\": \"linear\"}]]\n",
    "        W_struct_param_list.append(struct_param_weight)\n",
    "        b_struct_param_list.append(struct_param_bias)\n",
    "    generative_Net = Generative_Net(input_size = statistics_output_neurons,\n",
    "                                    num_context_neurons = num_context_neurons,\n",
    "                                    W_struct_param_list = W_struct_param_list,\n",
    "                                    b_struct_param_list = b_struct_param_list,\n",
    "                                    settings_generative = {\"activation\": activation_generative},\n",
    "                                    settings_model = {\"activation\": activation_model},\n",
    "                                    learnable_latent_param = learnable_latent_param,\n",
    "                                    last_layer_linear = True,\n",
    "                                    is_cuda = is_cuda,\n",
    "                                   )\n",
    "    if is_uncertainty_net:\n",
    "        generative_Net_logstd = Generative_Net(input_size = statistics_output_neurons,\n",
    "                                                num_context_neurons = num_context_neurons,\n",
    "                                                W_struct_param_list = W_struct_param_list,\n",
    "                                                b_struct_param_list = b_struct_param_list,\n",
    "                                                settings_generative = {\"activation\": activation_generative},\n",
    "                                                settings_model = {\"activation\": activation_model},\n",
    "                                                learnable_latent_param = learnable_latent_param,\n",
    "                                                last_layer_linear = True,\n",
    "                                                is_cuda = is_cuda,\n",
    "                                               )\n",
    "    else:\n",
    "        generative_Net_logstd = None\n",
    "    if isParallel:\n",
    "        print(\"Using Parallel training.\")\n",
    "        statistics_Net = nn.DataParallel(statistics_Net)\n",
    "        generative_Net = nn.DataParallel(generative_Net)\n",
    "        if is_uncertainty_net:\n",
    "            generative_Net_logstd = nn.DataParallel(generative_Net_logstd)\n",
    "    return statistics_Net, generative_Net, generative_Net_logstd\n",
    "\n",
    "\n",
    "def get_tasks(task_id_list, num_train, num_test, task_settings = {}, is_cuda = False, **kwargs):\n",
    "    num_tasks = num_train + num_test\n",
    "    tasks = {}\n",
    "    for j in range(num_tasks):\n",
    "        task_id = np.random.choice(task_id_list)\n",
    "        num_examples = task_settings[\"num_examples\"] if \"num_examples\" in task_settings else 2000\n",
    "        if task_id[:12] == \"latent-linear\":\n",
    "            task = get_latent_model_data(task_settings[\"z_settings\"], settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:10] == \"polynomial\":\n",
    "            order = int(task_id.split(\"-\")[1])\n",
    "            task = get_polynomial_class(task_settings[\"z_settings\"], order = order, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:8] == \"Legendre\":\n",
    "            order = int(task_id.split(\"-\")[1])\n",
    "            task = get_Legendre_class(task_settings[\"z_settings\"], order = order, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id[:2] == \"M-\":\n",
    "            task_mode = task_id.split(\"-\")[1]\n",
    "            task = get_master_function(task_settings[\"z_settings\"], mode = task_mode, settings = task_settings, num_examples = num_examples, is_cuda = is_cuda,)\n",
    "        elif task_id == \"bounce-states\":\n",
    "            task = get_bouncing_states(data_format = \"states\", settings = task_settings, num_examples = num_examples, is_cuda = is_cuda, **kwargs)\n",
    "        elif task_id == \"bounce-images\":\n",
    "            task = get_bouncing_states(data_format = \"images\", settings = task_settings, num_examples = num_examples, is_cuda = is_cuda, **kwargs)\n",
    "        else:\n",
    "            task = Dataset_Gen(task_id, settings = {\"domain\": (-3,3),\n",
    "                                                    \"num_train\": 200,\n",
    "                                                    \"num_test\": 200,\n",
    "                                                    \"isTorch\": True,\n",
    "                                                   })\n",
    "        for k in range(num_tasks):\n",
    "            if \"{0}_{1}\".format(task_id, k) in tasks:\n",
    "                continue\n",
    "            else:\n",
    "                task_key = \"{0}_{1}\".format(task_id, k)\n",
    "        tasks[task_key] = task\n",
    "    task_id_train = np.random.choice(list(tasks.keys()), num_train, replace = False).tolist()\n",
    "    tasks_train = {key: value for key, value in tasks.items() if key in task_id_train}\n",
    "    tasks_test = {key: value for key, value in tasks.items() if key not in task_id_train}\n",
    "    tasks_train = OrderedDict(sorted(tasks_train.items(), key=lambda t: t[0]))\n",
    "    tasks_test = OrderedDict(sorted(tasks_test.items(), key=lambda t: t[0]))\n",
    "    return tasks_train, tasks_test\n",
    "\n",
    "\n",
    "def evaluate(task, statistics_Net, generative_Net, generative_Net_logstd = None, criterion = None, is_VAE = False, is_regulated_net = False):\n",
    "    ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "    loss_fun = Loss_Fun(core = \"mse\")\n",
    "    if is_VAE:\n",
    "        statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "        statistics_sampled = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "        y_pred_sampled = generative_Net(X_test, statistics_sampled)\n",
    "        loss_sampled, KLD = criterion(y_pred_sampled, y_test, statistics_mu, statistics_logvar)\n",
    "        \n",
    "        y_pred = generative_Net(X_test, statistics_mu)\n",
    "        loss = criterion.criterion(y_pred, y_test)\n",
    "        mse = loss_fun(y_pred, y_test)\n",
    "        return loss.data[0], loss_sampled.data[0], mse.data[0], KLD.data[0]\n",
    "    else:\n",
    "        if generative_Net_logstd is None:\n",
    "            statistics = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "            if is_regulated_net:\n",
    "                statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "            y_pred = generative_Net(X_test, statistics)\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            mse = loss_fun(y_pred, y_test)\n",
    "        else:\n",
    "            statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "            if is_regulated_net:\n",
    "                statistics_mu = get_regulated_statistics(generative_Net, statistics_mu)\n",
    "                statistics_logvar = get_regulated_statistics(generative_Net_logstd, statistics_logvar)\n",
    "            y_pred = generative_Net(X_test, statistics_mu)\n",
    "            y_pred_logstd = generative_Net_logstd(X_test, statistics_logvar)\n",
    "            loss = criterion(y_pred, y_test, log_std = y_pred_logstd)\n",
    "            mse = loss_fun(y_pred, y_test)\n",
    "        return loss.data[0], loss.data[0], mse.data[0], 0\n",
    "\n",
    "\n",
    "def get_reg(reg_dict, statistics_Net = None, generative_Net = None, is_cuda = False):\n",
    "    reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "    if is_cuda:\n",
    "        reg = reg.cuda()\n",
    "    for net_name, reg_info in reg_dict.items():\n",
    "        if net_name == \"statistics_Net\":\n",
    "            reg_net = statistics_Net\n",
    "        elif net_name == \"generative_Net\":\n",
    "            reg_net = generative_Net\n",
    "        if isinstance(reg_net, nn.DataParallel):\n",
    "            reg_net = reg_net.module\n",
    "        for reg_type, reg_amp in reg_info.items():\n",
    "            reg = reg + reg_net.get_regularization(source = [reg_type]) * reg_amp\n",
    "    return reg\n",
    "\n",
    "\n",
    "def get_regulated_statistics(generative_Net, statistics):\n",
    "    assert len(statistics.view(-1)) == len(generative_Net.struct_param) * 2 or len(statistics.view(-1)) == len(generative_Net.struct_param)\n",
    "    if len(statistics.view(-1)) == len(generative_Net.struct_param) * 2:\n",
    "        statistics = {i: statistics.view(-1)[2*i: 2*i+2] for i in range(len(generative_Net.struct_param))}\n",
    "    else:\n",
    "        statistics = {i: statistics.view(-1)[i: i+1] for i in range(len(generative_Net.struct_param))}\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def load_trained_models(filename):\n",
    "    statistics_Net = torch.load(filename + \"statistics_Net.pt\")\n",
    "    generative_Net = torch.load(filename + \"generative_Net.pt\")\n",
    "    data_record = pickle.load(open(filename + \"data.p\", \"rb\"))\n",
    "    return statistics_Net, generative_Net, data_record\n",
    "\n",
    "\n",
    "def plot_task_ensembles(tasks, statistics_Net, generative_Net, is_VAE = False, is_regulated_net = False, title = None, isplot = True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    statistics_list = []\n",
    "    z_list = []\n",
    "    for task_key, task in tasks.items():\n",
    "        (_, (X_test, y_test)), info = task \n",
    "        if is_VAE:\n",
    "            statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_test, y_test], 1))\n",
    "            statistics = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "        else:\n",
    "            statistics = statistics_Net(torch.cat([X_test, y_test], 1))\n",
    "            if isinstance(statistics, tuple):\n",
    "                statistics = statistics[0]\n",
    "        if X_test.is_cuda:\n",
    "            statistics_cpu = statistics.cpu()\n",
    "        else:\n",
    "            statistics_cpu = statistics\n",
    "        statistics_list.append(statistics_cpu.data.numpy()[0])\n",
    "        if is_regulated_net:\n",
    "            statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "        y_pred = generative_Net(X_test, statistics)\n",
    "        z_list.append(info[\"z\"])\n",
    "        if isplot:\n",
    "            if X_test.is_cuda:\n",
    "                y_pred = y_pred.cpu()\n",
    "                y_test = y_test.cpu()\n",
    "            plt.plot(y_test.data.numpy()[:,0], y_pred.data.numpy()[:,0], \".\", markersize = 1, alpha = 0.5)\n",
    "    if title is not None and isplot:\n",
    "        plt.title(title)\n",
    "    if isplot:\n",
    "        plt.show()\n",
    "    return np.array(statistics_list), np.array(z_list)\n",
    "\n",
    "\n",
    "def plot_individual_tasks(tasks, statistics_Net, generative_Net, generative_Net_logstd = None, max_plots = 24, \n",
    "                          is_VAE = False, is_regulated_net = False, xlim = (-4, 4), sample_times = None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    num_columns = 8\n",
    "    max_plots = max(num_columns * 3, max_plots)\n",
    "    num_rows = int(np.ceil(max_plots / num_columns))\n",
    "    fig = plt.figure(figsize = (25, num_rows * 3.3))\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    statistics_list = []\n",
    "    if len(tasks) > max_plots:\n",
    "        chosen_id = np.random.choice(list(tasks.keys()), max_plots, replace = False).tolist()\n",
    "        chosen_id = sorted(chosen_id)\n",
    "    else:\n",
    "        chosen_id = sorted(list(tasks.keys()))\n",
    "    i = 0\n",
    "    is_cuda = tasks[list(tasks.keys())[0]][0][0][0].is_cuda\n",
    "    if xlim is not None:\n",
    "        X_linspace = Variable(torch.linspace(xlim[0], xlim[1], 200).unsqueeze(1))\n",
    "        if is_cuda:\n",
    "            X_linspace = X_linspace.cuda()\n",
    "    for task_id, task in tasks.items():\n",
    "        ((X_train, y_train), (X_test, y_test)), info = task\n",
    "        input_size = X_test.size(1)\n",
    "        chosen_dim = np.random.choice(range(input_size))\n",
    "        if is_VAE:\n",
    "            statistics, statistics_logvar = statistics_Net(torch.cat([X_test, y_test], 1))\n",
    "        else:\n",
    "            if generative_Net_logstd is None:\n",
    "                statistics = statistics_Net(torch.cat([X_test, y_test], 1))\n",
    "            else:\n",
    "                statistics, statistics_logvar = statistics_Net(torch.cat([X_test, y_test], 1))\n",
    "        if X_test.is_cuda:\n",
    "            statistics_cpu = statistics.cpu()\n",
    "        else:\n",
    "            statistics_cpu = statistics\n",
    "        statistics_list.append(statistics_cpu.data.numpy().squeeze())\n",
    "        if task_id not in chosen_id:\n",
    "            continue\n",
    "        \n",
    "        ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "        if sample_times is None:\n",
    "            if input_size == 1:\n",
    "                if is_regulated_net:\n",
    "                    statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "                y_pred = generative_Net(X_linspace, statistics)\n",
    "                if X_test.is_cuda:\n",
    "                    X_linspace_cpu = X_linspace.cpu()\n",
    "                    y_pred_cpu = y_pred.cpu()\n",
    "                else:\n",
    "                    X_linspace_cpu = X_linspace\n",
    "                    y_pred_cpu = y_pred\n",
    "                ax.plot(X_linspace_cpu.data.numpy()[:, chosen_dim], y_pred_cpu.data.numpy().squeeze(), \"-r\", markersize = 3, label = \"pred\")\n",
    "                if generative_Net_logstd is not None:\n",
    "                    if is_regulated_net:\n",
    "                        statistics_logvar = get_regulated_statistics(generative_Net_logstd, statistics_logvar)\n",
    "                    y_pred_std = torch.exp(generative_Net_logstd(X_linspace, statistics_logvar))\n",
    "                    if X_test.is_cuda:\n",
    "                        y_pred_std_cpu = y_pred_std.cpu()\n",
    "                    else:\n",
    "                        y_pred_std_cpu = y_pred_std\n",
    "                    ax.fill_between(X_linspace_cpu.data.numpy()[:, chosen_dim], (y_pred_cpu - y_pred_std_cpu).data.numpy().squeeze(), (y_pred_cpu + y_pred_std_cpu).data.numpy().squeeze(), color = \"r\", alpha = 0.3)\n",
    "#             else:\n",
    "#                 if is_regulated_net:\n",
    "#                     statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "#                 y_pred = generative_Net(X_linspace, statistics)\n",
    "        else:\n",
    "            y_pred_list = []\n",
    "            for j in range(sample_times):\n",
    "                statistics_sampled = sample_Gaussian(statistics, statistics_logvar)\n",
    "                y_pred = generative_Net(X_linspace, statistics)\n",
    "                y_pred_list.append(y_pred.data.numpy())\n",
    "            y_pred_list = np.concatenate(y_pred_list, 1)\n",
    "            y_pred_mean = np.mean(y_pred_list, 1)\n",
    "            y_pred_std = np.std(y_pred_list, 1)\n",
    "            print(y_pred_std.mean())\n",
    "            if is_cuda:\n",
    "                X_linspace_cpu = X_linspace.cpu()\n",
    "                y_pred_mean_cpu = y_pred_mean.cpu()\n",
    "                y_pred_std_cpu = y_pred_std.cpu()\n",
    "            else:\n",
    "                X_linspace_cpu = X_linspace\n",
    "                y_pred_mean_cpu = y_pred_mean\n",
    "                y_pred_std_cpu = y_pred_std\n",
    "            ax.errorbar(X_linspace_cpu.data.numpy()[:, chosen_dim], y_pred_mean_cpu, yerr = y_pred_std_cpu, fmt=\"-r\", markersize = 3, label = \"pred\")\n",
    "        if is_cuda:\n",
    "            X_test_cpu = X_test.cpu()\n",
    "            y_test_cpu = y_test.cpu()\n",
    "        else:\n",
    "            X_test_cpu = X_test\n",
    "            y_test_cpu = y_test  \n",
    "        ax.plot(X_test_cpu.data.numpy()[:, chosen_dim], y_test_cpu.data.numpy().squeeze(), \".\", markersize = 3, label = \"target\")\n",
    "        \n",
    "        ax.set_xlabel(\"x_{0}\".format(chosen_dim))\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_title(task_id)\n",
    "        i += 1\n",
    "    plt.show()\n",
    "    return [statistics_list]\n",
    "\n",
    "\n",
    "def plot_individual_tasks_bounce(tasks, num_examples_show = 40, num_tasks_show = 6, master_model = None, num_shots = None):\n",
    "    import matplotlib.pylab as plt\n",
    "    fig = plt.figure(figsize = (25, num_tasks_show / 3 * 8))\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    tasks_key_show = np.random.choice(list(tasks.keys()), num_tasks_show)\n",
    "    for k, task_key in enumerate(tasks_key_show):\n",
    "        ((X_train, y_train), (X_test, y_test)), _ = tasks[task_key]\n",
    "        num_steps = int(X_test.size(1) / 2)\n",
    "        is_cuda = X_train.is_cuda\n",
    "        X_test_numpy = X_test.cpu() if is_cuda else X_test\n",
    "        y_test_numpy = y_test.cpu() if is_cuda else y_test\n",
    "        X_test_numpy = X_test_numpy.data.numpy().reshape(-1, num_steps, 2)\n",
    "        y_test_numpy = y_test_numpy.data.numpy().reshape(-1, 1, 2)\n",
    "        if master_model is not None:\n",
    "            if num_shots is None:\n",
    "                statistics = master_model.statistics_Net.forward_inputs(X_train, y_train)\n",
    "            else:\n",
    "                idx = torch.LongTensor(np.random.choice(range(len(X_train)), num_shots, replace = False))\n",
    "                if is_cuda:\n",
    "                    idx = idx.cuda()\n",
    "                statistics = master_model.statistics_Net.forward_inputs(X_train[idx], y_train[idx])\n",
    "            y_pred = master_model.generative_Net(X_test, statistics)\n",
    "            y_pred_numpy = y_pred.cpu() if is_cuda else y_pred\n",
    "            y_pred_numpy = y_pred_numpy.data.numpy().reshape(-1, 1, 2)\n",
    "        \n",
    "        ax = fig.add_subplot(int(np.ceil(num_tasks_show / float(3))), 3, k + 1)\n",
    "        for i in range(len(X_test_numpy)):\n",
    "            if i > num_examples_show:\n",
    "                break\n",
    "            x_ele = X_test_numpy[i]\n",
    "            y_ele = y_test_numpy[i]\n",
    "            ax.plot(np.concatenate((x_ele[:,0], y_ele[:,0])), np.concatenate((x_ele[:,1], y_ele[:,1])), \".-\", color = COLOR_LIST[i % len(COLOR_LIST)])\n",
    "            ax.plot([y_ele[0,0]], [y_ele[0,1]], \"o\", color = \"r\")\n",
    "            ax.set_title(task_key)\n",
    "            if master_model is not None:\n",
    "                y_pred_ele = y_pred_numpy[i]\n",
    "                ax.plot(np.concatenate((x_ele[:,0], y_pred_ele[:,0])), np.concatenate((x_ele[:,1], y_pred_ele[:,1])), \".--\", color = COLOR_LIST[i % len(COLOR_LIST)])\n",
    "                ax.plot([y_pred_ele[0,0]], [y_pred_ele[0, 1]], \"o\", color = \"b\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_few_shot_loss(master_model, tasks, isplot = True):\n",
    "    import matplotlib.pylab as plt\n",
    "    plt.figure(figsize = (8,6))\n",
    "    num_shots_list = [20, 30, 40, 50, 70, 100, 200, 300, 500, 1000]\n",
    "    mse_list_whole = []\n",
    "    for task_key, task in tasks.items():\n",
    "        mse_list = []\n",
    "        ((X_train, y_train), (X_test, y_test)), _ = tasks[task_key]\n",
    "        is_cuda = X_train.is_cuda\n",
    "        for num_shots in num_shots_list:\n",
    "            idx = torch.LongTensor(np.random.choice(range(len(X_train)), num_shots, replace = False))\n",
    "            if is_cuda:\n",
    "                idx = idx.cuda()\n",
    "            X_few_shot = X_train[idx]\n",
    "            y_few_shot = y_train[idx]\n",
    "            statistics = master_model.statistics_Net.forward_inputs(X_few_shot, y_few_shot)\n",
    "            y_test_pred = master_model.generative_Net(X_test, statistics)\n",
    "            mse_list.append(nn.MSELoss()(y_test_pred, y_test).data[0])\n",
    "        mse_list_whole.append(mse_list)\n",
    "    mse_list_whole = np.array(mse_list_whole)\n",
    "    mse_mean = mse_list_whole.mean(0)\n",
    "    mse_std = mse_list_whole.std(0)\n",
    "    if isplot:\n",
    "        plt.errorbar(num_shots_list, mse_mean, mse_std, fmt = \"o\")\n",
    "        ax = plt.gca()\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"number of shots\")\n",
    "        ax.set_ylabel(\"mse\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.errorbar(num_shots_list, mse_mean, mse_std, fmt = \"o\")\n",
    "        ax = plt.gca()\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"number of shots\")\n",
    "        ax.set_ylabel(\"mse\")\n",
    "        plt.show()\n",
    "    return mse_list_whole\n",
    "\n",
    "\n",
    "def get_corrcoef(x, y):\n",
    "    import scipy\n",
    "    corrcoef = np.zeros((y.shape[1], x.shape[1]))\n",
    "    for i in range(corrcoef.shape[0]):\n",
    "        for j in range(corrcoef.shape[1]):\n",
    "            corrcoef[i, j] = scipy.stats.pearsonr(y[:,i], x[:, j])[0]\n",
    "    return corrcoef\n",
    "\n",
    "\n",
    "def plot_statistics_vs_z(z_list, statistics_list, mode = \"corrcoef\", title = None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    num_columns = 5\n",
    "    if isinstance(z_list, list):\n",
    "        z_list = np.stack(z_list, 0)\n",
    "    if isinstance(statistics_list, list):\n",
    "        statistics_list = np.stack(statistics_list, 0)\n",
    "    if len(z_list.shape) == 1:\n",
    "        z_list = np.expand_dims(z_list, 1)\n",
    "    z_size = z_list.shape[1]\n",
    "    num_rows = int(np.ceil(z_size / num_columns))\n",
    "    fig = plt.figure(figsize = (25, num_rows * 3.2))\n",
    "    \n",
    "    for i in range(z_size):\n",
    "        ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "        for j in range(statistics_list.shape[1]):\n",
    "            ax.plot(z_list[:,i], statistics_list[:,j], color = COLOR_LIST[j], marker = \".\", linestyle = 'None', alpha = 0.6, markersize = 2)\n",
    "            ax.set_title(\"statistics vs. z_{0}\".format(i))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot coefficient for linear regression:\n",
    "    if mode == \"corrcoef\":\n",
    "        print(\"statistics (row) vs. z (column) pearsonr correlation coefficient (abs value):\")\n",
    "        corrcoef = get_corrcoef(z_list, statistics_list)\n",
    "        plot_matrices([np.abs(corrcoef)], title = title)\n",
    "        print(\"statistics correlation matrix:\")\n",
    "        plot_matrices([np.abs(np.corrcoef(statistics_list, rowvar = False))])\n",
    "        print(\"pca explained variance ratio:\")\n",
    "        pca = PCA()\n",
    "        pca.fit(statistics_list)\n",
    "        print(pca.explained_variance_ratio_)\n",
    "    else:\n",
    "        print(\"statistics (row) vs. z (column) linear regression abs(coeff):\")\n",
    "        from sklearn import linear_model\n",
    "        reg = linear_model.LinearRegression()\n",
    "        coeff_list = []\n",
    "        for i in range(statistics_list.shape[1]):\n",
    "            reg.fit(z_list, statistics_list[:,i])\n",
    "            coeff_list.append(reg.coef_)\n",
    "        coeff_list = np.array(coeff_list)\n",
    "        plot_matrices([np.abs(coeff_list)])\n",
    "\n",
    "\n",
    "def plot_data_record(data_record, idx = None, is_VAE = False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    source = [\"loss\", \"loss_sampled\", \"mse\"] if is_VAE else [\"loss\", \"mse\"]\n",
    "    fig = plt.figure(figsize = (len(source) * 8, 6))\n",
    "    for i, key in enumerate(source):\n",
    "        if \"{0}_mean_train\".format(key) in data_record:\n",
    "            ax = fig.add_subplot(1, len(source), i + 1)\n",
    "            if idx is None:\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_mean_train\".format(key)], label = '{0}_mean_train'.format(key), c = \"b\")\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_mean_test\".format(key)], label = '{0}_mean_test'.format(key), c = \"r\")\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_median_train\".format(key)], label = '{0}_median_train'.format(key), c = \"b\", linestyle = \"--\")\n",
    "                ax.semilogy(data_record[\"iter\"], data_record[\"{0}_median_test\".format(key)], label = '{0}_median_test'.format(key), c = \"r\", linestyle = \"--\")\n",
    "\n",
    "                ax.legend()\n",
    "                ax.set_xlabel(\"training step\")\n",
    "                ax.set_ylabel(key)\n",
    "                ax.set_title(\"{0} vs. training step\".format(key))\n",
    "            else:\n",
    "                loss_train_list = [data_record[key][task_key][idx] for task_key in data_record[\"tasks_train\"][0].keys()]\n",
    "                loss_test_list = [data_record[key][task_key][-1] for task_key in data_record[\"tasks_test\"][0].keys()]\n",
    "                ax.hist(loss_train_list, bins = 20, density = True, alpha = 0.3, color=\"b\")\n",
    "                ax.hist(loss_test_list, bins = 20, density = True, alpha = 0.3, color=\"r\")\n",
    "                ax.axvline(x= np.mean(loss_train_list), c = \"b\", alpha = 0.6, label = \"train_mean\")\n",
    "                ax.axvline(x= np.median(loss_train_list), c = \"b\", linestyle = \"--\", alpha = 0.6, label = \"train_median\")\n",
    "                ax.axvline(x= np.mean(loss_test_list), c = \"r\", alpha = 0.6, label = \"test_mean\")\n",
    "                ax.axvline(x= np.median(loss_test_list), c = \"r\", linestyle = \"--\", alpha = 0.6, label = \"test_median\")\n",
    "                ax.legend()\n",
    "                ax.set_title(\"Histogram for {0}:\".format(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, z, zdim = 1, num_layers = 1, activation = \"tanh\"):\n",
    "    \"\"\"Generating latent-model data:\"\"\"\n",
    "    A0 = lambda z: np.tanh(z)\n",
    "    A1 = lambda z: z ** 2 / (1 + z ** 2)\n",
    "    A2 = lambda z: np.sin(z)\n",
    "    A3 = lambda z: z\n",
    "    A4 = lambda z: z ** 2 - z\n",
    "    input_size = x.shape[1]\n",
    "    if zdim == 1:\n",
    "        output = x[:,0:1] * A0(z) + x[:,1:2] * A1(z) + x[:,2:3] * A2(z) + x[:,3:4] * A3(z) + A4(z)\n",
    "        output = get_activation(activation)(output)\n",
    "        if num_layers >= 2:\n",
    "            pass\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_latent_model_data(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn() * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand() * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    num_layers = settings[\"num_layers\"] if \"num_layers\" in settings else 1\n",
    "    activation = settings[\"activation\"] if \"activation\" in settings else \"tanh\"\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-5,5)\n",
    "    input_size = settings[\"input_size\"] if \"input_size\" in settings else 5\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    \n",
    "    X = Variable(torch.rand(num_examples, input_size) * (xlim[1] - xlim[0]) + xlim[0], requires_grad = False)\n",
    "    y = f(X, z, zdim = settings[\"zdim\"], num_layers = num_layers, activation = activation)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.data.numpy(), y.data.numpy(), test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "\n",
    "def get_polynomial_class(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    order = 3,\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn(order + 1) * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand(order + 1) * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-3,3)\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    X = np.random.rand(num_examples, 1) * (xlim[1] - xlim[0]) + xlim[0]\n",
    "    y = z[0]\n",
    "    for i in range(1, order + 1):\n",
    "        y = y + X ** i * z[i]\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "def get_Legendre_class(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    order = 3,\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn(order + 1) * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand(order + 1) * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-1,1)\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    \n",
    "    X = np.random.rand(num_examples, 1) * (xlim[1] - xlim[0]) + xlim[0]\n",
    "    y = np.polynomial.legendre.legval(X, z)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "def get_master_function(\n",
    "    z_settings = [\"Gaussian\", (0, 1)],\n",
    "    mode = \"sawtooth\",\n",
    "    settings = {},\n",
    "    num_examples = 1000,\n",
    "    isTorch = True,\n",
    "    is_cuda = False,\n",
    "    ):\n",
    "    def trianglewave(\n",
    "        x,\n",
    "        frequency = 0.25,\n",
    "        height = 1,\n",
    "        ):\n",
    "        remainder = x % (1 / float(frequency))\n",
    "        slope = height * frequency * 2\n",
    "        return 2 * np.minimum(slope * remainder, 2 * height - slope * remainder) - 1\n",
    "    def S(x):\n",
    "        return np.sin(x * np.pi / 2)\n",
    "    def Gaussian(x):\n",
    "        return 1 / np.sqrt(2 * np.pi) * np.exp(- x ** 2 / 2)\n",
    "    def Softplus(x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    if z_settings[0] == \"Gaussian\":\n",
    "        mu, std = z_settings[1]\n",
    "        z = np.random.randn(4) * std + mu\n",
    "    elif z_settings[0] == \"uniform\":\n",
    "        zlim = z_settings[1]\n",
    "        z = np.random.rand(4) * (zlim[1] - zlim[0]) + zlim[0]\n",
    "    else:\n",
    "        raise Exception(\"z_settings[0] of {0} not recognized!\".format(z_settings[0]))\n",
    "    xlim = settings[\"xlim\"] if \"xlim\" in settings else (-3,3)\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    z[0] = np.abs(z[0]) + 0.5\n",
    "    z[2] = np.abs(z[2] + 1)\n",
    "    frequency = z[0]\n",
    "    x0 = z[1]\n",
    "    amp = z[2]\n",
    "    const = z[3]\n",
    "    \n",
    "    X = np.random.rand(num_examples, 1) * (xlim[1] - xlim[0]) + xlim[0]\n",
    "    if mode == \"sawtooth\":\n",
    "        f = trianglewave\n",
    "    elif mode == \"sin\":\n",
    "        f = S\n",
    "    elif mode == \"tanh\":\n",
    "        f = np.tanh\n",
    "    elif mode == \"Gaussian\":\n",
    "        f = Gaussian\n",
    "    elif mode == \"softplus\":\n",
    "        f = Softplus\n",
    "    else:\n",
    "        raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "    \n",
    "    y = f((X - x0) * frequency) * amp + const\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    if isTorch:\n",
    "        X_train = Variable(torch.FloatTensor(X_train), requires_grad = False)\n",
    "        y_train = Variable(torch.FloatTensor(y_train), requires_grad = False)\n",
    "        X_test = Variable(torch.FloatTensor(X_test), requires_grad = False)\n",
    "        y_test = Variable(torch.FloatTensor(y_test), requires_grad = False)\n",
    "        if is_cuda:\n",
    "            X_train = X_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "            X_test = X_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": z}\n",
    "\n",
    "\n",
    "def get_bouncing_states(settings, num_examples, data_format = \"states\", is_cuda = False, **kwargs):\n",
    "    from AI_scientist.variational.util_variational import get_env_data\n",
    "    from AI_scientist.settings.a2c_env_settings import ENV_SETTINGS_CHOICE\n",
    "    render = kwargs[\"render\"] if \"render\" in kwargs else False\n",
    "    test_size = settings[\"test_size\"] if \"test_size\" in settings else 0.2\n",
    "    env_name = \"envBounceStates\"\n",
    "    screen_size = ENV_SETTINGS_CHOICE[env_name][\"screen_height\"]\n",
    "    ball_radius = ENV_SETTINGS_CHOICE[env_name][\"ball_radius\"]\n",
    "    \n",
    "    vertex_bottom_left = tuple(np.random.rand(2) * screen_size / 4 + ball_radius)\n",
    "    vertex_bottom_right = (screen_size - np.random.rand() * screen_size / 4 - ball_radius, np.random.rand() * screen_size / 4 + ball_radius)\n",
    "    vertex_top_right = tuple(screen_size - np.random.rand(2) * screen_size / 4 - ball_radius)\n",
    "    vertex_top_left = (np.random.rand() * screen_size / 4 + ball_radius, screen_size - np.random.rand() * screen_size / 4 - ball_radius)\n",
    "    boundaries = [vertex_bottom_left, vertex_bottom_right, vertex_top_right, vertex_top_left]\n",
    "    \n",
    "    ((X_train, y_train), (X_test, y_test), (reflected_train, reflected_test)), info = \\\n",
    "        get_env_data(\n",
    "            env_name,\n",
    "            data_format = data_format,\n",
    "            num_examples = num_examples,\n",
    "            test_size = test_size,\n",
    "            isplot = False,\n",
    "            is_cuda = False,\n",
    "            output_dims = (0,1),\n",
    "            episode_length = 200,\n",
    "            boundaries = boundaries,\n",
    "            render = render,\n",
    "            is_flatten = True,\n",
    "            max_range = (0, screen_size),\n",
    "            verbose = True,\n",
    "        )\n",
    "    if is_cuda:\n",
    "        X_train = X_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        X_test = X_test.cuda()\n",
    "        y_test = y_test.cuda()\n",
    "    return ((X_train, y_train), (X_test, y_test)), {\"z\": np.array(boundaries).reshape(-1)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
