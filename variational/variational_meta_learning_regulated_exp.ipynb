{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116013050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from AI_scientist.util import plot_matrices, make_dir, get_struct_str, get_args, Early_Stopping, record_data, manifold_embedding\n",
    "from AI_scientist.settings.filepath import variational_model_PATH\n",
    "from AI_scientist.pytorch.net import Net\n",
    "from AI_scientist.pytorch.util_pytorch import Loss_with_uncertainty\n",
    "from AI_scientist.variational.variational_meta_learning import Master_Model, Statistics_Net, Generative_Net, load_model_dict, get_regulated_statistics\n",
    "from AI_scientist.variational.variational_meta_learning import VAE_Loss, sample_Gaussian, clone_net, get_nets, get_tasks, evaluate, get_reg, load_trained_models\n",
    "from AI_scientist.variational.variational_meta_learning import plot_task_ensembles, plot_individual_tasks, plot_statistics_vs_z, plot_data_record, get_corrcoef\n",
    "from AI_scientist.variational.variational_meta_learning import get_latent_model_data, get_polynomial_class, get_Legendre_class, get_master_function\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id_list = [\n",
    "# \"latent_model_linear\",\n",
    "# \"polynomial_3\",\n",
    "# \"Legendre_3\",\n",
    "# \"master_sawtooth\",\n",
    "# \"master_sin\",\n",
    "# \"master_Gaussian\",\n",
    "# \"master_tanh\",\n",
    "# \"master_softplus\",\n",
    "\"2Dbouncing\",\n",
    "]\n",
    "exp_id = \"test2\"\n",
    "input_size = 6\n",
    "statistics_output_neurons = 6\n",
    "is_VAE = False\n",
    "is_uncertainty_net = False\n",
    "is_regulated_net = False\n",
    "VAE_beta = 0.2\n",
    "\n",
    "output_size = 1\n",
    "lr = 5e-5\n",
    "num_train_tasks = 10\n",
    "num_test_tasks = 5\n",
    "batch_size_task = min(100, num_train_tasks)\n",
    "num_backwards = 1\n",
    "num_iter = 20000\n",
    "pre_pooling_neurons = 100\n",
    "num_context_neurons = 0\n",
    "statistics_pooling = \"max\"\n",
    "patience = 400\n",
    "reg_amp = 1e-6\n",
    "activation_gen = \"leakyRelu\"\n",
    "activation_model = \"leakyRelu\"\n",
    "optim_mode = \"individual\"\n",
    "loss_core = \"huber\"\n",
    "array_id = \"0\"\n",
    "\n",
    "exp_id = get_args(exp_id, 1)\n",
    "task_id_list = get_args(task_id_list, 2, type = \"tuple\")\n",
    "statistics_output_neurons = get_args(statistics_output_neurons, 3, type = \"int\")\n",
    "is_VAE = get_args(is_VAE, 4, type = \"bool\")\n",
    "VAE_beta = get_args(VAE_beta, 5, type = \"float\")\n",
    "lr = get_args(lr, 6, type = \"float\")\n",
    "batch_size_task = get_args(batch_size_task, 7, type = \"int\")\n",
    "pre_pooling_neurons = get_args(pre_pooling_neurons, 8, type = \"int\")\n",
    "num_context_neurons = get_args(num_context_neurons, 9, type = \"int\")\n",
    "statistics_pooling = get_args(statistics_pooling, 10)\n",
    "reg_amp = get_args(reg_amp, 11, type = \"float\")\n",
    "activation_gen = get_args(activation_gen, 12)\n",
    "activation_model = get_args(activation_model, 13)\n",
    "optim_mode = get_args(optim_mode, 14)\n",
    "is_uncertainty_net = get_args(is_uncertainty_net, 15, \"bool\")\n",
    "loss_core = get_args(loss_core, 16)\n",
    "array_id = get_args(array_id, 17)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    isplot = True\n",
    "except:\n",
    "    isplot = False\n",
    "\n",
    "reg_dict = {\"statistics_Net\": {\"weight\": reg_amp, \"bias\": reg_amp},\n",
    "            \"generative_Net\": {\"weight\": reg_amp, \"bias\": reg_amp, \"W_gen\": reg_amp, \"b_gen\": reg_amp}}\n",
    "task_settings = {\n",
    "    \"zdim\": 1,\n",
    "    \"z_settings\": [\"Gaussian\", (0, 1)],\n",
    "    \"num_layers\": 1,\n",
    "    \"xlim\": (-4, 4),\n",
    "    \"activation\": \"softplus\",\n",
    "    \"input_size\": input_size,\n",
    "    \"num_examples\": 2000,\n",
    "}\n",
    "struct_param_pre = [\n",
    "        [60, \"Simple_Layer\", {}],\n",
    "        [60, \"Simple_Layer\", {}],\n",
    "        [60, \"Simple_Layer\", {}],\n",
    "        [pre_pooling_neurons, \"Simple_Layer\", {\"activation\": \"linear\"}],\n",
    "    ]\n",
    "struct_param_post = None\n",
    "struct_param_gen_base = [\n",
    "        [60, \"Simple_Layer\", {}],\n",
    "        [60, \"Simple_Layer\", {}],\n",
    "        [60, \"Simple_Layer\", {}],\n",
    "]\n",
    "isParallel = False\n",
    "inspect_interval = 50\n",
    "save_interval = 500\n",
    "filename = variational_model_PATH + \"/trained_models/{0}/Net_{1}_input_{2}_({3},{4})_statistics-neurons_{5}_pre_{6}_pooling_{7}_context_{8}_batch_{9}_backward_{10}_VAE_{11}_{12}_uncertainty_{13}_lr_{14}_reg_{15}_actgen_{16}_actmodel_{17}_struct_{18}_{19}_core_{20}_{21}_\".format(exp_id, task_id_list, input_size, num_train_tasks, num_test_tasks, statistics_output_neurons, pre_pooling_neurons, statistics_pooling, num_context_neurons, batch_size_task, num_backwards, is_VAE, VAE_beta, is_uncertainty_net, lr, reg_amp, activation_gen, activation_model, get_struct_str(struct_param_gen_base), optim_mode, loss_core, exp_id)\n",
    "make_dir(filename)\n",
    "print(filename)\n",
    "\n",
    "\n",
    "statistics_Net, generative_Net, generative_Net_logstd = get_nets(input_size = input_size, output_size = output_size, \n",
    "                                          pre_pooling_neurons = pre_pooling_neurons, statistics_output_neurons = statistics_output_neurons, num_context_neurons = num_context_neurons,\n",
    "                                          struct_param_pre = struct_param_pre,\n",
    "                                          struct_param_gen_base = struct_param_gen_base,\n",
    "                                          activation_statistics = activation_gen,\n",
    "                                          activation_generative = activation_gen,\n",
    "                                          activation_model = activation_model,\n",
    "                                          statistics_pooling = statistics_pooling,\n",
    "                                          isParallel = isParallel,\n",
    "                                          is_VAE = is_VAE,\n",
    "                                          is_uncertainty_net = is_uncertainty_net,\n",
    "                                         )\n",
    "if is_regulated_net:\n",
    "    struct_param_regulated_Net = [\n",
    "            [40, \"Simple_Layer\", {}],\n",
    "            [40, \"Simple_Layer\", {}],\n",
    "            [1, \"Simple_Layer\", {\"activation\": \"linear\"}],\n",
    "    ]\n",
    "    generative_Net = Net(input_size = input_size, struct_param = struct_param_regulated_Net, settings = {\"activation\": activation_model})\n",
    "master_model = Master_Model(statistics_Net, generative_Net, generative_Net_logstd)\n",
    "if is_uncertainty_net:\n",
    "    optimizer = optim.Adam(chain.from_iterable([statistics_Net.parameters(), generative_Net.parameters(), generative_Net_logstd.parameters()]), lr = lr)\n",
    "else:\n",
    "    optimizer = optim.Adam(chain.from_iterable([statistics_Net.parameters(), generative_Net.parameters()]), lr = lr)\n",
    "\n",
    "if loss_core == \"mse\":\n",
    "    loss_fun_core = nn.MSELoss(size_average = True)\n",
    "elif loss_core == \"huber\":\n",
    "    loss_fun_core = nn.SmoothL1Loss(size_average = True) \n",
    "else:\n",
    "    raise\n",
    "if is_VAE:\n",
    "    criterion = VAE_Loss(criterion = loss_fun_core, prior = \"Gaussian\", beta = VAE_beta)\n",
    "else:\n",
    "    if is_uncertainty_net:\n",
    "        criterion = Loss_with_uncertainty(core = loss_core)\n",
    "    else:\n",
    "        criterion = loss_fun_core\n",
    "early_stopping = Early_Stopping(patience = patience)\n",
    "tasks_train, tasks_test = get_tasks(task_id_list, num_train_tasks, num_test_tasks, task_settings = task_settings)\n",
    "all_keys = list(tasks_train.keys()) + list(tasks_test.keys())\n",
    "\n",
    "data_record = {\"loss\": {key: [] for key in all_keys}, \"loss_sampled\": {key: [] for key in all_keys}, \"mse\": {key: [] for key in all_keys},\n",
    "               \"reg\": {key: [] for key in all_keys}, \"KLD\": {key: [] for key in all_keys}}\n",
    "info_dict = {\"array_id\": array_id}\n",
    "info_dict[\"data_record\"] = data_record\n",
    "info_dict[\"model_dict\"] = []\n",
    "record_data(data_record, [exp_id, tasks_train, tasks_test, task_id_list, task_settings, reg_dict, is_uncertainty_net, lr, pre_pooling_neurons, num_backwards, batch_size_task, \n",
    "                          struct_param_gen_base, struct_param_pre, struct_param_post, statistics_pooling, activation_gen, activation_model], \n",
    "            [\"exp_id\", \"tasks_train\", \"tasks_test\", \"task_id_list\", \"task_settings\", \"reg_dict\", \"is_uncertainty_net\", \"lr\", \"pre_pooling_neurons\", \"num_backwards\", \"batch_size_task\",\n",
    "             \"struct_param_gen_base\", \"struct_param_pre\", \"struct_param_post\", \"statistics_pooling\", \"activation_gen\", \"activation_model\"])\n",
    "for i in range(num_iter + 1):\n",
    "    chosen_task_keys = np.random.choice(list(tasks_train.keys()), batch_size_task, replace = False).tolist()\n",
    "    if optim_mode == \"individual\":\n",
    "        if is_VAE:\n",
    "            KLD_total = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        for task_key, task in tasks_train.items():\n",
    "            if task_key not in chosen_task_keys:\n",
    "                continue\n",
    "            ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "            for k in range(num_backwards):\n",
    "                optimizer.zero_grad()\n",
    "                if is_VAE:\n",
    "                    statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                    statistics = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "                    if is_regulated_net:\n",
    "                        statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "                    y_pred = generative_Net(X_train, statistics)\n",
    "                    loss, KLD = criterion(y_pred, y_train, mu = statistics_mu, logvar = statistics_logvar)\n",
    "                    KLD_total = KLD_total + KLD\n",
    "                else:\n",
    "                    if is_uncertainty_net:\n",
    "                        statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                        y_pred = generative_Net(X_train, statistics_mu)\n",
    "                        y_pred_logstd = generative_Net_logstd(X_train, statistics_logvar)\n",
    "                        loss = criterion(y_pred, y_train, log_std = y_pred_logstd)\n",
    "                    else:\n",
    "                        statistics = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                        if is_regulated_net:\n",
    "                            statistics = get_regulated_statistics(generative_Net, statistics)\n",
    "                        y_pred = generative_Net(X_train, statistics)\n",
    "                        loss = criterion(y_pred, y_train)\n",
    "                reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net)\n",
    "                loss = loss + reg\n",
    "                loss.backward(retain_graph = True)\n",
    "                optimizer.step()\n",
    "        # Perform gradient on the KL-divergence:\n",
    "        if is_VAE:\n",
    "            KLD_total = KLD_total / batch_size_task\n",
    "            optimizer.zero_grad()\n",
    "            KLD_total.backward()\n",
    "            optimizer.step()\n",
    "            record_data(data_record, [KLD_total], [\"KLD_total\"])\n",
    "    elif optim_mode == \"sum\":\n",
    "        optimizer.zero_grad()\n",
    "        loss_total = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        for task_key, task in tasks_train.items():\n",
    "            if task_key not in chosen_task_keys:\n",
    "                continue\n",
    "            ((X_train, y_train), (X_test, y_test)), _ = task\n",
    "            if is_VAE:\n",
    "                statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                statistics = sample_Gaussian(statistics_mu, statistics_logvar)\n",
    "                y_pred = generative_Net(X_train, statistics)\n",
    "                loss, KLD = criterion(y_pred, y_train, mu = statistics_mu, logvar = statistics_logvar)\n",
    "                loss = loss + KLD\n",
    "            else:\n",
    "                if is_uncertainty_net:\n",
    "                    statistics_mu, statistics_logvar = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                    y_pred = generative_Net(X_train, statistics_mu)\n",
    "                    y_pred_logstd = generative_Net_logstd(X_train, statistics_logvar)\n",
    "                    loss = criterion(y_pred, y_train, log_std = y_pred_logstd)\n",
    "                else:\n",
    "                    statistics = statistics_Net(torch.cat([X_train, y_train], 1))\n",
    "                    y_pred = generative_Net(X_train, statistics)\n",
    "                    loss = criterion(y_pred, y_train)\n",
    "            reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net)\n",
    "            loss_total = loss_total + loss + reg\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        raise Exception(\"optim_mode {0} not recognized!\".format(optim_mode))\n",
    "    \n",
    "\n",
    "    loss_test_record = []\n",
    "    for task_key, task in tasks_test.items():\n",
    "        loss_test, _, _, _ = evaluate(task, statistics_Net, generative_Net, generative_Net_logstd = generative_Net_logstd, criterion = criterion, is_VAE = is_VAE, is_regulated_net = is_regulated_net)\n",
    "        loss_test_record.append(loss_test)\n",
    "    to_stop = early_stopping.monitor(np.mean(loss_test_record))\n",
    "\n",
    "    # Validation and visualization:\n",
    "    if i % inspect_interval == 0 or to_stop:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"training tasks:\")\n",
    "        for task_key, task in tasks_train.items():\n",
    "            (_, (X_test, y_test)), _ = task\n",
    "            loss_test, loss_test_sampled, mse, KLD_test = evaluate(task, statistics_Net, generative_Net, generative_Net_logstd = generative_Net_logstd, criterion = criterion, is_VAE = is_VAE, is_regulated_net = is_regulated_net)\n",
    "            reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net).data.numpy()[0]\n",
    "            data_record[\"loss\"][task_key].append(loss_test)\n",
    "            data_record[\"loss_sampled\"][task_key].append(loss_test_sampled)\n",
    "            data_record[\"mse\"][task_key].append(mse)\n",
    "            data_record[\"reg\"][task_key].append(reg)\n",
    "            data_record[\"KLD\"][task_key].append(KLD_test)\n",
    "            print('{0}\\ttrain\\t{1}  \\tloss: {2:.5f}\\tloss_sampled:{3:.5f} \\tmse:{4:.5f}\\tKLD:{5:.6f}\\treg:{6:.6f}'.format(i, task_key, loss_test, loss_test_sampled, mse, KLD_test, reg))\n",
    "        for task_key, task in tasks_test.items():\n",
    "            (_, (X_test, y_test)), _ = task\n",
    "            loss_test, loss_test_sampled, mse, KLD_test = evaluate(task, statistics_Net, generative_Net, generative_Net_logstd = generative_Net_logstd, criterion = criterion, is_VAE = is_VAE, is_regulated_net = is_regulated_net)\n",
    "            reg = get_reg(reg_dict, statistics_Net = statistics_Net, generative_Net = generative_Net).data.numpy()[0]\n",
    "            data_record[\"loss\"][task_key].append(loss_test)\n",
    "            data_record[\"loss_sampled\"][task_key].append(loss_test_sampled)\n",
    "            data_record[\"mse\"][task_key].append(mse)\n",
    "            data_record[\"reg\"][task_key].append(reg)\n",
    "            data_record[\"KLD\"][task_key].append(KLD_test)\n",
    "            print('{0}\\ttrain\\t{1}  \\tloss: {2:.5f}\\tloss_sampled:{3:.5f} \\tmse:{4:.5f}\\tKLD:{5:.6f}\\treg:{6:.6f}'.format(i, task_key, loss_test, loss_test_sampled, mse, KLD_test, reg))\n",
    "        loss_train_list = [data_record[\"loss\"][task_key][-1] for task_key in tasks_train]\n",
    "        loss_test_list = [data_record[\"loss\"][task_key][-1] for task_key in tasks_test]\n",
    "        loss_train_sampled_list = [data_record[\"loss_sampled\"][task_key][-1] for task_key in tasks_train]\n",
    "        loss_test_sampled_list = [data_record[\"loss_sampled\"][task_key][-1] for task_key in tasks_test]\n",
    "        mse_train_list = [data_record[\"mse\"][task_key][-1] for task_key in tasks_train]\n",
    "        mse_test_list = [data_record[\"mse\"][task_key][-1] for task_key in tasks_test]\n",
    "        reg_train_list = [data_record[\"reg\"][task_key][-1] for task_key in tasks_train]\n",
    "        reg_test_list = [data_record[\"reg\"][task_key][-1] for task_key in tasks_test]\n",
    "        record_data(data_record, \n",
    "                    [np.mean(loss_train_list), np.median(loss_train_list), np.mean(reg_train_list), i,\n",
    "                     np.mean(loss_test_list), np.median(loss_test_list), np.mean(reg_test_list),\n",
    "                     np.mean(loss_train_sampled_list), np.median(loss_train_sampled_list), \n",
    "                     np.mean(loss_test_sampled_list), np.median(loss_test_sampled_list),\n",
    "                     np.mean(mse_train_list), np.median(mse_train_list), \n",
    "                     np.mean(mse_test_list), np.median(mse_test_list), \n",
    "                    ], \n",
    "                    [\"loss_mean_train\", \"loss_median_train\", \"reg_mean_train\", \"iter\",\n",
    "                     \"loss_mean_test\", \"loss_median_test\", \"reg_mean_test\",\n",
    "                     \"loss_sampled_mean_train\", \"loss_sampled_median_train\",\n",
    "                     \"loss_sampled_mean_test\", \"loss_sampled_median_test\", \n",
    "                     \"mse_mean_train\", \"mse_median_train\", \"mse_mean_test\", \"mse_median_test\", \n",
    "                    ])\n",
    "        if isplot:\n",
    "            plot_data_record(data_record, idx = -1, is_VAE = is_VAE)\n",
    "        print(\"Summary:\")\n",
    "        print('\\n{0}\\ttrain\\tloss_mean: {1:.5f}\\tloss_median: {2:.5f}\\tmse_mean: {3:.6f}\\tmse_median: {4:.6f}\\treg: {5:.6f}'.format(i, data_record[\"loss_mean_train\"][-1], data_record[\"loss_median_train\"][-1], data_record[\"mse_mean_train\"][-1], data_record[\"mse_median_train\"][-1], data_record[\"reg_mean_train\"][-1]))\n",
    "        print('{0}\\ttest\\tloss_mean: {1:.5f}\\tloss_median: {2:.5f}\\tmse_mean: {3:.6f}\\tmse_median: {4:.6f}\\treg: {5:.6f}'.format(i, data_record[\"loss_mean_test\"][-1], data_record[\"loss_median_test\"][-1], data_record[\"mse_mean_test\"][-1], data_record[\"mse_median_test\"][-1], data_record[\"reg_mean_test\"][-1]))\n",
    "        if is_VAE and \"KLD_total\" in locals():\n",
    "            print(\"KLD_total: {0:.5f}\".format(KLD_total.data.numpy()[0]))\n",
    "        if isplot:\n",
    "            plot_data_record(data_record, is_VAE = is_VAE)\n",
    "\n",
    "        # Plotting y_pred vs. y_target:\n",
    "        statistics_list_train, z_list_train = plot_task_ensembles(tasks_train, statistics_Net, generative_Net, is_VAE = is_VAE, is_regulated_net = is_regulated_net, title = \"y_pred_train vs. y_train\", isplot = isplot)\n",
    "        statistics_list_test, z_list_test = plot_task_ensembles(tasks_test, statistics_Net, generative_Net, is_VAE = is_VAE, is_regulated_net = is_regulated_net, title = \"y_pred_test vs. y_test\", isplot = isplot)\n",
    "        record_data(data_record, [np.array(z_list_train), np.array(z_list_test), np.array(statistics_list_train), np.array(statistics_list_test)], \n",
    "                    [\"z_list_train_list\", \"z_list_test_list\", \"statistics_list_train_list\", \"statistics_list_test_list\"])\n",
    "        if isplot:\n",
    "            print(\"train statistics vs. z:\")\n",
    "            plot_statistics_vs_z(z_list_train, statistics_list_train)\n",
    "            print(\"test statistics vs. z:\")\n",
    "            plot_statistics_vs_z(z_list_test, statistics_list_test)\n",
    "\n",
    "#             # Plotting individual test data:\n",
    "#             print(\"train tasks:\")\n",
    "#             plot_individual_tasks(tasks_train, statistics_Net, generative_Net, generative_Net_logstd = generative_Net_logstd, is_VAE = is_VAE, is_regulated_net = is_regulated_net, xlim = task_settings[\"xlim\"])\n",
    "#             print(\"test tasks:\")\n",
    "#             plot_individual_tasks(tasks_test, statistics_Net, generative_Net, generative_Net_logstd = generative_Net_logstd, is_VAE = is_VAE, is_regulated_net = is_regulated_net, xlim = task_settings[\"xlim\"])\n",
    "        print(\"=\" * 50 + \"\\n\\n\")\n",
    "        try:\n",
    "            sys.stdout.flush()\n",
    "        except:\n",
    "            pass\n",
    "    if i % save_interval == 0 or to_stop:\n",
    "        record_data(info_dict, [master_model.model_dict, i], [\"model_dict\", \"iter\"])\n",
    "        pickle.dump(info_dict, open(filename + \"data.p\", \"wb\"))\n",
    "    if to_stop:\n",
    "        print(\"The training loss stops decreasing for {0} steps. Early stopping at {1}.\".format(patience, i))\n",
    "        break\n",
    "\n",
    "\n",
    "# Plotting:\n",
    "if isplot:\n",
    "    for task_key in tasks_train:\n",
    "        plt.semilogy(data_record[\"loss\"][task_key], alpha = 0.6)\n",
    "    plt.show()\n",
    "    for task_key in tasks_test:\n",
    "        plt.semilogy(data_record[\"loss\"][task_key], alpha = 0.6)\n",
    "    plt.show()\n",
    "print(\"completed\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = \"exp1.2\" # Standard\n",
    "dirname = \"../data/variational/trained_models/{0}/\".format(exp_id)\n",
    "filename = \"Net_['master_tanh']_input_1_(100,100)_statistics-neurons_4_pre_100_pooling_max_context_0_batch_100_backward_1_VAE_True_0.2_lr_0.0001_reg_1e-06_actgen_elu_actmodel_leakyRelu_struct_60Si-60Si-60Si_exp1.2_\"\n",
    "statistics_Net, generative_Net, data_record = load_trained_models(dirname + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id_list = [\n",
    "# \"latent_model_linear\",\n",
    "# \"polynomial_3\",\n",
    "# \"Legendre_3\",\n",
    "# \"master_sawtooth\",\n",
    "# \"master_sin\",\n",
    "# \"master_Gaussian\",\n",
    "\"master_tanh\",\n",
    "# \"master_softplus\",\n",
    "]\n",
    "is_VAE = True\n",
    "num_test_tasks = 1000\n",
    "task_settings = {\n",
    "    \"zdim\": 1,\n",
    "    \"z_settings\": [\"Gaussian\", (0, 1)],\n",
    "    \"num_layers\": 1,\n",
    "    \"xlim\": (-4, 4),\n",
    "    \"activation\": \"softplus\",\n",
    "    \"input_size\": 1,\n",
    "    \"num_examples\": 500,\n",
    "}\n",
    "plot_data_record(data_record, is_VAE = is_VAE)\n",
    "tasks_train, tasks_test = get_tasks(task_id_list, 10, num_test_tasks, task_settings = task_settings)\n",
    "statistics_list_test, z_list_test = plot_task_ensembles(tasks_test, statistics_Net, generative_Net, is_VAE = is_VAE, title = \"y_pred_test vs. y_test\")\n",
    "print(\"test statistics vs. z:\")\n",
    "plot_statistics_vs_z(z_list_test, statistics_list_test)\n",
    "_ = plot_individual_tasks(tasks_test, statistics_Net, generative_Net, is_VAE = is_VAE, xlim = task_settings[\"xlim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick learn analysis:\n",
    "(_, (X_test, y_test)), _ = tasks_test['master_tanh_1']\n",
    "\n",
    "num_steps_statistics = 50\n",
    "lr_statistics = 1e-2\n",
    "optimizer_statistics = \"LBFGS\"\n",
    "\n",
    "# num_steps = 50\n",
    "# lr = 1e-3\n",
    "# optimizer = \"adam\"\n",
    "\n",
    "num_steps = 50\n",
    "lr = 1e-2\n",
    "optimizer = \"LBFGS\"\n",
    "\n",
    "master_model.get_statistics(X_test, y_test)\n",
    "y_pred = master_model(X_test)\n",
    "loss_list_0 = master_model.statistics_quick_learn(X_test, y_test, num_steps = num_steps_statistics, lr = lr_statistics, optimizer = optimizer_statistics)\n",
    "y_pred_new = master_model(X_test)\n",
    "plt.plot(X_test.data.numpy(), y_test.data.numpy(), \".k\", label = \"target\", markersize = 3, alpha = 0.6)\n",
    "plt.plot(X_test.data.numpy(), y_pred.data.numpy(), \".b\", label = \"initial\", markersize = 3, alpha = 0.6)\n",
    "plt.plot(X_test.data.numpy(), y_pred_new.data.numpy(), \".r\", label = \"optimized\", markersize = 3, alpha = 0.6)\n",
    "plt.legend()\n",
    "plt.title(\"Only optimizing statistics\")\n",
    "plt.show()\n",
    "\n",
    "master_model = Master_Model(statistics_Net, generative_Net)\n",
    "master_model.get_statistics(X_test, y_test)\n",
    "master_model.use_clone_net(clone_parameters = True)\n",
    "y_pred = master_model(X_test)\n",
    "loss_list_1 = master_model.clone_net_quick_learn(X_test, y_test, num_steps = num_steps, lr = lr, optimizer = optimizer)\n",
    "y_pred_new = master_model(X_test)\n",
    "plt.plot(X_test.data.numpy(), y_test.data.numpy(), \".k\", label = \"target\", markersize = 3, alpha = 0.6)\n",
    "plt.plot(X_test.data.numpy(), y_pred.data.numpy(), \".b\", label = \"initial\", markersize = 3, alpha = 0.6)\n",
    "plt.plot(X_test.data.numpy(), y_pred_new.data.numpy(), \".r\", label = \"optimized\", markersize = 3, alpha = 0.6)\n",
    "plt.legend()\n",
    "plt.title(\"Optimizing cloned net\")\n",
    "plt.show()\n",
    "\n",
    "master_model = Master_Model(statistics_Net, generative_Net)\n",
    "master_model.get_statistics(X_test, y_test)\n",
    "master_model.use_clone_net(clone_parameters = False)\n",
    "W_core, _ = master_model.cloned_net.get_weights_bias(W_source = \"core\")\n",
    "y_pred = master_model(X_test)\n",
    "loss_list_2 = master_model.clone_net_quick_learn(X_test, y_test, num_steps = num_steps, lr = lr, optimizer = optimizer)\n",
    "y_pred_new = master_model(X_test)\n",
    "plt.plot(X_test.data.numpy(), y_test.data.numpy(), \".k\", label = \"target\", markersize = 3, alpha = 0.6)\n",
    "plt.plot(X_test.data.numpy(), y_pred.data.numpy(), \".b\", label = \"initial\", markersize = 3, alpha = 0.6)\n",
    "plt.plot(X_test.data.numpy(), y_pred_new.data.numpy(), \".r\", label = \"optimized\", markersize = 3, alpha = 0.6)\n",
    "plt.legend()\n",
    "plt.title(\"Optimizing from_scratch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(loss_list_0)), loss_list_0, label = \"loss_statistics\")\n",
    "plt.plot(range(len(loss_list_1)), loss_list_1, label = \"loss_clone\")\n",
    "plt.plot(range(len(loss_list_2)), loss_list_2, label = \"loss_scratch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.semilogy(range(len(loss_list_0)), loss_list_0, label = \"loss_statistics\")\n",
    "plt.semilogy(range(len(loss_list_1)), loss_list_1, label = \"loss_clone\")\n",
    "plt.semilogy(range(len(loss_list_2)), loss_list_2, label = \"loss_scratch\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression of statistics vs. z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "# reg = linear_model.Ridge(alpha = .5)\n",
    "# reg = linear_model.Lasso(alpha = 0.1)\n",
    "# reg = linear_model.BayesianRidge()\n",
    "# reg = linear_model.HuberRegressor()\n",
    "coeff_list = []\n",
    "for i in range(statistics_list_test.shape[1]):\n",
    "    reg.fit(z_list_test, statistics_list_test[:,i])\n",
    "    coeff_list.append(reg.coef_)\n",
    "coeff_list = np.array(coeff_list)\n",
    "plot_matrices([abs(coeff_list)])\n",
    "print(coeff_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting neural network to statistics vs. z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_param = [\n",
    "        [120, \"Simple_Layer\", {\"activation\": \"leakyRelu\"}],\n",
    "        [120, \"Simple_Layer\", {\"activation\": \"leakyRelu\"}],\n",
    "        [120, \"Simple_Layer\", {\"activation\": \"leakyRelu\"}],\n",
    "        [4, \"Simple_Layer\", {\"activation\": \"linear\"}],\n",
    "    ]\n",
    "settings = {\"activation\": \"linear\"}\n",
    "fit_net = Net(input_size = 4, struct_param = struct_param, settings = settings)\n",
    "model = Model(model = fit_net)\n",
    "lr = 5e-4\n",
    "model.compile(loss_type = \"huber\",\n",
    "              optimizer = \"adam\",\n",
    "              lr = {\"attention\": lr, \"modules\": lr},\n",
    "              reg_settings = {\"L1\": {\"scale\": 1e-6 * np.linspace(0, 1, 10000)}},\n",
    "             )\n",
    "model.fit(statistics_list_test, z_list_test, \n",
    "          validation_data = [statistics_list_test, z_list_test],\n",
    "          epochs = 10000,\n",
    "          batch_size = 100,\n",
    "          inspect_interval = 100,\n",
    "          record_mode = 2,\n",
    "          verbose = 1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_train2, tasks_test2 = get_tasks(task_id_list, 10, num_test_tasks, task_settings = task_settings)\n",
    "(_, (X_test, y_test)), z = tasks_test2['Legendre_3_1002']\n",
    "mu, logvar = statistics_Net(torch.cat([X_test, y_test],1))\n",
    "print(\"z:      \", z[\"z\"])\n",
    "print(\"z_pred: \", fit_net(mu).data.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of statistics_list_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.hist(z_list_test[:,i], bins = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.hist(statistics_list_test[:,i], bins = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(statistics_list_test)\n",
    "print(\"explained variance:\", pca.explained_variance_ratio_)\n",
    "print(\"singular values:\", pca.singular_values_)\n",
    "print(\"components:\\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifold_embedding(statistics_list_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
