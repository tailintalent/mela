{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from mela.util import plot_matrices\n",
    "from mela.pytorch.modules import get_Layer, load_layer_dict\n",
    "from mela.pytorch.util_pytorch import softmax, get_activation, get_criterion, get_optimizer, get_full_struct_param, to_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_dict_net(model_dict, is_cuda = False):\n",
    "    net_type = model_dict[\"type\"]\n",
    "    if net_type == \"Net\":\n",
    "        return Net(input_size = model_dict[\"input_size\"],\n",
    "                   struct_param = model_dict[\"struct_param\"],\n",
    "                   W_init_list = model_dict[\"weights\"],\n",
    "                   b_init_list = model_dict[\"bias\"],\n",
    "                   settings = model_dict[\"settings\"],\n",
    "                   is_cuda = is_cuda,\n",
    "                  )\n",
    "    elif net_type == \"ConvNet\":\n",
    "        return ConvNet(input_channels = model_dict[\"input_channels\"],\n",
    "                       struct_param = model_dict[\"struct_param\"],\n",
    "                       W_init_list = model_dict[\"weights\"],\n",
    "                       b_init_list = model_dict[\"bias\"],\n",
    "                       settings = model_dict[\"settings\"],\n",
    "                       is_cuda = is_cuda,\n",
    "                      )\n",
    "    else:\n",
    "        raise Exception(\"net_type {0} not recognized!\".format(net_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        struct_param,\n",
    "        W_init_list = None,     # initialization for weights\n",
    "        b_init_list = None,     # initialization for bias\n",
    "        settings = {},          # Default settings for each layer, if the settings for the layer is not provided in struct_param\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = deepcopy(settings)\n",
    "        self.is_cuda = is_cuda\n",
    "        \n",
    "        self.init_layers(deepcopy(struct_param))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def struct_param(self):\n",
    "        return [getattr(self, \"layer_{0}\".format(i)).struct_param for i in range(self.num_layers)]\n",
    "\n",
    "\n",
    "    def init_layers(self, struct_param):\n",
    "        for k, layer_struct_param in enumerate(struct_param):\n",
    "            num_neurons_prev = struct_param[k - 1][0] if k > 0 else self.input_size\n",
    "            num_neurons = layer_struct_param[0]\n",
    "            W_init = self.W_init_list[k] if self.W_init_list is not None else None\n",
    "            b_init = self.b_init_list[k] if self.b_init_list is not None else None\n",
    "\n",
    "            # Get settings for the current layer:\n",
    "            layer_settings = deepcopy(self.settings) if bool(self.settings) else {}\n",
    "            layer_settings.update(layer_struct_param[2])\n",
    "            layer_settings.pop(\"snap_dict\", None)\n",
    "            if \"snap_dict\" in self.settings and k in self.settings[\"snap_dict\"]:\n",
    "                layer_settings[\"snap_dict\"] = self.settings[\"snap_dict\"][k]\n",
    "\n",
    "            # Construct layer:\n",
    "            layer = get_Layer(layer_type = layer_struct_param[1],\n",
    "                              input_size = num_neurons_prev,\n",
    "                              output_size = num_neurons,\n",
    "                              W_init = W_init,\n",
    "                              b_init = b_init,\n",
    "                              settings = layer_settings,\n",
    "                              is_cuda = self.is_cuda,\n",
    "                             )\n",
    "            setattr(self, \"layer_{0}\".format(k), layer)\n",
    "\n",
    "\n",
    "    def init_bias_with_input(self, input, mode = \"std_sqrt\", neglect_last_layer = True):\n",
    "        output = input\n",
    "        for k, layer_struct_param in enumerate(self.struct_param):\n",
    "            if neglect_last_layer and k == len(self.struct_param) - 1:\n",
    "                break\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            layer.init_bias_with_input(output, mode = mode)\n",
    "            output = layer(output)\n",
    "\n",
    "\n",
    "    def forward(self, input, p_dict = None):\n",
    "        output = input\n",
    "        for k in range(len(self.struct_param)):\n",
    "            if p_dict is None:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "            else:\n",
    "                output = getattr(self, \"layer_{0}\".format(k))(output, p_dict = p_dict[k])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def init_with_p_dict(self, p_dict):\n",
    "        if p_dict is not None:\n",
    "            for k in range(self.num_layers):\n",
    "                if k in p_dict:\n",
    "                    layer = getattr(self, \"layer_{0}\".format(k))\n",
    "                    layer.init_with_p_dict(p_dict[k])\n",
    "\n",
    "\n",
    "    def get_param_names(self, source):\n",
    "        param_names_list = []\n",
    "        for k in range(len(self.struct_param)):\n",
    "            param_names = getattr(self, \"layer_{0}\".format(k)).get_param_names(source = source)\n",
    "            param_names = [\"layer_{0}.{1}\".format(k, name) for name in param_names]\n",
    "            param_names_list = param_names_list + param_names\n",
    "        return param_names_list\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(len(self.struct_param)):\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            reg = reg + layer.get_regularization(mode = mode, source = source)\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def reset_layer(self, layer_id, layer):\n",
    "        setattr(self, \"layer_{0}\".format(layer_id), layer)\n",
    "\n",
    "\n",
    "    def insert_layer(self, layer_id, layer):\n",
    "        if layer_id < 0:\n",
    "            layer_id += self.num_layers\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            if next_layer.struct_param[1] == \"Simple_Layer\":\n",
    "                assert next_layer.input_size == layer.output_size, \"The inserted layer's output_size {0} must be compatible with next layer_{1}'s input_size {2}!\"\\\n",
    "                    .format(layer.output_size, layer_id + 1, next_layer.input_size)\n",
    "        for i in range(self.num_layers - 1, layer_id - 1, -1):\n",
    "            setattr(self, \"layer_{0}\".format(i + 1), getattr(self, \"layer_{0}\".format(i)))\n",
    "        setattr(self, \"layer_{0}\".format(layer_id), layer)\n",
    "        self.num_layers += 1\n",
    "    \n",
    "    \n",
    "    def remove_layer(self, layer_id):\n",
    "        if layer_id < 0:\n",
    "            layer_id += self.num_layers\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            num_neurons_prev = self.struct_param[layer_id - 1][0] if layer_id > 0 else self.input_size\n",
    "            replaced_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            if replaced_layer.struct_param[1] == \"Simple_Layer\":\n",
    "                assert replaced_layer.input_size == num_neurons_prev, \\\n",
    "                    \"After deleting layer_{0}, the replaced layer's input_size {1} must be compatible with previous layer's output neurons {2}!\"\\\n",
    "                        .format(layer_id, replaced_layer.input_size, num_neurons_prev)\n",
    "        for i in range(layer_id, self.num_layers - 1):\n",
    "            setattr(self, \"layer_{0}\".format(i), getattr(self, \"layer_{0}\".format(i + 1)))\n",
    "        self.num_layers -= 1\n",
    "\n",
    "\n",
    "    def prune_neurons(self, layer_id, neuron_ids):\n",
    "        if layer_id < 0:\n",
    "            layer_id = self.num_layers + layer_id\n",
    "        layer = getattr(self, \"layer_{0}\".format(layer_id))\n",
    "        layer.prune_output_neurons(neuron_ids)\n",
    "        self.reset_layer(layer_id, layer)\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            next_layer.prune_input_neurons(neuron_ids)\n",
    "            self.reset_layer(layer_id + 1, next_layer)\n",
    "\n",
    "\n",
    "    def add_neurons(self, layer_id, num_neurons, mode = (\"imitation\", \"zeros\")):\n",
    "        if not isinstance(mode, list) and not isinstance(mode, tuple):\n",
    "            mode = (mode, mode)\n",
    "        if layer_id < 0:\n",
    "            layer_id = self.num_layers + layer_id\n",
    "        layer = getattr(self, \"layer_{0}\".format(layer_id))\n",
    "        layer.add_output_neurons(num_neurons, mode = mode[0])\n",
    "        self.reset_layer(layer_id, layer)\n",
    "        if layer_id < self.num_layers - 1:\n",
    "            next_layer = getattr(self, \"layer_{0}\".format(layer_id + 1))\n",
    "            next_layer.add_input_neurons(num_neurons, mode = mode[1])\n",
    "            self.reset_layer(layer_id + 1, next_layer)\n",
    "    \n",
    "    \n",
    "    def split_to_net_ensemble(self, mode = \"standardize\"):\n",
    "        num_models = self.struct_param[-1][0]\n",
    "        model_core = deepcopy(self)\n",
    "        if mode == \"standardize\":\n",
    "            last_layer = getattr(model_core, \"layer_{0}\".format(model_core.num_layers - 1))\n",
    "            last_layer.standardize(mode = \"b_mean_zero\")\n",
    "        else:\n",
    "            raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        model_list = [deepcopy(model_core) for i in range(num_models)]\n",
    "        for i, model in enumerate(model_list):\n",
    "            to_prune = list(range(num_models))\n",
    "            to_prune.pop(i)\n",
    "            model.prune_neurons(-1, to_prune)\n",
    "        return construct_net_ensemble_from_nets(model_list)\n",
    "            \n",
    "\n",
    "    def inspect_operation(self, input_, operation_between):\n",
    "        output = input_\n",
    "        for k in range(*operation_between):\n",
    "            output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = None, b_source = None, layer_ids = None, isplot = False, raise_error = True):\n",
    "        layer_ids = range(len(self.struct_param)) if layer_ids is None else layer_ids\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        if W_source is not None:\n",
    "            for k in range(len(self.struct_param)):\n",
    "                if k in layer_ids:\n",
    "                    if W_source == \"core\":\n",
    "                        try:\n",
    "                            W, _ = getattr(self, \"layer_{0}\".format(k)).get_weights_bias()\n",
    "                        except Exception as e:\n",
    "                            if raise_error:\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(e)\n",
    "                            W = np.array([np.NaN])\n",
    "                    else:\n",
    "                        raise Exception(\"W_source '{0}' not recognized!\".format(W_source))\n",
    "                    W_list.append(W)\n",
    "        \n",
    "        if b_source is not None:\n",
    "            for k in range(len(self.struct_param)):\n",
    "                if k in layer_ids:\n",
    "                    if b_source == \"core\":\n",
    "                        try:\n",
    "                            _, b = getattr(self, \"layer_{0}\".format(k)).get_weights_bias()\n",
    "                        except Exception as e:\n",
    "                            if raise_error:\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(e)\n",
    "                            b = np.array([np.NaN])\n",
    "                    else:\n",
    "                        raise Exception(\"b_source '{0}' not recognized!\".format(b_source))\n",
    "                b_list.append(b)\n",
    "                \n",
    "        if isplot:\n",
    "            if W_source is not None:\n",
    "                print(\"weight {0}:\".format(W_source))\n",
    "                plot_matrices(W_list)\n",
    "            if b_source is not None:\n",
    "                print(\"bias {0}:\".format(b_source))\n",
    "                plot_matrices(b_list)\n",
    "        return W_list, b_list\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"Net\"}\n",
    "        model_dict[\"input_size\"] = self.input_size\n",
    "        model_dict[\"struct_param\"] = get_full_struct_param(self.struct_param, self.settings)\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"net_type\"] = \"Net\"\n",
    "        return model_dict\n",
    "\n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_net(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cell(Net):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size, \n",
    "        struct_param,\n",
    "        W_init_list = None,     # initialization for weights\n",
    "        b_init_list = None,     # initialization for bias\n",
    "        settings = {},          # Default settings for each layer, if the settings for the layer is not provided in struct_param\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        self.input_size_true = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        assert hidden_size == struct_param[-1][0], \"hidden_size must be equal to the num_neurons in the last layer of struct_param!\"\n",
    "        super(RNN_Cell, self).__init__(\n",
    "            input_size = input_size + hidden_size,\n",
    "            struct_param = struct_param,\n",
    "            W_init_list = W_init_list,\n",
    "            b_init_list = b_init_list,\n",
    "            settings = settings,\n",
    "            is_cuda = is_cuda,\n",
    "        )\n",
    "\n",
    "\n",
    "    def check_forward_input(self, input):\n",
    "        if input.size(1) != self.input_size_true:\n",
    "            raise RuntimeError(\n",
    "                \"input has inconsistent input_size: got {}, expected {}\".format(\n",
    "                    input.size(1), self.input_size_true))\n",
    "\n",
    "\n",
    "    def check_forward_hidden(self, input, hx, hidden_label=''):\n",
    "        if input.size(0) != hx.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n",
    "                    input.size(0), hidden_label, hx.size(0)))\n",
    "\n",
    "        if hx.size(1) != self.hidden_size:\n",
    "            raise RuntimeError(\n",
    "                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n",
    "                    hidden_label, input.size(1), self.input_size_true))\n",
    "\n",
    "\n",
    "    def forward(self, input, hx, **kwargs):\n",
    "        self.check_forward_input(input)\n",
    "        self.check_forward_hidden(input, hx)\n",
    "        output = torch.cat((input, hx), -1)\n",
    "        for k in range(len(self.struct_param)):\n",
    "            output = getattr(self, \"layer_{0}\".format(k))(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class RNN_with_encoder(nn.Module):\n",
    "    def __init__(self, input_size = 1, hidden_size = 50, struct_param_rnn = None, struct_param_encoder = None, struct_param_decoder = None):\n",
    "        super(RNN_with_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.struct_param_rnn = struct_param_rnn\n",
    "        self.struct_param_encoder = struct_param_encoder\n",
    "        self.struct_param_decoder = struct_param_decoder \n",
    "        if self.struct_param_encoder is not None:\n",
    "            self.encoder = Net(input_size = input_size, struct_param = struct_param_encoder)\n",
    "            input_size_encode = struct_param_encoder[-1][0]\n",
    "        else:\n",
    "            input_size_encode = input_size\n",
    "        if self.struct_param_rnn is None:\n",
    "            self.rnn1 = nn.RNNCell(input_size_encode, hidden_size)\n",
    "        else:\n",
    "            self.rnn1 = RNN_Cell(input_size_encode, hidden_size, struct_param_rnn)\n",
    "        if self.struct_param_decoder is not None:\n",
    "            self.decoder = Net(input_size = hidden_size, struct_param = struct_param_decoder)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = Variable(torch.zeros(input.size(0), self.hidden_size).float(), requires_grad = False)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            input_t_encode = self.encoder(input_t) if self.struct_param_encoder is not None else input_t\n",
    "            h_t = self.rnn1(input_t_encode, h_t)\n",
    "            output = self.decoder(h_t)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            # The first self.input_size dimensions are the prediction of next time step (The following dimensions may be uncertainty.):\n",
    "            input_t_encode = self.encoder(output[...,:self.input_size]) if self.struct_param_encoder is not None else output\n",
    "            h_t = self.rnn1(input_t_encode, h_t)\n",
    "            output = self.decoder(h_t)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def get_hidden(self, input, future = 0):\n",
    "        hiddens = []\n",
    "        h_t = Variable(torch.zeros(input.size(0), self.hidden_size).float(), requires_grad = False)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            input_t_encode = self.encoder(input_t) if self.struct_param_encoder is not None else input_t\n",
    "            h_t = self.rnn1(input_t_encode, h_t)\n",
    "            output = self.decoder(h_t)\n",
    "            hiddens.append(h_t)\n",
    "        for i in range(future):# if we should predict the future\n",
    "            input_t_encode = self.encoder(output[...,:self.input_size]) if self.struct_param_encoder is not None else output\n",
    "            h_t = self.rnn1(input_t_encode, h_t)\n",
    "            output = self.decoder(h_t)\n",
    "            hiddens.append(h_t)\n",
    "        hiddens = torch.stack(hiddens, 1)\n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        struct_param,\n",
    "        W_init_list = None,\n",
    "        b_init_list = None,\n",
    "        settings = {},\n",
    "        is_cuda = False,\n",
    "        ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.struct_param = struct_param\n",
    "        self.W_init_list = W_init_list\n",
    "        self.b_init_list = b_init_list\n",
    "        self.settings = settings\n",
    "        self.num_layers = len(struct_param)\n",
    "        self.is_cuda = is_cuda\n",
    "        for i in range(len(self.struct_param)):\n",
    "            if i > 0:\n",
    "                if \"Pool\" not in self.struct_param[i - 1][1] and \"Unpool\" not in self.struct_param[i - 1][1] and \"Upsample\" not in self.struct_param[i - 1][1]:\n",
    "                    num_channels_prev = self.struct_param[i - 1][0]\n",
    "                else: \n",
    "                    num_channels_prev = self.struct_param[i - 2][0]\n",
    "            else:\n",
    "                num_channels_prev = input_channels\n",
    "            num_channels = self.struct_param[i][0]\n",
    "            layer_type = self.struct_param[i][1]\n",
    "            layer_settings = self.struct_param[i][2]\n",
    "            if layer_type == \"Conv2d\":\n",
    "                layer = nn.Conv2d(num_channels_prev, \n",
    "                                  num_channels,\n",
    "                                  kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                  stride = layer_settings[\"stride\"],\n",
    "                                  padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                  dilation = layer_settings[\"dilation\"] if \"dilation\" in layer_settings else 1,\n",
    "                                 )\n",
    "            elif layer_type == \"ConvTranspose2d\":\n",
    "                layer = nn.ConvTranspose2d(num_channels_prev,\n",
    "                                           num_channels,\n",
    "                                           kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                           stride = layer_settings[\"stride\"],\n",
    "                                           padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                           dilation = layer_settings[\"dilation\"] if \"dilation\" in layer_settings else 1,\n",
    "                                          )\n",
    "            elif layer_type == \"MaxPool2d\":\n",
    "                layer = nn.MaxPool2d(kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                     stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else None,\n",
    "                                     padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                     return_indices = layer_settings[\"return_indices\"] if \"return_indices\" in layer_settings else False,\n",
    "                                    )\n",
    "            elif layer_type == \"MaxUnpool2d\":\n",
    "                layer = nn.MaxUnpool2d(kernel_size = layer_settings[\"kernel_size\"],\n",
    "                                       stride = layer_settings[\"stride\"] if \"stride\" in layer_settings else None,\n",
    "                                       padding = layer_settings[\"padding\"] if \"padding\" in layer_settings else 0,\n",
    "                                      )\n",
    "            elif layer_type == \"Upsample\":\n",
    "                layer = nn.Upsample(scale_factor = layer_settings[\"scale_factor\"],\n",
    "                                    mode = layer_settings[\"mode\"] if \"mode\" in layer_settings else \"nearest\",\n",
    "                                   )\n",
    "            elif layer_type == \"BatchNorm2d\":\n",
    "                layer = nn.BatchNorm2d(num_features = num_channels)\n",
    "            else:\n",
    "                raise Exception(\"layer_type {0} not recognized!\".format(layer_type))\n",
    "            \n",
    "            # Initialize using provided initial values:\n",
    "            if self.W_init_list is not None and self.W_init_list[i] is not None:\n",
    "                layer.weight.data = torch.FloatTensor(self.W_init_list[i])\n",
    "                layer.bias.data = torch.FloatTensor(self.b_init_list[i])\n",
    "            \n",
    "            setattr(self, \"layer_{0}\".format(i), layer)\n",
    "        if self.is_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    def forward(self, input, indices_list = None):\n",
    "        output = input\n",
    "        if indices_list is None:\n",
    "            indices_list = []\n",
    "        for i in range(len(self.struct_param)):\n",
    "            if \"Unpool\" in self.struct_param[i][1]:\n",
    "                output_tentative = getattr(self, \"layer_{0}\".format(i))(output, indices_list.pop(-1))\n",
    "            else:\n",
    "                output_tentative = getattr(self, \"layer_{0}\".format(i))(output)\n",
    "            if isinstance(output_tentative, tuple):\n",
    "                output, indices = output_tentative\n",
    "                indices_list.append(indices)\n",
    "            else:\n",
    "                output = output_tentative\n",
    "            if \"activation\" in self.struct_param[i][2]:\n",
    "                activation = self.struct_param[i][2][\"activation\"]\n",
    "            else:\n",
    "                if \"activation\" in self.settings:\n",
    "                    activation = self.settings[\"activation\"]\n",
    "                else:\n",
    "                    activation = \"relu\"\n",
    "                if \"Pool\" in self.struct_param[i - 1][1] or \"Unpool\" in self.struct_param[i - 1][1] or \"Upsample\" in self.struct_param[i - 1][1]:\n",
    "                    activation = \"linear\"\n",
    "            output = get_activation(activation)(output)\n",
    "        return output, indices_list\n",
    "\n",
    "\n",
    "    def get_regularization(self, source = [\"weight\", \"bias\"], mode = \"L1\"):\n",
    "        reg = Variable(torch.FloatTensor([0]), requires_grad = False)\n",
    "        if self.is_cuda:\n",
    "            reg = reg.cuda()\n",
    "        for k in range(self.num_layers):\n",
    "            layer = getattr(self, \"layer_{0}\".format(k))\n",
    "            for source_ele in source:\n",
    "                if source_ele == \"weight\":\n",
    "                    item = layer.weight\n",
    "                elif source_ele == \"bias\":\n",
    "                    item = layer.bias\n",
    "                if mode == \"L1\":\n",
    "                    reg = reg + item.abs().sum()\n",
    "                elif mode == \"L2\":\n",
    "                    reg = reg + (item ** 2).sum()\n",
    "                else:\n",
    "                    raise Exception(\"mode {0} not recognized!\".format(mode))\n",
    "        return reg\n",
    "\n",
    "\n",
    "    def get_weights_bias(self, W_source = \"core\", b_source = \"core\"):\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        param_available = [\"Conv2d\", \"ConvTranspose2d\", \"BatchNorm2d\"]\n",
    "        for k in range(self.num_layers):\n",
    "            if self.struct_param[k][1] in param_available:\n",
    "                layer = getattr(self, \"layer_{0}\".format(k))\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(to_np_array(layer.weight))\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(to_np_array(layer.bias))\n",
    "            else:\n",
    "                if W_source == \"core\":\n",
    "                    W_list.append(None)\n",
    "                if b_source == \"core\":\n",
    "                    b_list.append(None)\n",
    "        return W_list, b_list\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_dict(self):\n",
    "        model_dict = {\"type\": \"ConvNet\"}\n",
    "        model_dict[\"input_channels\"] = self.input_channels\n",
    "        model_dict[\"struct_param\"] = self.struct_param\n",
    "        model_dict[\"settings\"] = self.settings\n",
    "        model_dict[\"weights\"], model_dict[\"bias\"] = self.get_weights_bias(W_source = \"core\", b_source = \"core\")\n",
    "        return model_dict\n",
    "    \n",
    "\n",
    "    def load_model_dict(self, model_dict):\n",
    "        new_net = load_model_dict_net(model_dict, is_cuda = self.is_cuda)\n",
    "        self.__dict__.update(new_net.__dict__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
